### [Industrial-Grade Sensor Simulation via Gaussian Splatting: A Modular Framework for Scalable Editing and Full-Stack Validation](https://arxiv.org/abs/2503.11731)
*Xianming Zeng,Sicong Du,Qifeng Chen,Lizhe Liu,Haoyu Shu,Jiaxuan Gao,Jiarun Liu,Jiulong Xu,Jianyun Xu,Mingxia Chen,Yiru Zhao,Peng Chen,Yapeng Xue,Chunming Zhao,Sheng Yang,Qiang Li*
<details>
  <summary>Abstract</summary>
Sensor simulation is pivotal for scalable validation of autonomous driving
systems, yet existing Neural Radiance Fields (NeRF) based methods face
applicability and efficiency challenges in industrial workflows. This paper
introduces a Gaussian Splatting (GS) based system to address these challenges:
We first break down sensor simulator components and analyze the possible
advantages of GS over NeRF. Then in practice, we refactor three crucial
components through GS, to leverage its explicit scene representation and
real-time rendering: (1) choosing the 2D neural Gaussian representation for
physics-compliant scene and sensor modeling, (2) proposing a scene editing
pipeline to leverage Gaussian primitives library for data augmentation, and (3)
coupling a controllable diffusion model for scene expansion and harmonization.
We implement this framework on a proprietary autonomous driving dataset
supporting cameras and LiDAR sensors. We demonstrate through ablation studies
that our approach reduces frame-wise simulation latency, achieves better
geometric and photometric consistency, and enables interpretable explicit scene
editing and expansion. Furthermore, we showcase how integrating such a GS-based
sensor simulator with traffic and dynamic simulators enables full-stack testing
of end-to-end autonomy algorithms. Our work provides both algorithmic insights
and practical validation, establishing GS as a cornerstone for industrial-grade
sensor simulation.
</details>

### [Safe Vision-Language Models via Unsafe Weights Manipulation](https://arxiv.org/abs/2503.11742)
*Moreno D'Inc√†,Elia Peruzzo,Xingqian Xu,Humphrey Shi,Nicu Sebe,Massimiliano Mancini*
<details>
  <summary>Abstract</summary>
Vision-language models (VLMs) often inherit the biases and unsafe
associations present within their large-scale training dataset. While recent
approaches mitigate unsafe behaviors, their evaluation focuses on how safe the
model is on unsafe inputs, ignoring potential shortcomings on safe ones. In
this paper, we first revise safety evaluation by introducing SafeGround, a new
set of metrics that evaluate safety at different levels of granularity. With
this metric, we uncover a surprising issue of training-based methods: they make
the model less safe on safe inputs. From this finding, we take a different
direction and explore whether it is possible to make a model safer without
training, introducing Unsafe Weights Manipulation (UWM). UWM uses a calibration
set of safe and unsafe instances to compare activations between safe and unsafe
content, identifying the most important parameters for processing the latter.
Their values are then manipulated via negation. Experiments show that UWM
achieves the best tradeoff between safety and knowledge preservation,
consistently improving VLMs on unsafe queries while outperforming even
training-based state-of-the-art methods on safe ones.
</details>

### [Making Every Step Effective: Jailbreaking Large Vision-Language Models Through Hierarchical KV Equalization](https://arxiv.org/abs/2503.11750)
*Shuyang Hao,Yiwei Wang,Bryan Hooi,Jun Liu,Muhao Chen,Zi Huang,Yujun Cai*
<details>
  <summary>Abstract</summary>
In the realm of large vision-language models (LVLMs), adversarial jailbreak
attacks serve as a red-teaming approach to identify safety vulnerabilities of
these models and their associated defense mechanisms. However, we identify a
critical limitation: not every adversarial optimization step leads to a
positive outcome, and indiscriminately accepting optimization results at each
step may reduce the overall attack success rate. To address this challenge, we
introduce HKVE (Hierarchical Key-Value Equalization), an innovative
jailbreaking framework that selectively accepts gradient optimization results
based on the distribution of attention scores across different layers, ensuring
that every optimization step positively contributes to the attack. Extensive
experiments demonstrate HKVE's significant effectiveness, achieving attack
success rates of 75.08% on MiniGPT4, 85.84% on LLaVA and 81.00% on Qwen-VL,
substantially outperforming existing methods by margins of 20.43\%, 21.01\% and
26.43\% respectively. Furthermore, making every step effective not only leads
to an increase in attack success rate but also allows for a reduction in the
number of iterations, thereby lowering computational costs. Warning: This paper
contains potentially harmful example data.
</details>

### [Rethinking Multi-modal Object Detection from the Perspective of Mono-Modality Feature Learning](https://arxiv.org/abs/2503.11780)
*Tianyi Zhao,Boyang Liu,Yanglei Gao,Yiming Sun,Maoxun Yuan,Xingxing Wei*
<details>
  <summary>Abstract</summary>
Multi-Modal Object Detection (MMOD), due to its stronger adaptability to
various complex environments, has been widely applied in various applications.
Extensive research is dedicated to the RGB-IR object detection, primarily
focusing on how to integrate complementary features from RGB-IR modalities.
However, they neglect the mono-modality insufficient learning problem that the
decreased feature extraction capability in multi-modal joint learning. This
leads to an unreasonable but prevalent phenomenon--Fusion Degradation, which
hinders the performance improvement of the MMOD model. Motivated by this, in
this paper, we introduce linear probing evaluation to the multi-modal detectors
and rethink the multi-modal object detection task from the mono-modality
learning perspective. Therefore, we construct an novel framework called
M$^2$D-LIF, which consists of the Mono-Modality Distillation (M$^2$D) method
and the Local Illumination-aware Fusion (LIF) module. The M$^2$D-LIF framework
facilitates the sufficient learning of mono-modality during multi-modal joint
training and explores a lightweight yet effective feature fusion manner to
achieve superior object detection performance. Extensive experiments conducted
on three MMOD datasets demonstrate that our M$^2$D-LIF effectively mitigates
the Fusion Degradation phenomenon and outperforms the previous SOTA detectors.
</details>

### [Color Matching Using Hypernetwork-Based Kolmogorov-Arnold Networks](https://arxiv.org/abs/2503.11781)
*Artem Nikonorov,Georgy Perevozchikov,Andrei Korepanov,Nancy Mehta,Mahmoud Afifi,Egor Ershov,Radu Timofte*
<details>
  <summary>Abstract</summary>
We present cmKAN, a versatile framework for color matching. Given an input
image with colors from a source color distribution, our method effectively and
accurately maps these colors to match a target color distribution in both
supervised and unsupervised settings. Our framework leverages the spline
capabilities of Kolmogorov-Arnold Networks (KANs) to model the color matching
between source and target distributions. Specifically, we developed a
hypernetwork that generates spatially varying weight maps to control the
nonlinear splines of a KAN, enabling accurate color matching. As part of this
work, we introduce a first large-scale dataset of paired images captured by two
distinct cameras and evaluate the efficacy of our and existing methods in
matching colors. We evaluated our approach across various color-matching tasks,
including: (1) raw-to-raw mapping, where the source color distribution is in
one camera's raw color space and the target in another camera's raw space; (2)
raw-to-sRGB mapping, where the source color distribution is in a camera's raw
space and the target is in the display sRGB space, emulating the color
rendering of a camera ISP; and (3) sRGB-to-sRGB mapping, where the goal is to
transfer colors from a source sRGB space (e.g., produced by a source camera
ISP) to a target sRGB space (e.g., from a different camera ISP). The results
show that our method outperforms existing approaches by 37.3% on average for
supervised and unsupervised cases while remaining lightweight compared to other
methods. The codes, dataset, and pre-trained models are available at:
https://github.com/gosha20777/cmKAN
</details>

### [ECLARE: Efficient cross-planar learning for anisotropic resolution enhancement](https://arxiv.org/abs/2503.11787)
*Samuel W. Remedios,Shuwen Wei,Shuo Han,Jinwei Zhang,Aaron Carass,Kurt G. Schilling,Dzung L. Pham,Jerry L. Prince,Blake E. Dewey*
<details>
  <summary>Abstract</summary>
In clinical imaging, magnetic resonance (MR) image volumes are often acquired
as stacks of 2D slices, permitting decreased scan times, improved
signal-to-noise ratio, and image contrasts unique to 2D MR pulse sequences.
While this is sufficient for clinical evaluation, automated algorithms designed
for 3D analysis perform sub-optimally on 2D-acquired scans, especially those
with thick slices and gaps between slices. Super-resolution (SR) methods aim to
address this problem, but previous methods do not address all of the following:
slice profile shape estimation, slice gap, domain shift, and non-integer /
arbitrary upsampling factors. In this paper, we propose ECLARE (Efficient
Cross-planar Learning for Anisotropic Resolution Enhancement), a self-SR method
that addresses each of these factors. ECLARE estimates the slice profile from
the 2D-acquired multi-slice MR volume, trains a network to learn the mapping
from low-resolution to high-resolution in-plane patches from the same volume,
and performs SR with anti-aliasing. We compared ECLARE to cubic B-spline
interpolation, SMORE, and other contemporary SR methods. We used realistic and
representative simulations so that quantitative performance against a ground
truth could be computed, and ECLARE outperformed all other methods in both
signal recovery and downstream tasks. On real data for which there is no ground
truth, ECLARE demonstrated qualitative superiority over other methods as well.
Importantly, as ECLARE does not use external training data it cannot suffer
from domain shift between training and testing. Our code is open-source and
available at https://www.github.com/sremedios/eclare.
</details>

### [StyleMorpheus: A Style-Based 3D-Aware Morphable Face Model](https://arxiv.org/abs/2503.11792)
*Peizhi Yan,Rabab K. Ward,Dan Wang,Qiang Tang,Shan Du*
<details>
  <summary>Abstract</summary>
For 3D face modeling, the recently developed 3D-aware neural rendering
methods are able to render photorealistic face images with arbitrary viewing
directions. The training of the parametric controllable 3D-aware face models,
however, still relies on a large-scale dataset that is lab-collected. To
address this issue, this paper introduces "StyleMorpheus", the first
style-based neural 3D Morphable Face Model (3DMM) that is trained on
in-the-wild images. It inherits 3DMM's disentangled controllability (over face
identity, expression, and appearance) but without the need for accurately
reconstructed explicit 3D shapes. StyleMorpheus employs an auto-encoder
structure. The encoder aims at learning a representative disentangled
parametric code space and the decoder improves the disentanglement using shape
and appearance-related style codes in the different sub-modules of the network.
Furthermore, we fine-tune the decoder through style-based generative
adversarial learning to achieve photorealistic 3D rendering quality. The
proposed style-based design enables StyleMorpheus to achieve state-of-the-art
3D-aware face reconstruction results, while also allowing disentangled control
of the reconstructed face. Our model achieves real-time rendering speed,
allowing its use in virtual reality applications. We also demonstrate the
capability of the proposed style-based design in face editing applications such
as style mixing and color editing. Project homepage:
https://github.com/ubc-3d-vision-lab/StyleMorpheus.
</details>

### [Semantic-Clipping: Efficient Vision-Language Modeling with Semantic-Guidedd Visual Selection](https://arxiv.org/abs/2503.11794)
*Bangzheng Li,Fei Wang,Wenxuan Zhou,Nan Xu,Ben Zhou,Sheng Zhang,Hoifung Poon,Muhao Chen*
<details>
  <summary>Abstract</summary>
Vision-Language Models (VLMs) leverage aligned visual encoders to transform
images into visual tokens, allowing them to be processed similarly to text by
the backbone large language model (LLM). This unified input paradigm enables
VLMs to excel in vision-language tasks such as visual question answering (VQA).
To improve fine-grained visual reasoning, recent advancements in
vision-language modeling introduce image cropping techniques that feed all
encoded sub-images into the model. However, this approach significantly
increases the number of visual tokens, leading to inefficiency and potential
distractions for the LLM. To address the generalization challenges of image
representation in VLMs, we propose a lightweight, universal framework that
seamlessly integrates with existing VLMs to enhance their ability to process
finegrained details. Our method leverages textual semantics to identify key
visual areas, improving VQA performance without requiring any retraining of the
VLM. Additionally, it incorporates textual signals into the visual encoding
process, enhancing both efficiency and effectiveness. The proposed method,
SEMCLIP, strengthens the visual understanding of a 7B VLM, LLaVA-1.5 by 3.3% on
average across 7 benchmarks, and particularly by 5.3% on the challenging
detailed understanding benchmark V*.
</details>

### [Human-in-the-Loop Local Corrections of 3D Scene Layouts via Infilling](https://arxiv.org/abs/2503.11806)
*Christopher Xie,Armen Avetisyan,Henry Howard-Jenkins,Yawar Siddiqui,Julian Straub,Richard Newcombe,Vasileios Balntas,Jakob Engel*
<details>
  <summary>Abstract</summary>
We present a novel human-in-the-loop approach to estimate 3D scene layout
that uses human feedback from an egocentric standpoint. We study this approach
through introduction of a novel local correction task, where users identify
local errors and prompt a model to automatically correct them. Building on
SceneScript, a state-of-the-art framework for 3D scene layout estimation that
leverages structured language, we propose a solution that structures this
problem as "infilling", a task studied in natural language processing. We train
a multi-task version of SceneScript that maintains performance on global
predictions while significantly improving its local correction ability. We
integrate this into a human-in-the-loop system, enabling a user to iteratively
refine scene layout estimates via a low-friction "one-click fix'' workflow. Our
system enables the final refined layout to diverge from the training
distribution, allowing for more accurate modelling of complex layouts.
</details>

### [Mitigating Bad Ground Truth in Supervised Machine Learning based Crop Classification: A Multi-Level Framework with Sentinel-2 Images](https://arxiv.org/abs/2503.11807)
*Sanayya A,Amoolya Shetty,Abhijeet Sharma,Venkatesh Ravichandran,Masthan Wali Gosuvarapalli,Sarthak Jain,Priyamvada Nanjundiah,Ujjal Kr Dutta,Divya Sharma*
<details>
  <summary>Abstract</summary>
In agricultural management, precise Ground Truth (GT) data is crucial for
accurate Machine Learning (ML) based crop classification. Yet, issues like crop
mislabeling and incorrect land identification are common. We propose a
multi-level GT cleaning framework while utilizing multi-temporal Sentinel-2
data to address these issues. Specifically, this framework utilizes generating
embeddings for farmland, clustering similar crop profiles, and identification
of outliers indicating GT errors. We validated clusters with False Colour
Composite (FCC) checks and used distance-based metrics to scale and automate
this verification process. The importance of cleaning the GT data became
apparent when the models were trained on the clean and unclean data. For
instance, when we trained a Random Forest model with the clean GT data, we
achieved upto 70\% absolute percentage points higher for the F1 score metric.
This approach advances crop classification methodologies, with potential for
applications towards improving loan underwriting and agricultural
decision-making.
</details>

### [Towards a Unified Copernicus Foundation Model for Earth Vision](https://arxiv.org/abs/2503.11849)
*Yi Wang,Zhitong Xiong,Chenying Liu,Adam J. Stewart,Thomas Dujardin,Nikolaos Ioannis Bountos,Angelos Zavras,Franziska Gerken,Ioannis Papoutsis,Laura Leal-Taix√©,Xiao Xiang Zhu*
<details>
  <summary>Abstract</summary>
Advances in Earth observation (EO) foundation models have unlocked the
potential of big satellite data to learn generic representations from space,
benefiting a wide range of downstream applications crucial to our planet.
However, most existing efforts remain limited to fixed spectral sensors, focus
solely on the Earth's surface, and overlook valuable metadata beyond imagery.
In this work, we take a step towards next-generation EO foundation models with
three key components: 1) Copernicus-Pretrain, a massive-scale pretraining
dataset that integrates 18.7M aligned images from all major Copernicus Sentinel
missions, spanning from the Earth's surface to its atmosphere; 2)
Copernicus-FM, a unified foundation model capable of processing any spectral or
non-spectral sensor modality using extended dynamic hypernetworks and flexible
metadata encoding; and 3) Copernicus-Bench, a systematic evaluation benchmark
with 15 hierarchical downstream tasks ranging from preprocessing to specialized
applications for each Sentinel mission. Our dataset, model, and benchmark
greatly improve the scalability, versatility, and multimodal adaptability of EO
foundation models, while also creating new opportunities to connect EO,
weather, and climate research. Codes, datasets and models are available at
https://github.com/zhu-xlab/Copernicus-FM.
</details>

### [DecAlign: Hierarchical Cross-Modal Alignment for Decoupled Multimodal Representation Learning](https://arxiv.org/abs/2503.11892)
*Chengxuan Qian,Shuo Xing,Shawn Li,Yue Zhao,Zhengzhong Tu*
<details>
  <summary>Abstract</summary>
Multimodal representation learning aims to capture both shared and
complementary semantic information across multiple modalities. However, the
intrinsic heterogeneity of diverse modalities presents substantial challenges
to achieve effective cross-modal collaboration and integration. To address
this, we introduce DecAlign, a novel hierarchical cross-modal alignment
framework designed to decouple multimodal representations into modality-unique
(heterogeneous) and modality-common (homogeneous) features. For handling
heterogeneity, we employ a prototype-guided optimal transport alignment
strategy leveraging gaussian mixture modeling and multi-marginal transport
plans, thus mitigating distribution discrepancies while preserving
modality-unique characteristics. To reinforce homogeneity, we ensure semantic
consistency across modalities by aligning latent distribution matching with
Maximum Mean Discrepancy regularization. Furthermore, we incorporate a
multimodal transformer to enhance high-level semantic feature fusion, thereby
further reducing cross-modal inconsistencies. Our extensive experiments on four
widely used multimodal benchmarks demonstrate that DecAlign consistently
outperforms existing state-of-the-art methods across five metrics. These
results highlight the efficacy of DecAlign in enhancing superior cross-modal
alignment and semantic consistency while preserving modality-unique features,
marking a significant advancement in multimodal representation learning
scenarios. Our project page is at https://taco-group.github.io/DecAlign and the
code is available at https://github.com/taco-group/DecAlign.
</details>

### [UStyle: Waterbody Style Transfer of Underwater Scenes by Depth-Guided Feature Synthesis](https://arxiv.org/abs/2503.11893)
*Md Abu Bakr Siddique,Junliang Liu,Piyush Singh,Md Jahidul Islam*
<details>
  <summary>Abstract</summary>
The concept of waterbody style transfer remains largely unexplored in the
underwater imaging and vision literature. Traditional image style transfer
(STx) methods primarily focus on artistic and photorealistic blending, often
failing to preserve object and scene geometry in images captured in
high-scattering mediums such as underwater. The wavelength-dependent nonlinear
attenuation and depth-dependent backscattering artifacts further complicate
learning underwater image STx from unpaired data. This paper introduces UStyle,
the first data-driven learning framework for transferring waterbody styles
across underwater images without requiring prior reference images or scene
information. We propose a novel depth-aware whitening and coloring transform
(DA-WCT) mechanism that integrates physics-based waterbody synthesis to ensure
perceptually consistent stylization while preserving scene structure. To
enhance style transfer quality, we incorporate carefully designed loss
functions that guide UStyle to maintain colorfulness, lightness, structural
integrity, and frequency-domain characteristics, as well as high-level content
in VGG and CLIP (contrastive language-image pretraining) feature spaces. By
addressing domain-specific challenges, UStyle provides a robust framework for
no-reference underwater image STx, surpassing state-of-the-art (SOTA) methods
that rely solely on end-to-end reconstruction loss. Furthermore, we introduce
the UF7D dataset, a curated collection of high-resolution underwater images
spanning seven distinct waterbody styles, establishing a benchmark to support
future research in underwater image STx. The UStyle inference pipeline and UF7D
dataset are released at: https://github.com/uf-robopi/UStyle.
</details>

### [Upcycling Text-to-Image Diffusion Models for Multi-Task Capabilities](https://arxiv.org/abs/2503.11905)
*Ruchika Chavhan,Abhinav Mehrotra,Malcolm Chadwick,Alberto Gil Ramos,Luca Morreale,Mehdi Noroozi,Sourav Bhattacharya*
<details>
  <summary>Abstract</summary>
Text-to-image synthesis has witnessed remarkable advancements in recent
years. Many attempts have been made to adopt text-to-image models to support
multiple tasks. However, existing approaches typically require
resource-intensive re-training or additional parameters to accommodate for the
new tasks, which makes the model inefficient for on-device deployment. We
propose Multi-Task Upcycling (MTU), a simple yet effective recipe that extends
the capabilities of a pre-trained text-to-image diffusion model to support a
variety of image-to-image generation tasks. MTU replaces Feed-Forward Network
(FFN) layers in the diffusion model with smaller FFNs, referred to as experts,
and combines them with a dynamic routing mechanism. To the best of our
knowledge, MTU is the first multi-task diffusion modeling approach that
seamlessly blends multi-tasking with on-device compatibility, by mitigating the
issue of parameter inflation. We show that the performance of MTU is on par
with the single-task fine-tuned diffusion models across several tasks including
image editing, super-resolution, and inpainting, while maintaining similar
latency and computational load (GFLOPs) as the single-task fine-tuned models.
</details>

### [A Survey on SAR ship classification using Deep Learning](https://arxiv.org/abs/2503.11906)
*Ch Muhammad Awais,Marco Reggiannini,Davide Moroni,Emanuele Salerno*
<details>
  <summary>Abstract</summary>
Deep learning (DL) has emerged as a powerful tool for Synthetic Aperture
Radar (SAR) ship classification. This survey comprehensively analyzes the
diverse DL techniques employed in this domain. We identify critical trends and
challenges, highlighting the importance of integrating handcrafted features,
utilizing public datasets, data augmentation, fine-tuning, explainability
techniques, and fostering interdisciplinary collaborations to improve DL model
performance. This survey establishes a first-of-its-kind taxonomy for
categorizing relevant research based on DL models, handcrafted feature use, SAR
attribute utilization, and the impact of fine-tuning. We discuss the
methodologies used in SAR ship classification tasks and the impact of different
techniques. Finally, the survey explores potential avenues for future research,
including addressing data scarcity, exploring novel DL architectures,
incorporating interpretability techniques, and establishing standardized
performance metrics. By addressing these challenges and leveraging advancements
in DL, researchers can contribute to developing more accurate and efficient
ship classification systems, ultimately enhancing maritime surveillance and
related applications.
</details>

### [k-fold Subsampling based Sequential Backward Feature Elimination](https://arxiv.org/abs/2503.11919)
*Jeonghwan Park,Kang Li,Huiyu Zhou*
<details>
  <summary>Abstract</summary>
We present a new wrapper feature selection algorithm for human detection.
This algorithm is a hybrid feature selection approach combining the benefits of
filter and wrapper methods. It allows the selection of an optimal feature
vector that well represents the shapes of the subjects in the images. In
detail, the proposed feature selection algorithm adopts the k-fold subsampling
and sequential backward elimination approach, while the standard linear support
vector machine (SVM) is used as the classifier for human detection. We apply
the proposed algorithm to the publicly accessible INRIA and ETH pedestrian full
image datasets with the PASCAL VOC evaluation criteria. Compared to other state
of the arts algorithms, our feature selection based approach can improve the
detection speed of the SVM classifier by over 50% with up to 2% better
detection accuracy. Our algorithm also outperforms the equivalent systems
introduced in the deformable part model approach with around 9% improvement in
the detection accuracy.
</details>

### [Generating a Biometrically Unique and Realistic Iris Database](https://arxiv.org/abs/2503.11930)
*Jingxuan Zhang,Robert J. Hart,Ziqian Bi,Shiaofen Fang,Susan Walsh*
<details>
  <summary>Abstract</summary>
The use of the iris as a biometric identifier has increased dramatically over
the last 30 years, prompting privacy and security concerns about the use of
iris images in research. It can be difficult to acquire iris image databases
due to ethical concerns, and this can be a barrier for those performing
biometrics research. In this paper, we describe and show how to create a
database of realistic, biometrically unidentifiable colored iris images by
training a diffusion model within an open-source diffusion framework. Not only
were we able to verify that our model is capable of creating iris textures that
are biometrically unique from the training data, but we were also able to
verify that our model output creates a full distribution of realistic iris
pigmentations. We highlight the fact that the utility of diffusion networks to
achieve these criteria with relative ease, warrants additional research in its
use within the context of iris database generation and presentation attack
security.
</details>

### [SPRINT: Script-agnostic Structure Recognition in Tables](https://arxiv.org/abs/2503.11932)
*Dhruv Kudale,Badri Vishal Kasuba,Venkatapathy Subramanian,Parag Chaudhuri,Ganesh Ramakrishnan*
<details>
  <summary>Abstract</summary>
Table Structure Recognition (TSR) is vital for various downstream tasks like
information retrieval, table reconstruction, and document understanding. While
most state-of-the-art (SOTA) research predominantly focuses on TSR in English
documents, the need for similar capabilities in other languages is evident,
considering the global diversity of data. Moreover, creating substantial
labeled data in non-English languages and training these SOTA models from
scratch is costly and time-consuming. We propose TSR as a language-agnostic
cell arrangement prediction and introduce SPRINT, Script-agnostic Structure
Recognition in Tables. SPRINT uses recently introduced Optimized Table
Structure Language (OTSL) sequences to predict table structures. We show that
when coupled with a pre-trained table grid estimator, SPRINT can improve the
overall tree edit distance-based similarity structure scores of tables even for
non-English documents. We experimentally evaluate our performance across
benchmark TSR datasets including PubTabNet, FinTabNet, and PubTables-1M. Our
findings reveal that SPRINT not only matches SOTA models in performance on
standard datasets but also demonstrates lower latency. Additionally, SPRINT
excels in accurately identifying table structures in non-English documents,
surpassing current leading models by showing an absolute average increase of
11.12%. We also present an algorithm for converting valid OTSL predictions into
a widely used HTML-based table representation. To encourage further research,
we release our code and Multilingual Scanned and Scene Table Structure
Recognition Dataset, MUSTARD labeled with OTSL sequences for 1428 tables in
thirteen languages encompassing several scripts at
https://github.com/IITB-LEAP-OCR/SPRINT
</details>

### [Design of an Expression Recognition Solution Employing the Global Channel-Spatial Attention Mechanism](https://arxiv.org/abs/2503.11935)
*Jun Yu,Yang Zheng,Lei Wang,Yongqi Wang,Shengfan Xu*
<details>
  <summary>Abstract</summary>
Facial expression recognition is a challenging classification task with broad
application prospects in the field of human - computer interaction. This paper
aims to introduce the methods of our upcoming 8th Affective Behavior Analysis
in the Wild (ABAW) competition to be held at CVPR2025. To address issues such
as low recognition accuracy caused by subtle expression changes and multi -
scales in facial expression recognition in videos, we propose global channel -
spatial attention and median - enhanced spatial - channel attention to
strengthen feature processing for speech and images respectively. Secondly, to
fully utilize the complementarity between the speech and facial expression
modalities, a speech - and - facial - expression key - frame alignment
technique is adopted to calculate the weights of speech and facial expressions.
These weights are input into the feature fusion layer for multi - scale dilated
fusion, which effectively improves the recognition rate of facial expression
recognition. In the facial expression recognition task of the 6th ABAW
competition, our method achieved excellent results on the official validation
set, which fully demonstrates the effectiveness and competitiveness of the
proposed method.
</details>

### [Att-Adapter: A Robust and Precise Domain-Specific Multi-Attributes T2I Diffusion Adapter via Conditional Variational Autoencoder](https://arxiv.org/abs/2503.11937)
*Wonwoong Cho,Yan-Ying Chen,Matthew Klenk,David I. Inouye,Yanxia Zhang*
<details>
  <summary>Abstract</summary>
Text-to-Image (T2I) Diffusion Models have achieved remarkable performance in
generating high quality images. However, enabling precise control of continuous
attributes, especially multiple attributes simultaneously, in a new domain
(e.g., numeric values like eye openness or car width) with text-only guidance
remains a significant challenge. To address this, we introduce the Attribute
(Att) Adapter, a novel plug-and-play module designed to enable fine-grained,
multi-attributes control in pretrained diffusion models. Our approach learns a
single control adapter from a set of sample images that can be unpaired and
contain multiple visual attributes. The Att-Adapter leverages the decoupled
cross attention module to naturally harmonize the multiple domain attributes
with text conditioning. We further introduce Conditional Variational
Autoencoder (CVAE) to the Att-Adapter to mitigate overfitting, matching the
diverse nature of the visual world. Evaluations on two public datasets show
that Att-Adapter outperforms all LoRA-based baselines in controlling continuous
attributes. Additionally, our method enables a broader control range and also
improves disentanglement across multiple attributes, surpassing StyleGAN-based
techniques. Notably, Att-Adapter is flexible, requiring no paired synthetic
data for training, and is easily scalable to multiple attributes within a
single model.
</details>

### [Your Text Encoder Can Be An Object-Level Watermarking Controller](https://arxiv.org/abs/2503.11945)
*Naresh Kumar Devulapally,Mingzhen Huang,Vishal Asnani,Shruti Agarwal,Siwei Lyu,Vishnu Suresh Lokhande*
<details>
  <summary>Abstract</summary>
Invisible watermarking of AI-generated images can help with copyright
protection, enabling detection and identification of AI-generated media. In
this work, we present a novel approach to watermark images of T2I Latent
Diffusion Models (LDMs). By only fine-tuning text token embeddings $W_*$, we
enable watermarking in selected objects or parts of the image, offering greater
flexibility compared to traditional full-image watermarking. Our method
leverages the text encoder's compatibility across various LDMs, allowing
plug-and-play integration for different LDMs. Moreover, introducing the
watermark early in the encoding stage improves robustness to adversarial
perturbations in later stages of the pipeline. Our approach achieves $99\%$ bit
accuracy ($48$ bits) with a $10^5 \times$ reduction in model parameters,
enabling efficient watermarking.
</details>

### [SPOC: Spatially-Progressing Object State Change Segmentation in Video](https://arxiv.org/abs/2503.11953)
*Priyanka Mandikal,Tushar Nagarajan,Alex Stoken,Zihui Xue,Kristen Grauman*
<details>
  <summary>Abstract</summary>
Object state changes in video reveal critical information about human and
agent activity. However, existing methods are limited to temporal localization
of when the object is in its initial state (e.g., the unchopped avocado) versus
when it has completed a state change (e.g., the chopped avocado), which limits
applicability for any task requiring detailed information about the progress of
the actions and its spatial localization. We propose to deepen the problem by
introducing the spatially-progressing object state change segmentation task.
The goal is to segment at the pixel-level those regions of an object that are
actionable and those that are transformed. We introduce the first model to
address this task, designing a VLM-based pseudo-labeling approach, state-change
dynamics constraints, and a novel WhereToChange benchmark built on in-the-wild
Internet videos. Experiments on two datasets validate both the challenge of the
new task as well as the promise of our model for localizing exactly where and
how fast objects are changing in video. We further demonstrate useful
implications for tracking activity progress to benefit robotic agents. Project
page: https://vision.cs.utexas.edu/projects/spoc-spatially-progressing-osc
</details>

### [CHOrD: Generation of Collision-Free, House-Scale, and Organized Digital Twins for 3D Indoor Scenes with Controllable Floor Plans and Optimal Layouts](https://arxiv.org/abs/2503.11958)
*Chong Su,Yingbin Fu,Zheyuan Hu,Jing Yang,Param Hanji,Shaojun Wang,Xuan Zhao,Cengiz √ñztireli,Fangcheng Zhong*
<details>
  <summary>Abstract</summary>
We introduce CHOrD, a novel framework for scalable synthesis of 3D indoor
scenes, designed to create house-scale, collision-free, and hierarchically
structured indoor digital twins. In contrast to existing methods that directly
synthesize the scene layout as a scene graph or object list, CHOrD incorporates
a 2D image-based intermediate layout representation, enabling effective
prevention of collision artifacts by successfully capturing them as
out-of-distribution (OOD) scenarios during generation. Furthermore, unlike
existing methods, CHOrD is capable of generating scene layouts that adhere to
complex floor plans with multi-modal controls, enabling the creation of
coherent, house-wide layouts robust to both geometric and semantic variations
in room structures. Additionally, we propose a novel dataset with expanded
coverage of household items and room configurations, as well as significantly
improved data quality. CHOrD demonstrates state-of-the-art performance on both
the 3D-FRONT and our proposed datasets, delivering photorealistic, spatially
coherent indoor scene synthesis adaptable to arbitrary floor plan variations.
</details>

### [Evaluation of Intra-operative Patient-specific Methods for Point Cloud Completion for Minimally Invasive Liver Interventions](https://arxiv.org/abs/2503.11969)
*Nakul Poudel,Zixin Yang,Kelly Merrell,Richard Simon,Cristian A. Linte*
<details>
  <summary>Abstract</summary>
The registration between the pre-operative model and the intra-operative
surface is crucial in image-guided liver surgery, as it facilitates the
effective use of pre-operative information during the procedure. However, the
intra-operative surface, usually represented as a point cloud, often has
limited coverage, especially in laparoscopic surgery, and is prone to holes and
noise, posing significant challenges for registration methods. Point cloud
completion methods have the potential to alleviate these issues. Thus, we
explore six state-of-the-art point cloud completion methods to identify the
optimal completion method for liver surgery applications. We focus on a
patient-specific approach for liver point cloud completion from a partial liver
surface under three cases: canonical pose, non-canonical pose, and canonical
pose with noise. The transformer-based method, AdaPoinTr, outperforms all other
methods to generate a complete point cloud from the given partial liver point
cloud under the canonical pose. On the other hand, our findings reveal
substantial performance degradation of these methods under non-canonical poses
and noisy settings, highlighting the limitations of these methods, which
suggests the need for a robust point completion method for its application in
image-guided liver surgery.
</details>

### [DynaGSLAM: Real-Time Gaussian-Splatting SLAM for Online Rendering, Tracking, Motion Predictions of Moving Objects in Dynamic Scenes](https://arxiv.org/abs/2503.11979)
*Runfa Blark Li,Mahdi Shaghaghi,Keito Suzuki,Xinshuang Liu,Varun Moparthi,Bang Du,Walker Curtis,Martin Renschler,Ki Myung Brian Lee,Nikolay Atanasov,Truong Nguyen*
<details>
  <summary>Abstract</summary>
Simultaneous Localization and Mapping (SLAM) is one of the most important
environment-perception and navigation algorithms for computer vision, robotics,
and autonomous cars/drones. Hence, high quality and fast mapping becomes a
fundamental problem. With the advent of 3D Gaussian Splatting (3DGS) as an
explicit representation with excellent rendering quality and speed,
state-of-the-art (SOTA) works introduce GS to SLAM. Compared to classical
pointcloud-SLAM, GS-SLAM generates photometric information by learning from
input camera views and synthesize unseen views with high-quality textures.
However, these GS-SLAM fail when moving objects occupy the scene that violate
the static assumption of bundle adjustment. The failed updates of moving GS
affects the static GS and contaminates the full map over long frames. Although
some efforts have been made by concurrent works to consider moving objects for
GS-SLAM, they simply detect and remove the moving regions from GS rendering
("anti'' dynamic GS-SLAM), where only the static background could benefit from
GS. To this end, we propose the first real-time GS-SLAM, "DynaGSLAM'', that
achieves high-quality online GS rendering, tracking, motion predictions of
moving objects in dynamic scenes while jointly estimating accurate ego motion.
Our DynaGSLAM outperforms SOTA static & "Anti'' dynamic GS-SLAM on three
dynamic real datasets, while keeping speed and memory efficiency in practice.
</details>

### [DecompDreamer: Advancing Structured 3D Asset Generation with Multi-Object Decomposition and Gaussian Splatting](https://arxiv.org/abs/2503.11981)
*Utkarsh Nath,Rajeev Goel,Rahul Khurana,Kyle Min,Mark Ollila,Pavan Turaga,Varun Jampani,Tejaswi Gowda*
<details>
  <summary>Abstract</summary>
Text-to-3D generation saw dramatic advances in recent years by leveraging
Text-to-Image models. However, most existing techniques struggle with
compositional prompts, which describe multiple objects and their spatial
relationships. They often fail to capture fine-grained inter-object
interactions. We introduce DecompDreamer, a Gaussian splatting-based training
routine designed to generate high-quality 3D compositions from such complex
prompts. DecompDreamer leverages Vision-Language Models (VLMs) to decompose
scenes into structured components and their relationships. We propose a
progressive optimization strategy that first prioritizes joint relationship
modeling before gradually shifting toward targeted object refinement. Our
qualitative and quantitative evaluations against state-of-the-art text-to-3D
models demonstrate that DecompDreamer effectively generates intricate 3D
compositions with superior object disentanglement, offering enhanced control
and flexibility in 3D generation. Project page :
https://decompdreamer3d.github.io
</details>

### [Fraesormer: Learning Adaptive Sparse Transformer for Efficient Food Recognition](https://arxiv.org/abs/2503.11995)
*Shun Zou,Yi Zou,Mingya Zhang,Shipeng Luo,Zhihao Chen,Guangwei Gao*
<details>
  <summary>Abstract</summary>
In recent years, Transformer has witnessed significant progress in food
recognition. However, most existing approaches still face two critical
challenges in lightweight food recognition: (1) the quadratic complexity and
redundant feature representation from interactions with irrelevant tokens; (2)
static feature recognition and single-scale representation, which overlook the
unstructured, non-fixed nature of food images and the need for multi-scale
features. To address these, we propose an adaptive and efficient sparse
Transformer architecture (Fraesormer) with two core designs: Adaptive Top-k
Sparse Partial Attention (ATK-SPA) and Hierarchical Scale-Sensitive Feature
Gating Network (HSSFGN). ATK-SPA uses a learnable Gated Dynamic Top-K Operator
(GDTKO) to retain critical attention scores, filtering low query-key matches
that hinder feature aggregation. It also introduces a partial channel mechanism
to reduce redundancy and promote expert information flow, enabling local-global
collaborative modeling. HSSFGN employs gating mechanism to achieve multi-scale
feature representation, enhancing contextual semantic information. Extensive
experiments show that Fraesormer outperforms state-of-the-art methods. code is
available at https://zs1314.github.io/Fraesormer.
</details>

### [3D Gaussian Splatting against Moving Objects for High-Fidelity Street Scene Reconstruction](https://arxiv.org/abs/2503.12001)
*Peizhen Zheng,Longfei Wei,Dongjing Jiang,Jianfei Zhang*
<details>
  <summary>Abstract</summary>
The accurate reconstruction of dynamic street scenes is critical for
applications in autonomous driving, augmented reality, and virtual reality.
Traditional methods relying on dense point clouds and triangular meshes
struggle with moving objects, occlusions, and real-time processing constraints,
limiting their effectiveness in complex urban environments. While multi-view
stereo and neural radiance fields have advanced 3D reconstruction, they face
challenges in computational efficiency and handling scene dynamics. This paper
proposes a novel 3D Gaussian point distribution method for dynamic street scene
reconstruction. Our approach introduces an adaptive transparency mechanism that
eliminates moving objects while preserving high-fidelity static scene details.
Additionally, iterative refinement of Gaussian point distribution enhances
geometric accuracy and texture representation. We integrate directional
encoding with spatial position optimization to optimize storage and rendering
efficiency, reducing redundancy while maintaining scene integrity. Experimental
results demonstrate that our method achieves high reconstruction quality,
improved rendering performance, and adaptability in large-scale dynamic
environments. These contributions establish a robust framework for real-time,
high-precision 3D reconstruction, advancing the practicality of dynamic scene
modeling across multiple applications. The source code for this work is
available to the public at https://github.com/deepcoxcom/3dgs
</details>

### [ROS-SAM: High-Quality Interactive Segmentation for Remote Sensing Moving Object](https://arxiv.org/abs/2503.12006)
*Zhe Shan,Yang Liu,Lei Zhou,Cheng Yan,Heng Wang,Xia Xie*
<details>
  <summary>Abstract</summary>
The availability of large-scale remote sensing video data underscores the
importance of high-quality interactive segmentation. However, challenges such
as small object sizes, ambiguous features, and limited generalization make it
difficult for current methods to achieve this goal. In this work, we propose
ROS-SAM, a method designed to achieve high-quality interactive segmentation
while preserving generalization across diverse remote sensing data. The ROS-SAM
is built upon three key innovations: 1) LoRA-based fine-tuning, which enables
efficient domain adaptation while maintaining SAM's generalization ability, 2)
Enhancement of deep network layers to improve the discriminability of extracted
features, thereby reducing misclassifications, and 3) Integration of global
context with local boundary details in the mask decoder to generate
high-quality segmentation masks. Additionally, we design the data pipeline to
ensure the model learns to better handle objects at varying scales during
training while focusing on high-quality predictions during inference.
Experiments on remote sensing video datasets show that the redesigned data
pipeline boosts the IoU by 6%, while ROS-SAM increases the IoU by 13%. Finally,
when evaluated on existing remote sensing object tracking datasets, ROS-SAM
demonstrates impressive zero-shot capabilities, generating masks that closely
resemble manual annotations. These results confirm ROS-SAM as a powerful tool
for fine-grained segmentation in remote sensing applications. Code is available
at https://github.com/ShanZard/ROS-SAM.
</details>

### [UniMamba: Unified Spatial-Channel Representation Learning with Group-Efficient Mamba for LiDAR-based 3D Object Detection](https://arxiv.org/abs/2503.12009)
*Xin Jin,Haisheng Su,Kai Liu,Cong Ma,Wei Wu,Fei Hui,Junchi Yan*
<details>
  <summary>Abstract</summary>
Recent advances in LiDAR 3D detection have demonstrated the effectiveness of
Transformer-based frameworks in capturing the global dependencies from point
cloud spaces, which serialize the 3D voxels into the flattened 1D sequence for
iterative self-attention. However, the spatial structure of 3D voxels will be
inevitably destroyed during the serialization process. Besides, due to the
considerable number of 3D voxels and quadratic complexity of Transformers,
multiple sequences are grouped before feeding to Transformers, leading to a
limited receptive field. Inspired by the impressive performance of State Space
Models (SSM) achieved in the field of 2D vision tasks, in this paper, we
propose a novel Unified Mamba (UniMamba), which seamlessly integrates the
merits of 3D convolution and SSM in a concise multi-head manner, aiming to
perform "local and global" spatial context aggregation efficiently and
simultaneously. Specifically, a UniMamba block is designed which mainly
consists of spatial locality modeling, complementary Z-order serialization and
local-global sequential aggregator. The spatial locality modeling module
integrates 3D submanifold convolution to capture the dynamic spatial position
embedding before serialization. Then the efficient Z-order curve is adopted for
serialization both horizontally and vertically. Furthermore, the local-global
sequential aggregator adopts the channel grouping strategy to efficiently
encode both "local and global" spatial inter-dependencies using multi-head SSM.
Additionally, an encoder-decoder architecture with stacked UniMamba blocks is
formed to facilitate multi-scale spatial learning hierarchically. Extensive
experiments are conducted on three popular datasets: nuScenes, Waymo and
Argoverse 2. Particularly, our UniMamba achieves 70.2 mAP on the nuScenes
dataset.
</details>

### [Learning Dual-Domain Multi-Scale Representations for Single Image Deraining](https://arxiv.org/abs/2503.12014)
*Shun Zou,Yi Zou,Mingya Zhang,Shipeng Luo,Guangwei Gao,Guojun Qi*
<details>
  <summary>Abstract</summary>
Existing image deraining methods typically rely on single-input,
single-output, and single-scale architectures, which overlook the joint
multi-scale information between external and internal features. Furthermore,
single-domain representations are often too restrictive, limiting their ability
to handle the complexities of real-world rain scenarios. To address these
challenges, we propose a novel Dual-Domain Multi-Scale Representation Network
(DMSR). The key idea is to exploit joint multi-scale representations from both
external and internal domains in parallel while leveraging the strengths of
both spatial and frequency domains to capture more comprehensive properties.
Specifically, our method consists of two main components: the Multi-Scale
Progressive Spatial Refinement Module (MPSRM) and the Frequency Domain Scale
Mixer (FDSM). The MPSRM enables the interaction and coupling of multi-scale
expert information within the internal domain using a hierarchical modulation
and fusion strategy. The FDSM extracts multi-scale local information in the
spatial domain, while also modeling global dependencies in the frequency
domain. Extensive experiments show that our model achieves state-of-the-art
performance across six benchmark datasets.
</details>

### [QDM: Quadtree-Based Region-Adaptive Sparse Diffusion Models for Efficient Image Super-Resolution](https://arxiv.org/abs/2503.12015)
*Donglin Yang,Paul Vicol,Xiaojuan Qi,Renjie Liao,Xiaofan Zhang*
<details>
  <summary>Abstract</summary>
Deep learning-based super-resolution (SR) methods often perform pixel-wise
computations uniformly across entire images, even in homogeneous regions where
high-resolution refinement is redundant. We propose the Quadtree Diffusion
Model (QDM), a region-adaptive diffusion framework that leverages a quadtree
structure to selectively enhance detail-rich regions while reducing
computations in homogeneous areas. By guiding the diffusion with a quadtree
derived from the low-quality input, QDM identifies key regions-represented by
leaf nodes-where fine detail is essential and applies minimal refinement
elsewhere. This mask-guided, two-stream architecture adaptively balances
quality and efficiency, producing high-fidelity outputs with low computational
redundancy. Experiments demonstrate QDM's effectiveness in high-resolution SR
tasks across diverse image types, particularly in medical imaging (e.g., CT
scans), where large homogeneous regions are prevalent. Furthermore, QDM
outperforms or is comparable to state-of-the-art SR methods on standard
benchmarks while significantly reducing computational costs, highlighting its
efficiency and suitability for resource-limited environments. Our code is
available at https://github.com/linYDTHU/QDM.
</details>

### [Compose Your Aesthetics: Empowering Text-to-Image Models with the Principles of Art](https://arxiv.org/abs/2503.12018)
*Zhe Jin,Tat-Seng Chua*
<details>
  <summary>Abstract</summary>
Text-to-Image (T2I) diffusion models (DM) have garnered widespread adoption
due to their capability in generating high-fidelity outputs and accessibility
to anyone able to put imagination into words. However, DMs are often
predisposed to generate unappealing outputs, much like the random images on the
internet they were trained on. Existing approaches to address this are founded
on the implicit premise that visual aesthetics is universal, which is limiting.
Aesthetics in the T2I context should be about personalization and we propose
the novel task of aesthetics alignment which seeks to align user-specified
aesthetics with the T2I generation output. Inspired by how artworks provide an
invaluable perspective to approach aesthetics, we codify visual aesthetics
using the compositional framework artists employ, known as the Principles of
Art (PoA). To facilitate this study, we introduce CompArt, a large-scale
compositional art dataset building on top of WikiArt with PoA analysis
annotated by a capable Multimodal LLM. Leveraging the expressive power of LLMs
and training a lightweight and transferrable adapter, we demonstrate that T2I
DMs can effectively offer 10 compositional controls through user-specified PoA
conditions. Additionally, we design an appropriate evaluation framework to
assess the efficacy of our approach.
</details>

### [SteerX: Creating Any Camera-Free 3D and 4D Scenes with Geometric Steering](https://arxiv.org/abs/2503.12024)
*Byeongjun Park,Hyojun Go,Hyelin Nam,Byung-Hoon Kim,Hyungjin Chung,Changick Kim*
<details>
  <summary>Abstract</summary>
Recent progress in 3D/4D scene generation emphasizes the importance of
physical alignment throughout video generation and scene reconstruction.
However, existing methods improve the alignment separately at each stage,
making it difficult to manage subtle misalignments arising from another stage.
Here, we present SteerX, a zero-shot inference-time steering method that
unifies scene reconstruction into the generation process, tilting data
distributions toward better geometric alignment. To this end, we introduce two
geometric reward functions for 3D/4D scene generation by using pose-free
feed-forward scene reconstruction models. Through extensive experiments, we
demonstrate the effectiveness of SteerX in improving 3D/4D scene generation.
</details>

### [Leveraging Motion Information for Better Self-Supervised Video Correspondence Learning](https://arxiv.org/abs/2503.12026)
*Zihan Zhoua,Changrui Daia,Aibo Songa,Xiaolin Fang*
<details>
  <summary>Abstract</summary>
Self-supervised video correspondence learning depends on the ability to
accurately associate pixels between video frames that correspond to the same
visual object. However, achieving reliable pixel matching without supervision
remains a major challenge. To address this issue, recent research has focused
on feature learning techniques that aim to encode unique pixel representations
for matching. Despite these advances, existing methods still struggle to
achieve exact pixel correspondences and often suffer from false matches,
limiting their effectiveness in self-supervised settings.
  To this end, we explore an efficient self-supervised Video Correspondence
Learning framework (MER) that aims to accurately extract object details from
unlabeled videos. First, we design a dedicated Motion Enhancement Engine that
emphasizes capturing the dynamic motion of objects in videos. In addition, we
introduce a flexible sampling strategy for inter-pixel correspondence
information (Multi-Cluster Sampler) that enables the model to pay more
attention to the pixel changes of important objects in motion. Through
experiments, our algorithm outperforms the state-of-the-art competitors on
video correspondence learning tasks such as video object segmentation and video
object keypoint tracking.
</details>

### [Challenges in Plane Symmetry: From Theory to Perception](https://arxiv.org/abs/2503.12028)
*F. √áengel,V. Adanova,S. Tari*
<details>
  <summary>Abstract</summary>
The planar ornaments are created by repeating a base unit using a combination
of four primitive geometric operations: translation, rotation, reflection, and
glide reflection. According to group theory, different combinations of these
four geometric operations lead to different symmetry groups. In this work, we
select a single challenging ornament, and analyze it both from the theoretical
point of view and perceptual point of view. We present the perceptual
experiment results, where one can see that the symmetries that the participants
perceived from the ornaments do not match to what the theory dictates.
</details>

### [Real-Time Manipulation Action Recognition with a Factorized Graph Sequence Encoder](https://arxiv.org/abs/2503.12034)
*Enes Erdogan,Eren Erdal Aksoy,Sanem Sariel*
<details>
  <summary>Abstract</summary>
Recognition of human manipulation actions in real-time is essential for safe
and effective human-robot interaction and collaboration. The challenge lies in
developing a model that is both lightweight enough for real-time execution and
capable of generalization. While some existing methods in the literature can
run in real-time, they struggle with temporal scalability, i.e., they fail to
adapt to long-duration manipulations effectively. To address this, leveraging
the generalizable scene graph representations, we propose a new Factorized
Graph Sequence Encoder network that not only runs in real-time but also scales
effectively in the temporal dimension, thanks to its factorized encoder
architecture. Additionally, we introduce Hand Pooling operation, a simple
pooling operation for more focused extraction of the graph-level embeddings.
Our model outperforms the previous state-of-the-art real-time approach,
achieving a 14.3\% and 5.6\% improvement in F1-macro score on the KIT Bimanual
Action (Bimacs) Dataset and Collaborative Action (CoAx) Dataset, respectively.
Moreover, we conduct an extensive ablation study to validate our network design
choices. Finally, we compare our model with its architecturally similar
RGB-based model on the Bimacs dataset and show the limitations of this model in
contrast to ours on such an object-centric manipulation dataset.
</details>

### [MOS: Modeling Object-Scene Associations in Generalized Category Discovery](https://arxiv.org/abs/2503.12035)
*Zhengyuan Peng,Jinpeng Ma,Zhimin Sun,Ran Yi,Haichuan Song,Xin Tan,Lizhuang Ma*
<details>
  <summary>Abstract</summary>
Generalized Category Discovery (GCD) is a classification task that aims to
classify both base and novel classes in unlabeled images, using knowledge from
a labeled dataset. In GCD, previous research overlooks scene information or
treats it as noise, reducing its impact during model training. However, in this
paper, we argue that scene information should be viewed as a strong prior for
inferring novel classes. We attribute the misinterpretation of scene
information to a key factor: the Ambiguity Challenge inherent in GCD.
Specifically, novel objects in base scenes might be wrongly classified into
base categories, while base objects in novel scenes might be mistakenly
recognized as novel categories. Once the ambiguity challenge is addressed,
scene information can reach its full potential, significantly enhancing the
performance of GCD models. To more effectively leverage scene information, we
propose the Modeling Object-Scene Associations (MOS) framework, which utilizes
a simple MLP-based scene-awareness module to enhance GCD performance. It
achieves an exceptional average accuracy improvement of 4% on the challenging
fine-grained datasets compared to state-of-the-art methods, emphasizing its
superior performance in fine-grained GCD. The code is publicly available at
https://github.com/JethroPeng/MOS.
</details>

### [PSGait: Multimodal Gait Recognition using Parsing Skeleton](https://arxiv.org/abs/2503.12047)
*Hangrui Xu,Chuanrui Zhang,Zhengxian Wu,Peng Jiao,Haoqian Wang*
<details>
  <summary>Abstract</summary>
Gait recognition has emerged as a robust biometric modality due to its
non-intrusive nature and resilience to occlusion. Conventional gait recognition
methods typically rely on silhouettes or skeletons. Despite their success in
gait recognition for controlled laboratory environments, they usually fail in
real-world scenarios due to their limited information entropy for gait
representations. To achieve accurate gait recognition in the wild, we propose a
novel gait representation, named Parsing Skeleton. This representation
innovatively introduces the skeleton-guided human parsing method to capture
fine-grained body dynamics, so they have much higher information entropy to
encode the shapes and dynamics of fine-grained human parts during walking.
Moreover, to effectively explore the capability of the parsing skeleton
representation, we propose a novel parsing skeleton-based gait recognition
framework, named PSGait, which takes parsing skeletons and silhouettes as
input. By fusing these two modalities, the resulting image sequences are fed
into gait recognition models for enhanced individual differentiation. We
conduct comprehensive benchmarks on various datasets to evaluate our model.
PSGait outperforms existing state-of-the-art multimodal methods. Furthermore,
as a plug-and-play method, PSGait leads to a maximum improvement of 10.9% in
Rank-1 accuracy across various gait recognition models. These results
demonstrate the effectiveness and versatility of parsing skeletons for gait
recognition in the wild, establishing PSGait as a new state-of-the-art approach
for multimodal gait recognition.
</details>

### [TACO: Taming Diffusion for in-the-wild Video Amodal Completion](https://arxiv.org/abs/2503.12049)
*Ruijie Lu,Yixin Chen,Yu Liu,Jiaxiang Tang,Junfeng Ni,Diwen Wan,Gang Zeng,Siyuan Huang*
<details>
  <summary>Abstract</summary>
Humans can infer complete shapes and appearances of objects from limited
visual cues, relying on extensive prior knowledge of the physical world.
However, completing partially observable objects while ensuring consistency
across video frames remains challenging for existing models, especially for
unstructured, in-the-wild videos. This paper tackles the task of Video Amodal
Completion (VAC), which aims to generate the complete object consistently
throughout the video given a visual prompt specifying the object of interest.
Leveraging the rich, consistent manifolds learned by pre-trained video
diffusion models, we propose a conditional diffusion model, TACO, that
repurposes these manifolds for VAC. To enable its effective and robust
generalization to challenging in-the-wild scenarios, we curate a large-scale
synthetic dataset with multiple difficulty levels by systematically imposing
occlusions onto un-occluded videos. Building on this, we devise a progressive
fine-tuning paradigm that starts with simpler recovery tasks and gradually
advances to more complex ones. We demonstrate TACO's versatility on a wide
range of in-the-wild videos from Internet, as well as on diverse, unseen
datasets commonly used in autonomous driving, robotic manipulation, and scene
understanding. Moreover, we show that TACO can be effectively applied to
various downstream tasks like object reconstruction and pose estimation,
highlighting its potential to facilitate physical world understanding and
reasoning. Our project page is available at https://jason-aplp.github.io/TACO.
</details>

### [Tailor: An Integrated Text-Driven CG-Ready Human and Garment Generation System](https://arxiv.org/abs/2503.12052)
*Zhiyao Sun,Yu-Hui Wen,Matthieu Lin,Ho-Jui Fang,Sheng Ye,Tian Lv,Yong-Jin Liu*
<details>
  <summary>Abstract</summary>
Creating detailed 3D human avatars with garments typically requires
specialized expertise and labor-intensive processes. Although recent advances
in generative AI have enabled text-to-3D human/clothing generation, current
methods fall short in offering accessible, integrated pipelines for producing
ready-to-use clothed avatars. To solve this, we introduce Tailor, an integrated
text-to-avatar system that generates high-fidelity, customizable 3D humans with
simulation-ready garments. Our system includes a three-stage pipeline. We first
employ a large language model to interpret textual descriptions into
parameterized body shapes and semantically matched garment templates. Next, we
develop topology-preserving deformation with novel geometric losses to adapt
garments precisely to body geometries. Furthermore, an enhanced texture
diffusion module with a symmetric local attention mechanism ensures both view
consistency and photorealistic details. Quantitative and qualitative
evaluations demonstrate that Tailor outperforms existing SoTA methods in terms
of fidelity, usability, and diversity. Code will be available for academic use.
</details>

### [EHNet: An Efficient Hybrid Network for Crowd Counting and Localization](https://arxiv.org/abs/2503.12061)
*Yuqing Yan,Yirui Wu*
<details>
  <summary>Abstract</summary>
In recent years, crowd counting and localization have become crucial
techniques in computer vision, with applications spanning various domains. The
presence of multi-scale crowd distributions within a single image remains a
fundamental challenge in crowd counting tasks. To address these challenges, we
introduce the Efficient Hybrid Network (EHNet), a novel framework for efficient
crowd counting and localization. By reformulating crowd counting into a point
regression framework, EHNet leverages the Spatial-Position Attention Module
(SPAM) to capture comprehensive spatial contexts and long-range dependencies.
Additionally, we develop an Adaptive Feature Aggregation Module (AFAM) to
effectively fuse and harmonize multi-scale feature representations. Building
upon these, we introduce the Multi-Scale Attentive Decoder (MSAD). Experimental
results on four benchmark datasets demonstrate that EHNet achieves competitive
performance with reduced computational overhead, outperforming existing methods
on ShanghaiTech Part \_A, ShanghaiTech Part \_B, UCF-CC-50, and UCF-QNRF. Our
code is in https://anonymous.4open.science/r/EHNet.
</details>

### [DLA-Count: Dynamic Label Assignment Network for Dense Cell Distribution Counting](https://arxiv.org/abs/2503.12063)
*Yuqing Yan,Yirui Wu*
<details>
  <summary>Abstract</summary>
Cell counting remains a fundamental yet challenging task in medical and
biological research due to the diverse morphology of cells, their dense
distribution, and variations in image quality. We present DLA-Count, a
breakthrough approach to cell counting that introduces three key innovations:
(1) K-adjacent Hungarian Matching (KHM), which dramatically improves cell
matching in dense regions, (2) Multi-scale Deformable Gaussian Convolution
(MDGC), which adapts to varying cell morphologies, and (3) Gaussian-enhanced
Feature Decoder (GFD) for efficient multi-scale feature fusion. Our extensive
experiments on four challenging cell counting datasets (ADI, MBM, VGG, and DCC)
demonstrate that our method outperforms previous methods across diverse
datasets, with improvements in Mean Absolute Error of up to 46.7\% on ADI and
42.5\% on MBM datasets. Our code is available at
https://anonymous.4open.science/r/DLA-Count.
</details>

### [A Comprehensive Survey on Knowledge Distillation](https://arxiv.org/abs/2503.12067)
*Amir M. Mansourian,Rozhan Ahmadi,Masoud Ghafouri,Amir Mohammad Babaei,Elaheh Badali Golezani,Zeynab Yasamani Ghamchi,Vida Ramezanian,Alireza Taherian,Kimia Dinashi,Amirali Miri,Shohreh Kasaei*
<details>
  <summary>Abstract</summary>
Deep Neural Networks (DNNs) have achieved notable performance in the fields
of computer vision and natural language processing with various applications in
both academia and industry. However, with recent advancements in DNNs and
transformer models with a tremendous number of parameters, deploying these
large models on edge devices causes serious issues such as high runtime and
memory consumption. This is especially concerning with the recent large-scale
foundation models, Vision-Language Models (VLMs), and Large Language Models
(LLMs). Knowledge Distillation (KD) is one of the prominent techniques proposed
to address the aforementioned problems using a teacher-student architecture.
More specifically, a lightweight student model is trained using additional
knowledge from a cumbersome teacher model. In this work, a comprehensive survey
of knowledge distillation methods is proposed. This includes reviewing KD from
different aspects: distillation sources, distillation schemes, distillation
algorithms, distillation by modalities, applications of distillation, and
comparison among existing methods. In contrast to most existing surveys, which
are either outdated or simply update former surveys, this work proposes a
comprehensive survey with a new point of view and representation structure that
categorizes and investigates the most recent methods in knowledge distillation.
This survey considers various critically important subcategories, including KD
for diffusion models, 3D inputs, foundational models, transformers, and LLMs.
Furthermore, existing challenges in KD and possible future research directions
are discussed. Github page of the project:
https://github.com/IPL-Sharif/KD_Survey
</details>

### [Prototype-Based Image Prompting for Weakly Supervised Histopathological Image Segmentation](https://arxiv.org/abs/2503.12068)
*Qingchen Tang,Lei Fan,Maurice Pagnucco,Yang Song*
<details>
  <summary>Abstract</summary>
Weakly supervised image segmentation with image-level labels has drawn
attention due to the high cost of pixel-level annotations. Traditional methods
using Class Activation Maps (CAMs) often highlight only the most discriminative
regions, leading to incomplete masks. Recent approaches that introduce textual
information struggle with histopathological images due to inter-class
homogeneity and intra-class heterogeneity. In this paper, we propose a
prototype-based image prompting framework for histopathological image
segmentation. It constructs an image bank from the training set using
clustering, extracting multiple prototype features per class to capture
intra-class heterogeneity. By designing a matching loss between input features
and class-specific prototypes using contrastive learning, our method addresses
inter-class homogeneity and guides the model to generate more accurate CAMs.
Experiments on four datasets (LUAD-HistoSeg, BCSS-WSSS, GCSS, and BCSS) show
that our method outperforms existing weakly supervised segmentation approaches,
setting new benchmarks in histopathological image segmentation.
</details>

### [Robust Dataset Distillation by Matching Adversarial Trajectories](https://arxiv.org/abs/2503.12069)
*Wei Lai,Tianyu Ding,ren dongdong,Lei Wang,Jing Huo,Yang Gao,Wenbin Li*
<details>
  <summary>Abstract</summary>
Dataset distillation synthesizes compact datasets that enable models to
achieve performance comparable to training on the original large-scale
datasets. However, existing distillation methods overlook the robustness of the
model, resulting in models that are vulnerable to adversarial attacks when
trained on distilled data. To address this limitation, we introduce the task of
``robust dataset distillation", a novel paradigm that embeds adversarial
robustness into the synthetic datasets during the distillation process. We
propose Matching Adversarial Trajectories (MAT), a method that integrates
adversarial training into trajectory-based dataset distillation. MAT
incorporates adversarial samples during trajectory generation to obtain robust
training trajectories, which are then used to guide the distillation process.
As experimentally demonstrated, even through natural training on our distilled
dataset, models can achieve enhanced adversarial robustness while maintaining
competitive accuracy compared to existing distillation methods. Our work
highlights robust dataset distillation as a new and important research
direction and provides a strong baseline for future research to bridge the gap
between efficient training and adversarial robustness.
</details>

### [V-Stylist: Video Stylization via Collaboration and Reflection of MLLM Agents](https://arxiv.org/abs/2503.12077)
*Zhengrong Yue,Shaobin Zhuang,Kunchang Li,Yanbo Ding,Yali Wang*
<details>
  <summary>Abstract</summary>
Despite the recent advancement in video stylization, most existing methods
struggle to render any video with complex transitions, based on an open style
description of user query. To fill this gap, we introduce a generic multi-agent
system for video stylization, V-Stylist, by a novel collaboration and
reflection paradigm of multi-modal large language models. Specifically, our
V-Stylist is a systematical workflow with three key roles: (1) Video Parser
decomposes the input video into a number of shots and generates their text
prompts of key shot content. Via a concise video-to-shot prompting paradigm, it
allows our V-Stylist to effectively handle videos with complex transitions. (2)
Style Parser identifies the style in the user query and progressively search
the matched style model from a style tree. Via a robust tree-of-thought
searching paradigm, it allows our V-Stylist to precisely specify vague style
preference in the open user query. (3) Style Artist leverages the matched model
to render all the video shots into the required style. Via a novel multi-round
self-reflection paradigm, it allows our V-Stylist to adaptively adjust detail
control, according to the style requirement. With such a distinct design of
mimicking human professionals, our V-Stylist achieves a major breakthrough over
the primary challenges for effective and automatic video stylization.
Moreover,we further construct a new benchmark Text-driven Video Stylization
Benchmark (TVSBench), which fills the gap to assess stylization of complex
videos on open user queries. Extensive experiments show that, V-Stylist
achieves the state-of-the-art, e.g.,V-Stylist surpasses FRESCO and ControlVideo
by 6.05% and 4.51% respectively in overall average metrics, marking a
significant advance in video stylization.
</details>

### [FA-BARF: Frequency Adapted Bundle-Adjusting Neural Radiance Fields](https://arxiv.org/abs/2503.12086)
*Rui Qian,Chenyangguang Zhang,Yan Di,Guangyao Zhai,Ruida Zhang,Jiayu Guo,Benjamin Busam,Jian Pu*
<details>
  <summary>Abstract</summary>
Neural Radiance Fields (NeRF) have exhibited highly effective performance for
photorealistic novel view synthesis recently. However, the key limitation it
meets is the reliance on a hand-crafted frequency annealing strategy to recover
3D scenes with imperfect camera poses. The strategy exploits a temporal
low-pass filter to guarantee convergence while decelerating the joint
optimization of implicit scene reconstruction and camera registration. In this
work, we introduce the Frequency Adapted Bundle Adjusting Radiance Field
(FA-BARF), substituting the temporal low-pass filter for a frequency-adapted
spatial low-pass filter to address the decelerating problem. We establish a
theoretical framework to interpret the relationship between position encoding
of NeRF and camera registration and show that our frequency-adapted filter can
mitigate frequency fluctuation caused by the temporal filter. Furthermore, we
show that applying a spatial low-pass filter in NeRF can optimize camera poses
productively through radial uncertainty overlaps among various views. Extensive
experiments show that FA-BARF can accelerate the joint optimization process
under little perturbations in object-centric scenes and recover real-world
scenes with unknown camera poses. This implies wider possibilities for NeRF
applied in dense 3D mapping and reconstruction under real-time requirements.
The code will be released upon paper acceptance.
</details>

### [Temporally Consistent Mitral Annulus Measurements from Sparse Annotations in Echocardiographic Videos](https://arxiv.org/abs/2503.12087)
*Gino E. Jansen,Mark J. Schuuring,Berto J. Bouma,Ivana I≈°gum*
<details>
  <summary>Abstract</summary>
This work presents a novel approach to achieving temporally consistent mitral
annulus landmark localization in echocardiography videos using sparse
annotations. Our method introduces a self-supervised loss term that enforces
temporal consistency between neighboring frames, which smooths the position of
landmarks and enhances measurement accuracy over time. Additionally, we
incorporate realistic field-of-view augmentations to improve the recognition of
missing anatomical landmarks. We evaluate our approach on both a public and
private dataset, and demonstrate significant improvements in Mitral Annular
Plane Systolic Excursion (MAPSE) calculations and overall landmark tracking
stability. The method achieves a mean absolute MAPSE error of 1.81 $\pm$ 0.14
mm, an annulus size error of 2.46 $\pm$ 0.31 mm, and a landmark localization
error of 2.48 $\pm$ 0.07 mm. Finally, it achieves a 0.99 ROC-AUC for
recognition of missing landmarks.
</details>

### [SFMNet: Sparse Focal Modulation for 3D Object Detection](https://arxiv.org/abs/2503.12093)
*Oren Shrout,Ayellet Tal*
<details>
  <summary>Abstract</summary>
We propose SFMNet, a novel 3D sparse detector that combines the efficiency of
sparse convolutions with the ability to model long-range dependencies. While
traditional sparse convolution techniques efficiently capture local structures,
they struggle with modeling long-range relationships. However, capturing
long-range dependencies is fundamental for 3D object detection. In contrast,
transformers are designed to capture these long-range dependencies through
attention mechanisms. But, they come with high computational costs, due to
their quadratic query-key-value interactions. Furthermore, directly applying
attention to non-empty voxels is inefficient due to the sparse nature of 3D
scenes. Our SFMNet is built on a novel Sparse Focal Modulation (SFM) module,
which integrates short- and long-range contexts with linear complexity by
leveraging a new hierarchical sparse convolution design. This approach enables
SFMNet to achieve high detection performance with improved efficiency, making
it well-suited for large-scale LiDAR scenes. We show that our detector achieves
state-of-the-art performance on autonomous driving datasets.
</details>

### [E-SAM: Training-Free Segment Every Entity Model](https://arxiv.org/abs/2503.12094)
*Weiming Zhang,Dingwen Xiao,Lei Chen,Lin Wang*
<details>
  <summary>Abstract</summary>
Entity Segmentation (ES) aims at identifying and segmenting distinct entities
within an image without the need for predefined class labels. This
characteristic makes ES well-suited to open-world applications with adaptation
to diverse and dynamically changing environments, where new and previously
unseen entities may appear frequently. Existing ES methods either require large
annotated datasets or high training costs, limiting their scalability and
adaptability. Recently, the Segment Anything Model (SAM), especially in its
Automatic Mask Generation (AMG) mode, has shown potential for holistic image
segmentation. However, it struggles with over-segmentation and
under-segmentation, making it less effective for ES. In this paper, we
introduce E-SAM, a novel training-free framework that exhibits exceptional ES
capability. Specifically, we first propose Multi-level Mask Generation (MMG)
that hierarchically processes SAM's AMG outputs to generate reliable
object-level masks while preserving fine details at other levels. Entity-level
Mask Refinement (EMR) then refines these object-level masks into accurate
entity-level masks. That is, it separates overlapping masks to address the
redundancy issues inherent in SAM's outputs and merges similar masks by
evaluating entity-level consistency. Lastly, Under-Segmentation Refinement
(USR) addresses under-segmentation by generating additional high-confidence
masks fused with EMR outputs to produce the final ES map. These three modules
are seamlessly optimized to achieve the best ES without additional training
overhead. Extensive experiments demonstrate that E-SAM achieves
state-of-the-art performance compared to prior ES methods, demonstrating a
significant improvement by +30.1 on benchmark metrics.
</details>

### [Towards Vision Zero: The Accid3nD Dataset](https://arxiv.org/abs/2503.12095)
*Walter Zimmer,Ross Greer,Daniel Lehmberg,Marc Pavel,Holger Caesar,Xingcheng Zhou,Ahmed Ghita,Mohan Trivedi,Rui Song,Hu Cao,Akshay Gopalkrishnan,Alois C. Knoll*
<details>
  <summary>Abstract</summary>
Even though a significant amount of work has been done to increase the safety
of transportation networks, accidents still occur regularly. They must be
understood as unavoidable and sporadic outcomes of traffic networks. No public
dataset contains 3D annotations of real-world accidents recorded from roadside
sensors. We present the Accid3nD dataset, a collection of real-world highway
accidents in different weather and lighting conditions. It contains vehicle
crashes at high-speed driving with 2,634,233 labeled 2D bounding boxes,
instance masks, and 3D bounding boxes with track IDs. In total, the dataset
contains 111,945 labeled frames recorded from four roadside cameras and LiDARs
at 25 Hz. The dataset contains six object classes and is provided in the
OpenLABEL format. We propose an accident detection model that combines a
rule-based approach with a learning-based one. Experiments and ablation studies
on our dataset show the robustness of our proposed method. The dataset, model,
and code are available on our website: https://accident-dataset.github.io.
</details>

### [O-TPT: Orthogonality Constraints for Calibrating Test-time Prompt Tuning in Vision-Language Models](https://arxiv.org/abs/2503.12096)
*Ashshak Sharifdeen,Muhammad Akhtar Munir,Sanoojan Baliah,Salman Khan,Muhammad Haris Khan*
<details>
  <summary>Abstract</summary>
Test-time prompt tuning for vision-language models (VLMs) is getting
attention because of their ability to learn with unlabeled data without
fine-tuning. Although test-time prompt tuning methods for VLMs can boost
accuracy, the resulting models tend to demonstrate poor calibration, which
casts doubts on the reliability and trustworthiness of these models. Notably,
more attention needs to be devoted to calibrating the test-time prompt tuning
in vision-language models. To this end, we propose a new approach, called O-TPT
that introduces orthogonality constraints on the textual features corresponding
to the learnable prompts for calibrating test-time prompt tuning in VLMs.
Towards introducing orthogonality constraints, we make the following
contributions. First, we uncover new insights behind the suboptimal calibration
performance of existing methods relying on textual feature dispersion. Second,
we show that imposing a simple orthogonalization of textual features is a more
effective approach towards obtaining textual dispersion. We conduct extensive
experiments on various datasets with different backbones and baselines. The
results indicate that our method consistently outperforms the prior state of
the art in significantly reducing the overall average calibration error. Also,
our method surpasses the zero-shot calibration performance on fine-grained
classification tasks.
</details>

### [A Speech-to-Video Synthesis Approach Using Spatio-Temporal Diffusion for Vocal Tract MRI](https://arxiv.org/abs/2503.12102)
*Paula Andrea P√©rez-Toro,Tom√°s Arias-Vergara,Fangxu Xing,Xiaofeng Liu,Maureen Stone,Jiachen Zhuo,Juan Rafael Orozco-Arroyave,Elmar N√∂th,Jana Hutter,Jerry L. Prince,Andreas Maier,Jonghye Woo*
<details>
  <summary>Abstract</summary>
Understanding the relationship between vocal tract motion during speech and
the resulting acoustic signal is crucial for aided clinical assessment and
developing personalized treatment and rehabilitation strategies. Toward this
goal, we introduce an audio-to-video generation framework for creating Real
Time/cine-Magnetic Resonance Imaging (RT-/cine-MRI) visuals of the vocal tract
from speech signals. Our framework first preprocesses RT-/cine-MRI sequences
and speech samples to achieve temporal alignment, ensuring synchronization
between visual and audio data. We then employ a modified stable diffusion
model, integrating structural and temporal blocks, to effectively capture
movement characteristics and temporal dynamics in the synchronized data. This
process enables the generation of MRI sequences from new speech inputs,
improving the conversion of audio into visual data. We evaluated our framework
on healthy controls and tongue cancer patients by analyzing and comparing the
vocal tract movements in synthesized videos. Our framework demonstrated
adaptability to new speech inputs and effective generalization. In addition,
positive human evaluations confirmed its effectiveness, with realistic and
accurate visualizations, suggesting its potential for outpatient therapy and
personalized simulation of vocal tract visualizations.
</details>

### [Z-Magic: Zero-shot Multiple Attributes Guided Image Creator](https://arxiv.org/abs/2503.12124)
*Yingying Deng,Xiangyu He,Fan Tang,Weiming Dong*
<details>
  <summary>Abstract</summary>
The customization of multiple attributes has gained popularity with the
rising demand for personalized content creation. Despite promising empirical
results, the contextual coherence between different attributes has been largely
overlooked. In this paper, we argue that subsequent attributes should follow
the multivariable conditional distribution introduced by former attribute
creation. In light of this, we reformulate multi-attribute creation from a
conditional probability theory perspective and tackle the challenging zero-shot
setting. By explicitly modeling the dependencies between attributes, we further
enhance the coherence of generated images across diverse attribute
combinations. Furthermore, we identify connections between multi-attribute
customization and multi-task learning, effectively addressing the high
computing cost encountered in multi-attribute synthesis. Extensive experiments
demonstrate that Z-Magic outperforms existing models in zero-shot image
generation, with broad implications for AI-driven design and creative
applications.
</details>

### [Hyperbolic Safety-Aware Vision-Language Models](https://arxiv.org/abs/2503.12127)
*Tobia Poppi,Tejaswi Kasarla,Pascal Mettes,Lorenzo Baraldi,Rita Cucchiara*
<details>
  <summary>Abstract</summary>
Addressing the retrieval of unsafe content from vision-language models such
as CLIP is an important step towards real-world integration. Current efforts
have relied on unlearning techniques that try to erase the model's knowledge of
unsafe concepts. While effective in reducing unwanted outputs, unlearning
limits the model's capacity to discern between safe and unsafe content. In this
work, we introduce a novel approach that shifts from unlearning to an awareness
paradigm by leveraging the inherent hierarchical properties of the hyperbolic
space. We propose to encode safe and unsafe content as an entailment hierarchy,
where both are placed in different regions of hyperbolic space. Our HySAC,
Hyperbolic Safety-Aware CLIP, employs entailment loss functions to model the
hierarchical and asymmetrical relations between safe and unsafe image-text
pairs. This modelling, ineffective in standard vision-language models due to
their reliance on Euclidean embeddings, endows the model with awareness of
unsafe content, enabling it to serve as both a multimodal unsafe classifier and
a flexible content retriever, with the option to dynamically redirect unsafe
queries toward safer alternatives or retain the original output. Extensive
experiments show that our approach not only enhances safety recognition but
also establishes a more adaptable and interpretable framework for content
moderation in vision-language models. Our source code is available at
https://github.com/aimagelab/HySAC.
</details>

### [DiffGAP: A Lightweight Diffusion Module in Contrastive Space for Bridging Cross-Model Gap](https://arxiv.org/abs/2503.12131)
*Shentong Mo,Zehua Chen,Fan Bao,Jun Zhu*
<details>
  <summary>Abstract</summary>
Recent works in cross-modal understanding and generation, notably through
models like CLAP (Contrastive Language-Audio Pretraining) and CAVP (Contrastive
Audio-Visual Pretraining), have significantly enhanced the alignment of text,
video, and audio embeddings via a single contrastive loss. However, these
methods often overlook the bidirectional interactions and inherent noises
present in each modality, which can crucially impact the quality and efficacy
of cross-modal integration. To address this limitation, we introduce DiffGAP, a
novel approach incorporating a lightweight generative module within the
contrastive space. Specifically, our DiffGAP employs a bidirectional diffusion
process tailored to bridge the cross-modal gap more effectively. This involves
a denoising process on text and video embeddings conditioned on audio
embeddings and vice versa, thus facilitating a more nuanced and robust
cross-modal interaction. Our experimental results on VGGSound and AudioCaps
datasets demonstrate that DiffGAP significantly improves performance in
video/text-audio generation and retrieval tasks, confirming its effectiveness
in enhancing cross-modal understanding and generation capabilities.
</details>

### [Point-Cache: Test-time Dynamic and Hierarchical Cache for Robust and Generalizable Point Cloud Analysis](https://arxiv.org/abs/2503.12150)
*Hongyu Sun,Qiuhong Ke,Ming Cheng,Yongcai Wang,Deying Li,Chenhui Gou,Jianfei Cai*
<details>
  <summary>Abstract</summary>
This paper proposes a general solution to enable point cloud recognition
models to handle distribution shifts at test time. Unlike prior methods, which
rely heavily on training data-often inaccessible during online inference-and
are limited to recognizing a fixed set of point cloud classes predefined during
training, we explore a more practical and challenging scenario: adapting the
model solely based on online test data to recognize both previously seen
classes and novel, unseen classes at test time. To this end, we develop
Point-Cache, a hierarchical cache model that captures essential clues of online
test samples, particularly focusing on the global structure of point clouds and
their local-part details. Point-Cache, which serves as a rich 3D knowledge
base, is dynamically managed to prioritize the inclusion of high-quality
samples. Designed as a plug-and-play module, our method can be flexibly
integrated into large multimodal 3D models to support open-vocabulary point
cloud recognition. Notably, our solution operates with efficiency comparable to
zero-shot inference, as it is entirely training-free. Point-Cache demonstrates
substantial gains across 8 challenging benchmarks and 4 representative large 3D
models, highlighting its effectiveness. Code is available at
https://github.com/auniquesun/Point-Cache.
</details>

### [VTON 360: High-Fidelity Virtual Try-On from Any Viewing Direction](https://arxiv.org/abs/2503.12165)
*Zijian He,Yuwei Ning,Yipeng Qin,Wangrun Wang,Sibei Yang,Liang Lin,Guanbin Li*
<details>
  <summary>Abstract</summary>
Virtual Try-On (VTON) is a transformative technology in e-commerce and
fashion design, enabling realistic digital visualization of clothing on
individuals. In this work, we propose VTON 360, a novel 3D VTON method that
addresses the open challenge of achieving high-fidelity VTON that supports
any-view rendering. Specifically, we leverage the equivalence between a 3D
model and its rendered multi-view 2D images, and reformulate 3D VTON as an
extension of 2D VTON that ensures 3D consistent results across multiple views.
To achieve this, we extend 2D VTON models to include multi-view garments and
clothing-agnostic human body images as input, and propose several novel
techniques to enhance them, including: i) a pseudo-3D pose representation using
normal maps derived from the SMPL-X 3D human model, ii) a multi-view spatial
attention mechanism that models the correlations between features from
different viewing angles, and iii) a multi-view CLIP embedding that enhances
the garment CLIP features used in 2D VTON with camera information. Extensive
experiments on large-scale real datasets and clothing images from e-commerce
platforms demonstrate the effectiveness of our approach. Project page:
https://scnuhealthy.github.io/VTON360.
</details>

### [Learning Extremely High Density Crowds as Active Matters](https://arxiv.org/abs/2503.12168)
*Feixiang He,Jiangbei Yue,Jialin Zhu,Armin Seyfried,Dan Casas,Julien Pettr√©,He Wang*
<details>
  <summary>Abstract</summary>
Video-based high-density crowd analysis and prediction has been a
long-standing topic in computer vision. It is notoriously difficult due to, but
not limited to, the lack of high-quality data and complex crowd dynamics.
Consequently, it has been relatively under studied. In this paper, we propose a
new approach that aims to learn from in-the-wild videos, often with low quality
where it is difficult to track individuals or count heads. The key novelty is a
new physics prior to model crowd dynamics. We model high-density crowds as
active matter, a continumm with active particles subject to stochastic forces,
named 'crowd material'. Our physics model is combined with neural networks,
resulting in a neural stochastic differential equation system which can mimic
the complex crowd dynamics. Due to the lack of similar research, we adapt a
range of existing methods which are close to ours for comparison. Through
exhaustive evaluation, we show our model outperforms existing methods in
analyzing and forecasting extremely high-density crowds. Furthermore, since our
model is a continuous-time physics model, it can be used for simulation and
analysis, providing strong interpretability. This is categorically different
from most deep learning methods, which are discrete-time models and
black-boxes.
</details>

### [LAPIG: Language Guided Projector Image Generation with Surface Adaptation and Stylization](https://arxiv.org/abs/2503.12173)
*Yuchen Deng,Haibin Ling,Bingyao Huang*
<details>
  <summary>Abstract</summary>
We propose LAPIG, a language guided projector image generation method with
surface adaptation and stylization. LAPIG consists of a projector-camera system
and a target textured projection surface. LAPIG takes the user text prompt as
input and aims to transform the surface style using the projector. LAPIG's key
challenge is that due to the projector's physical brightness limitation and the
surface texture, the viewer's perceived projection may suffer from color
saturation and artifacts in both dark and bright regions, such that even with
the state-of-the-art projector compensation techniques, the viewer may see
clear surface texture-related artifacts. Therefore, how to generate a projector
image that follows the user's instruction while also displaying minimum surface
artifacts is an open problem. To address this issue, we propose projection
surface adaptation (PSA) that can generate compensable surface stylization. We
first train two networks to simulate the projector compensation and
project-and-capture processes, this allows us to find a satisfactory projector
image without real project-and-capture and utilize gradient descent for fast
convergence. Then, we design content and saturation losses to guide the
projector image generation, such that the generated image shows no clearly
perceivable artifacts when projected. Finally, the generated image is projected
for visually pleasing surface style morphing effects. The source code and video
are available on the project page: https://Yu-chen-Deng.github.io/LAPIG/.
</details>

### [Breaking the Box: Enhancing Remote Sensing Image Segmentation with Freehand Sketches](https://arxiv.org/abs/2503.12191)
*Ying Zang,Yuncan Gao,Jiangi Zhang,Yuangi Hu,Runlong Cao,Lanyun Zhu,Qi Zhu,Deyi Ji,Renjun Xu,Tianrun Chen*
<details>
  <summary>Abstract</summary>
This work advances zero-shot interactive segmentation for remote sensing
imagery through three key contributions. First, we propose a novel sketch-based
prompting method, enabling users to intuitively outline objects, surpassing
traditional point or box prompts. Second, we introduce LTL-Sensing, the first
dataset pairing human sketches with remote sensing imagery, setting a benchmark
for future research. Third, we present LTL-Net, a model featuring a multi-input
prompting transport module tailored for freehand sketches. Extensive
experiments show our approach significantly improves segmentation accuracy and
robustness over state-of-the-art methods like SAM, fostering more intuitive
human-AI collaboration in remote sensing analysis and enhancing its
applications.
</details>

### [S2IL: Structurally Stable Incremental Learning](https://arxiv.org/abs/2503.12193)
*S Balasubramanian,Yedu Krishna P,Talasu Sai Sriram,M Sai Subramaniam,Manepalli Pranav Phanindra Sai,Darshan Gera*
<details>
  <summary>Abstract</summary>
Feature Distillation (FD) strategies are proven to be effective in mitigating
Catastrophic Forgetting (CF) seen in Class Incremental Learning (CIL). However,
current FD approaches enforce strict alignment of feature magnitudes and
directions across incremental steps, limiting the model's ability to adapt to
new knowledge. In this paper we propose Structurally Stable Incremental
Learning(S22IL), a FD method for CIL that mitigates CF by focusing on
preserving the overall spatial patterns of features which promote flexible
(plasticity) yet stable representations that preserve old knowledge
(stability). We also demonstrate that our proposed method S2IL achieves strong
incremental accuracy and outperforms other FD methods on SOTA benchmark
datasets CIFAR-100, ImageNet-100 and ImageNet-1K. Notably, S2IL outperforms
other methods by a significant margin in scenarios that have a large number of
incremental tasks.
</details>

### [TLAC: Two-stage LMM Augmented CLIP for Zero-Shot Classification](https://arxiv.org/abs/2503.12206)
*Ans Munir,Faisal Z. Qureshi,Muhammad Haris Khan,Mohsen Ali*
<details>
  <summary>Abstract</summary>
Contrastive Language-Image Pretraining (CLIP) has shown impressive zero-shot
performance on image classification. However, state-of-the-art methods often
rely on fine-tuning techniques like prompt learning and adapter-based tuning to
optimize CLIP's performance. The necessity for fine-tuning significantly limits
CLIP's adaptability to novel datasets and domains. This requirement mandates
substantial time and computational resources for each new dataset. To overcome
this limitation, we introduce simple yet effective training-free approaches,
Single-stage LMM Augmented CLIP (SLAC) and Two-stage LMM Augmented CLIP (TLAC),
that leverages powerful Large Multimodal Models (LMMs), such as Gemini, for
image classification. The proposed methods leverages the capabilities of
pre-trained LMMs, allowing for seamless adaptation to diverse datasets and
domains without the need for additional training. Our approaches involve
prompting the LMM to identify objects within an image. Subsequently, the CLIP
text encoder determines the image class by identifying the dataset class with
the highest semantic similarity to the LLM predicted object. We evaluated our
models on 11 base-to-novel datasets and they achieved superior accuracy on 9 of
these, including benchmarks like ImageNet, SUN397 and Caltech101, while
maintaining a strictly training-free paradigm. Our overall accuracy of 83.44%
surpasses the previous state-of-the-art few-shot methods by a margin of 6.75%.
Our method achieved 83.6% average accuracy across 13 datasets, a 9.7%
improvement over the previous 73.9% state-of-the-art for training-free
approaches. Our method improves domain generalization, with a 3.6% gain on
ImageNetV2, 16.96% on ImageNet-S, and 12.59% on ImageNet-R, over prior few-shot
methods.
</details>

### [STAY Diffusion: Styled Layout Diffusion Model for Diverse Layout-to-Image Generation](https://arxiv.org/abs/2503.12213)
*Ruyu Wang,Xuefeng Hou,Sabrina Schmedding,Marco F. Huber*
<details>
  <summary>Abstract</summary>
In layout-to-image (L2I) synthesis, controlled complex scenes are generated
from coarse information like bounding boxes. Such a task is exciting to many
downstream applications because the input layouts offer strong guidance to the
generation process while remaining easily reconfigurable by humans. In this
paper, we proposed STyled LAYout Diffusion (STAY Diffusion), a diffusion-based
model that produces photo-realistic images and provides fine-grained control of
stylized objects in scenes. Our approach learns a global condition for each
layout, and a self-supervised semantic map for weight modulation using a novel
Edge-Aware Normalization (EA Norm). A new Styled-Mask Attention (SM Attention)
is also introduced to cross-condition the global condition and image feature
for capturing the objects' relationships. These measures provide consistent
guidance through the model, enabling more accurate and controllable image
generation. Extensive benchmarking demonstrates that our STAY Diffusion
presents high-quality images while surpassing previous state-of-the-art methods
in generation diversity, accuracy, and controllability.
</details>

### [Gun Detection Using Combined Human Pose and Weapon Appearance](https://arxiv.org/abs/2503.12215)
*Amulya Reddy Maligireddy,Manohar Reddy Uppula,Nidhi Rastogi,Yaswanth Reddy Parla*
<details>
  <summary>Abstract</summary>
The increasing frequency of firearm-related incidents has necessitated
advancements in security and surveillance systems, particularly in firearm
detection within public spaces. Traditional gun detection methods rely on
manual inspections and continuous human monitoring of CCTV footage, which are
labor-intensive and prone to high false positive and negative rates. To address
these limitations, we propose a novel approach that integrates human pose
estimation with weapon appearance recognition using deep learning techniques.
Unlike prior studies that focus on either body pose estimation or firearm
detection in isolation, our method jointly analyzes posture and weapon presence
to enhance detection accuracy in real-world, dynamic environments. To train our
model, we curated a diverse dataset comprising images from open-source
repositories such as IMFDB and Monash Guns, supplemented with AI-generated and
manually collected images from web sources. This dataset ensures robust
generalization and realistic performance evaluation under various surveillance
conditions. Our research aims to improve the precision and reliability of
firearm detection systems, contributing to enhanced public safety and threat
mitigation in high-risk areas.
</details>

### [Adaptive Label Correction for Robust Medical Image Segmentation with Noisy Labels](https://arxiv.org/abs/2503.12218)
*Chengxuan Qian,Kai Han,Siqi Ma,Chongwen Lyu,Zhenlong Yuan,Jun Chen,Zhe Liu*
<details>
  <summary>Abstract</summary>
Deep learning has shown remarkable success in medical image analysis, but its
reliance on large volumes of high-quality labeled data limits its
applicability. While noisy labeled data are easier to obtain, directly
incorporating them into training can degrade model performance. To address this
challenge, we propose a Mean Teacher-based Adaptive Label Correction (ALC)
self-ensemble framework for robust medical image segmentation with noisy
labels. The framework leverages the Mean Teacher architecture to ensure
consistent learning under noise perturbations. It includes an adaptive label
refinement mechanism that dynamically captures and weights differences across
multiple disturbance versions to enhance the quality of noisy labels.
Additionally, a sample-level uncertainty-based label selection algorithm is
introduced to prioritize high-confidence samples for network updates,
mitigating the impact of noisy annotations. Consistency learning is integrated
to align the predictions of the student and teacher networks, further enhancing
model robustness. Extensive experiments on two public datasets demonstrate the
effectiveness of the proposed framework, showing significant improvements in
segmentation performance. By fully exploiting the strengths of the Mean Teacher
structure, the ALC framework effectively processes noisy labels, adapts to
challenging scenarios, and achieves competitive results compared to
state-of-the-art methods.
</details>

### [LIAM: Multimodal Transformer for Language Instructions, Images, Actions and Semantic Maps](https://arxiv.org/abs/2503.12230)
*Yihao Wang,Raphael Memmesheimer,Sven Behnke*
<details>
  <summary>Abstract</summary>
The availability of large language models and open-vocabulary object
perception methods enables more flexibility for domestic service robots. The
large variability of domestic tasks can be addressed without implementing each
task individually by providing the robot with a task description along with
appropriate environment information. In this work, we propose LIAM - an
end-to-end model that predicts action transcripts based on language, image,
action, and map inputs. Language and image inputs are encoded with a CLIP
backbone, for which we designed two pre-training tasks to fine-tune its weights
and pre-align the latent spaces. We evaluate our method on the ALFRED dataset,
a simulator-generated benchmark for domestic tasks. Our results demonstrate the
importance of pre-aligning embedding spaces from different modalities and the
efficacy of incorporating semantic maps.
</details>

### [From Laboratory to Real World: A New Benchmark Towards Privacy-Preserved Visible-Infrared Person Re-Identification](https://arxiv.org/abs/2503.12232)
*Yan Jiang,Hao Yu,Xu Cheng,Haoyu Chen,Zhaodong Sun,Guoying Zhao*
<details>
  <summary>Abstract</summary>
Aiming to match pedestrian images captured under varying lighting conditions,
visible-infrared person re-identification (VI-ReID) has drawn intensive
research attention and achieved promising results. However, in real-world
surveillance contexts, data is distributed across multiple devices/entities,
raising privacy and ownership concerns that make existing centralized training
impractical for VI-ReID. To tackle these challenges, we propose L2RW, a
benchmark that brings VI-ReID closer to real-world applications. The rationale
of L2RW is that integrating decentralized training into VI-ReID can address
privacy concerns in scenarios with limited data-sharing regulation.
Specifically, we design protocols and corresponding algorithms for different
privacy sensitivity levels. In our new benchmark, we ensure the model training
is done in the conditions that: 1) data from each camera remains completely
isolated, or 2) different data entities (e.g., data controllers of a certain
region) can selectively share the data. In this way, we simulate scenarios with
strict privacy constraints which is closer to real-world conditions. Intensive
experiments with various server-side federated algorithms are conducted,
showing the feasibility of decentralized VI-ReID training. Notably, when
evaluated in unseen domains (i.e., new data entities), our L2RW, trained with
isolated data (privacy-preserved), achieves performance comparable to SOTAs
trained with shared data (privacy-unrestricted). We hope this work offers a
novel research entry for deploying VI-ReID that fits real-world scenarios and
can benefit the community.
</details>

### [RePerformer: Immersive Human-centric Volumetric Videos from Playback to Photoreal Reperformance](https://arxiv.org/abs/2503.12242)
*Yuheng Jiang,Zhehao Shen,Chengcheng Guo,Yu Hong,Zhuo Su,Yingliang Zhang,Marc Habermann,Lan Xu*
<details>
  <summary>Abstract</summary>
Human-centric volumetric videos offer immersive free-viewpoint experiences,
yet existing methods focus either on replaying general dynamic scenes or
animating human avatars, limiting their ability to re-perform general dynamic
scenes. In this paper, we present RePerformer, a novel Gaussian-based
representation that unifies playback and re-performance for high-fidelity
human-centric volumetric videos. Specifically, we hierarchically disentangle
the dynamic scenes into motion Gaussians and appearance Gaussians which are
associated in the canonical space. We further employ a Morton-based
parameterization to efficiently encode the appearance Gaussians into 2D
position and attribute maps. For enhanced generalization, we adopt 2D CNNs to
map position maps to attribute maps, which can be assembled into appearance
Gaussians for high-fidelity rendering of the dynamic scenes. For
re-performance, we develop a semantic-aware alignment module and apply
deformation transfer on motion Gaussians, enabling photo-real rendering under
novel motions. Extensive experiments validate the robustness and effectiveness
of RePerformer, setting a new benchmark for playback-then-reperformance
paradigm in human-centric volumetric videos.
</details>

### [Minuscule Cell Detection in AS-OCT Images with Progressive Field-of-View Focusing](https://arxiv.org/abs/2503.12249)
*Boyu Chen,Ameenat L. Solebo,Daqian Shi,Jinge Wu,Paul Taylor*
<details>
  <summary>Abstract</summary>
Anterior Segment Optical Coherence Tomography (AS-OCT) is an emerging imaging
technique with great potential for diagnosing anterior uveitis, a
vision-threatening ocular inflammatory condition. A hallmark of this condition
is the presence of inflammatory cells in the eye's anterior chamber, and
detecting these cells using AS-OCT images has attracted research interest.
While recent efforts aim to replace manual cell detection with automated
computer vision approaches, detecting extremely small (minuscule) objects in
high-resolution images, such as AS-OCT, poses substantial challenges: (1) each
cell appears as a minuscule particle, representing less than 0.005\% of the
image, making the detection difficult, and (2) OCT imaging introduces
pixel-level noise that can be mistaken for cells, leading to false positive
detections. To overcome these challenges, we propose a minuscule cell detection
framework through a progressive field-of-view focusing strategy. This strategy
systematically refines the detection scope from the whole image to a target
region where cells are likely to be present, and further to minuscule regions
potentially containing individual cells. Our framework consists of two modules.
First, a Field-of-Focus module uses a vision foundation model to segment the
target region. Subsequently, a Fine-grained Object Detection module introduces
a specialized Minuscule Region Proposal followed by a Spatial Attention Network
to distinguish individual cells from noise within the segmented region.
Experimental results demonstrate that our framework outperforms
state-of-the-art methods for cell detection, providing enhanced efficacy for
clinical applications. Our code is publicly available at:
https://github.com/joeybyc/MCD.
</details>

### [Enhancing Facial Expression Recognition through Dual-Direction Attention Mixed Feature Networks and CLIP: Application to 8th ABAW Challenge](https://arxiv.org/abs/2503.12260)
*Josep Cabacas-Maso,Elena Ortega-Beltr√°n,Ismael Benito-Altamirano,Carles Ventura*
<details>
  <summary>Abstract</summary>
We present our contribution to the 8th ABAW challenge at CVPR 2025, where we
tackle valence-arousal estimation, emotion recognition, and facial action unit
detection as three independent challenges. Our approach leverages the
well-known Dual-Direction Attention Mixed Feature Network (DDAMFN) for all
three tasks, achieving results that surpass the proposed baselines.
Additionally, we explore the use of CLIP for the emotion recognition challenge
as an additional experiment. We provide insights into the architectural choices
that contribute to the strong performance of our methods.
</details>

### [Handling Weak Complementary Relationships for Audio-Visual Emotion Recognition](https://arxiv.org/abs/2503.12261)
*R. Gnana Praveen,Jahangir Alam*
<details>
  <summary>Abstract</summary>
Multimodal emotion recognition has recently drawn a lot of interest in
affective computing as it has immense potential to outperform isolated unimodal
approaches. Audio and visual modalities are two predominant contact-free
channels in videos, which are often expected to carry a complementary
relationship with each other. However, audio and visual channels may not always
be complementary with each other, resulting in poor audio-visual feature
representations, thereby degrading the performance of the system. In this
paper, we propose a flexible audio-visual fusion model that can adapt to weak
complementary relationships using a gated attention mechanism. Specifically, we
extend the recursive joint cross-attention model by introducing gating
mechanism in every iteration to control the flow of information between the
input features and the attended features depending on the strength of their
complementary relationship. For instance, if the modalities exhibit strong
complementary relationships, the gating mechanism chooses cross-attended
features, otherwise non-attended features. To further improve the performance
of the system, we further introduce stage gating mechanism, which is used to
control the flow of information across the gated outputs of each iteration.
Therefore, the proposed model improves the performance of the system even when
the audio and visual modalities do not have a strong complementary relationship
with each other by adding more flexibility to the recursive joint cross
attention mechanism. The proposed model has been evaluated on the challenging
Affwild2 dataset and significantly outperforms the state-of-the-art fusion
approaches.
</details>

### [An Efficient Deep Learning-Based Approach to Automating Invoice Document Validation](https://arxiv.org/abs/2503.12267)
*Aziz Amari,Mariem Makni,Wissal Fnaich,Akram Lahmar,Fedi Koubaa,Oumayma Charrad,Mohamed Ali Zormati,Rabaa Youssef Douss*
<details>
  <summary>Abstract</summary>
In large organizations, the number of financial transactions can grow
rapidly, driving the need for fast and accurate multi-criteria invoice
validation. Manual processing remains error-prone and time-consuming, while
current automated solutions are limited by their inability to support a variety
of constraints, such as documents that are partially handwritten or
photographed with a mobile phone. In this paper, we propose to automate the
validation of machine written invoices using document layout analysis and
object detection techniques based on recent deep learning (DL) models. We
introduce a novel dataset consisting of manually annotated real-world invoices
and a multi-criteria validation process. We fine-tune and benchmark the most
relevant DL models on our dataset. Experimental results show the effectiveness
of the proposed pipeline and selected DL models in terms of achieving fast and
accurate validation of invoices.
</details>

### [Cracking the PUMA Challenge in 24 Hours with CellViT++ and nnU-Net](https://arxiv.org/abs/2503.12269)
*Negar Shahamiri,Moritz Rempe,Lukas Heine,Jens Kleesiek,Fabian H√∂rst*
<details>
  <summary>Abstract</summary>
Automatic tissue segmentation and nuclei detection is an important task in
pathology, aiding in biomarker extraction and discovery. The panoptic
segmentation of nuclei and tissue in advanced melanoma (PUMA) challenge aims to
improve tissue segmentation and nuclei detection in melanoma histopathology.
Unlike many challenge submissions focusing on extensive model tuning, our
approach emphasizes delivering a deployable solution within a 24-hour
development timeframe, using out-of-the-box frameworks. The pipeline combines
two models, namely CellViT++ for nuclei detection and nnU-Net for tissue
segmentation. Our results demonstrate a significant improvement in tissue
segmentation, achieving a Dice score of 0.750, surpassing the baseline score of
0.629. For nuclei detection, we obtained results comparable to the baseline in
both challenge tracks. The code is publicly available at
https://github.com/TIO-IKIM/PUMA.
</details>

### [Reflect-DiT: Inference-Time Scaling for Text-to-Image Diffusion Transformers via In-Context Reflection](https://arxiv.org/abs/2503.12271)
*Shufan Li,Konstantinos Kallidromitis,Akash Gokul,Arsh Koneru,Yusuke Kato,Kazuki Kozuka,Aditya Grover*
<details>
  <summary>Abstract</summary>
The predominant approach to advancing text-to-image generation has been
training-time scaling, where larger models are trained on more data using
greater computational resources. While effective, this approach is
computationally expensive, leading to growing interest in inference-time
scaling to improve performance. Currently, inference-time scaling for
text-to-image diffusion models is largely limited to best-of-N sampling, where
multiple images are generated per prompt and a selection model chooses the best
output. Inspired by the recent success of reasoning models like DeepSeek-R1 in
the language domain, we introduce an alternative to naive best-of-N sampling by
equipping text-to-image Diffusion Transformers with in-context reflection
capabilities. We propose Reflect-DiT, a method that enables Diffusion
Transformers to refine their generations using in-context examples of
previously generated images alongside textual feedback describing necessary
improvements. Instead of passively relying on random sampling and hoping for a
better result in a future generation, Reflect-DiT explicitly tailors its
generations to address specific aspects requiring enhancement. Experimental
results demonstrate that Reflect-DiT improves performance on the GenEval
benchmark (+0.19) using SANA-1.0-1.6B as a base model. Additionally, it
achieves a new state-of-the-art score of 0.81 on GenEval while generating only
20 samples per prompt, surpassing the previous best score of 0.80, which was
obtained using a significantly larger model (SANA-1.5-4.8B) with 2048 samples
under the best-of-N approach.
</details>

### [Exploration of VLMs for Driver Monitoring Systems Applications](https://arxiv.org/abs/2503.12281)
*Paola Natalia Ca√±as,Marcos Nieto,Oihana Otaegui,Igor Rodr√≠guez*
<details>
  <summary>Abstract</summary>
In recent years, we have witnessed significant progress in emerging deep
learning models, particularly Large Language Models (LLMs) and Vision-Language
Models (VLMs). These models have demonstrated promising results, indicating a
new era of Artificial Intelligence (AI) that surpasses previous methodologies.
Their extensive knowledge and zero-shot capabilities suggest a paradigm shift
in developing deep learning solutions, moving from data capturing and algorithm
training to just writing appropriate prompts. While the application of these
technologies has been explored across various industries, including automotive,
there is a notable gap in the scientific literature regarding their use in
Driver Monitoring Systems (DMS). This paper presents our initial approach to
implementing VLMs in this domain, utilising the Driver Monitoring Dataset to
evaluate their performance and discussing their advantages and challenges when
implemented in real-world scenarios.
</details>

### [REdiSplats: Ray Tracing for Editable Gaussian Splatting](https://arxiv.org/abs/2503.12284)
*Krzysztof Byrski,Grzegorz Wilczy≈Ñski,Weronika Smolak-Dy≈ºewska,Piotr Borycki,Dawid Baran,S≈Çawomir Tadeja,Przemys≈Çaw Spurek*
<details>
  <summary>Abstract</summary>
Gaussian Splatting (GS) has become one of the most important neural rendering
algorithms. GS represents 3D scenes using Gaussian components with trainable
color and opacity. This representation achieves high-quality renderings with
fast inference. Regrettably, it is challenging to integrate such a solution
with varying light conditions, including shadows and light reflections, manual
adjustments, and a physical engine. Recently, a few approaches have appeared
that incorporate ray-tracing or mesh primitives into GS to address some of
these caveats. However, no such solution can simultaneously solve all the
existing limitations of the classical GS. Consequently, we introduce
REdiSplats, which employs ray tracing and a mesh-based representation of flat
3D Gaussians. In practice, we model the scene using flat Gaussian distributions
parameterized by the mesh. We can leverage fast ray tracing and control
Gaussian modification by adjusting the mesh vertices. Moreover, REdiSplats
allows modeling of light conditions, manual adjustments, and physical
simulation. Furthermore, we can render our models using 3D tools such as
Blender or Nvdiffrast, which opens the possibility of integrating them with all
existing 3D graphics techniques dedicated to mesh representations.
</details>

### [Towards Self-Improving Systematic Cognition for Next-Generation Foundation MLLMs](https://arxiv.org/abs/2503.12303)
*Xiaoying Zhang,Da Peng,Yipeng Zhang,Zonghao Guo,Chengyue Wu,Chi Chen,Wei Ke,Helen Meng,Maosong Sun*
<details>
  <summary>Abstract</summary>
Despite their impressive capabilities, Multimodal Large Language Models
(MLLMs) face challenges with fine-grained perception and complex reasoning.
Prevalent pre-training approaches focus on enhancing perception by training on
high-quality image captions due to the extremely high cost of collecting
chain-of-thought (CoT) reasoning data for improving reasoning. While leveraging
advanced MLLMs for caption generation enhances scalability, the outputs often
lack comprehensiveness and accuracy. In this paper, we introduce Self-Improving
Cognition (SIcog), a self-learning framework designed to construct
next-generation foundation MLLMs by enhancing their systematic cognitive
capabilities through multimodal pre-training with self-generated data.
Specifically, we propose chain-of-description, an approach that improves an
MLLM's systematic perception by enabling step-by-step visual understanding,
ensuring greater comprehensiveness and accuracy. Additionally, we adopt a
structured CoT reasoning technique to enable MLLMs to integrate in-depth
multimodal reasoning. To construct a next-generation foundation MLLM with
self-improved cognition, SIcog first equips an MLLM with systematic perception
and reasoning abilities using minimal external annotations. The enhanced models
then generate detailed captions and CoT reasoning data, which are further
curated through self-consistency. This curated data is ultimately used to
refine the MLLM during multimodal pre-training, facilitating next-generation
foundation MLLM construction. Extensive experiments on both low- and
high-resolution MLLMs across diverse benchmarks demonstrate that, with merely
213K self-generated pre-training samples, SIcog produces next-generation
foundation MLLMs with significantly improved cognition, achieving
benchmark-leading performance compared to prevalent pre-training approaches.
</details>

### [Swift4D:Adaptive divide-and-conquer Gaussian Splatting for compact and efficient reconstruction of dynamic scene](https://arxiv.org/abs/2503.12307)
*Jiahao Wu,Rui Peng,Zhiyan Wang,Lu Xiao,Luyang Tang,Jinbo Yan,Kaiqiang Xiong,Ronggang Wang*
<details>
  <summary>Abstract</summary>
Novel view synthesis has long been a practical but challenging task, although
the introduction of numerous methods to solve this problem, even combining
advanced representations like 3D Gaussian Splatting, they still struggle to
recover high-quality results and often consume too much storage memory and
training time. In this paper we propose Swift4D, a divide-and-conquer 3D
Gaussian Splatting method that can handle static and dynamic primitives
separately, achieving a good trade-off between rendering quality and
efficiency, motivated by the fact that most of the scene is the static
primitive and does not require additional dynamic properties. Concretely, we
focus on modeling dynamic transformations only for the dynamic primitives which
benefits both efficiency and quality. We first employ a learnable decomposition
strategy to separate the primitives, which relies on an additional parameter to
classify primitives as static or dynamic. For the dynamic primitives, we employ
a compact multi-resolution 4D Hash mapper to transform these primitives from
canonical space into deformation space at each timestamp, and then mix the
static and dynamic primitives to produce the final output. This
divide-and-conquer method facilitates efficient training and reduces storage
redundancy. Our method not only achieves state-of-the-art rendering quality
while being 20X faster in training than previous SOTA methods with a minimum
storage requirement of only 30MB on real-world datasets. Code is available at
https://github.com/WuJH2001/swift4d.
</details>

### [Leveraging Vision Capabilities of Multimodal LLMs for Automated Data Extraction from Plots](https://arxiv.org/abs/2503.12326)
*Maciej P. Polak,Dane Morgan*
<details>
  <summary>Abstract</summary>
Automated data extraction from research texts has been steadily improving,
with the emergence of large language models (LLMs) accelerating progress even
further. Extracting data from plots in research papers, however, has been such
a complex task that it has predominantly been confined to manual data
extraction. We show that current multimodal large language models, with proper
instructions and engineered workflows, are capable of accurately extracting
data from plots. This capability is inherent to the pretrained models and can
be achieved with a chain-of-thought sequence of zero-shot engineered prompts we
call PlotExtract, without the need to fine-tune. We demonstrate PlotExtract
here and assess its performance on synthetic and published plots. We consider
only plots with two axes in this analysis. For plots identified as extractable,
PlotExtract finds points with over 90% precision (and around 90% recall) and
errors in x and y position of around 5% or lower. These results prove that
multimodal LLMs are a viable path for high-throughput data extraction for plots
and in many circumstances can replace the current manual methods of data
extraction.
</details>

### [CapArena: Benchmarking and Analyzing Detailed Image Captioning in the LLM Era](https://arxiv.org/abs/2503.12329)
*Kanzhi Cheng,Wenpo Song,Jiaxin Fan,Zheng Ma,Qiushi Sun,Fangzhi Xu,Chenyang Yan,Nuo Chen,Jianbing Zhang,Jiajun Chen*
<details>
  <summary>Abstract</summary>
Image captioning has been a longstanding challenge in vision-language
research. With the rise of LLMs, modern Vision-Language Models (VLMs) generate
detailed and comprehensive image descriptions. However, benchmarking the
quality of such captions remains unresolved. This paper addresses two key
questions: (1) How well do current VLMs actually perform on image captioning,
particularly compared to humans? We built CapArena, a platform with over 6000
pairwise caption battles and high-quality human preference votes. Our
arena-style evaluation marks a milestone, showing that leading models like
GPT-4o achieve or even surpass human performance, while most open-source models
lag behind. (2) Can automated metrics reliably assess detailed caption quality?
Using human annotations from CapArena, we evaluate traditional and recent
captioning metrics, as well as VLM-as-a-Judge. Our analysis reveals that while
some metrics (e.g., METEOR) show decent caption-level agreement with humans,
their systematic biases lead to inconsistencies in model ranking. In contrast,
VLM-as-a-Judge demonstrates robust discernment at both the caption and model
levels. Building on these insights, we release CapArena-Auto, an accurate and
efficient automated benchmark for detailed captioning, achieving 94.3%
correlation with human rankings at just $4 per test. Data and resources will be
open-sourced at https://caparena.github.io.
</details>

### [VideoMAP: Toward Scalable Mamba-based Video Autoregressive Pretraining](https://arxiv.org/abs/2503.12332)
*Yunze Liu,Peiran Wu,Cheng Liang,Junxiao Shen,Limin Wang,Li Yi*
<details>
  <summary>Abstract</summary>
Recent Mamba-based architectures for video understanding demonstrate
promising computational efficiency and competitive performance, yet struggle
with overfitting issues that hinder their scalability. To overcome this
challenge, we introduce VideoMAP, a Hybrid Mamba-Transformer framework
featuring a novel pre-training approach. VideoMAP uses a 4:1
Mamba-to-Transformer ratio, effectively balancing computational cost and model
capacity. This architecture, combined with our proposed frame-wise masked
autoregressive pre-training strategy, delivers significant performance gains
when scaling to larger models. Additionally, VideoMAP exhibits impressive
sample efficiency, significantly outperforming existing methods with less
training data. Experiments show that VideoMAP outperforms existing models
across various datasets, including Kinetics-400, Something-Something V2,
Breakfast, and COIN. Furthermore, we demonstrate the potential of VideoMAP as a
visual encoder for multimodal large language models, highlighting its ability
to reduce memory usage and enable the processing of longer video sequences. The
code is open-source at https://github.com/yunzeliu/MAP
</details>

### [GS-3I: Gaussian Splatting for Surface Reconstruction from Illumination-Inconsistent Images](https://arxiv.org/abs/2503.12335)
*Tengfei Wang,Yongmao Hou,Zhaoning Zhang,Yiwei Xu,Zongqian Zhan,Xin Wang*
<details>
  <summary>Abstract</summary>
Accurate geometric surface reconstruction, providing essential environmental
information for navigation and manipulation tasks, is critical for enabling
robotic self-exploration and interaction. Recently, 3D Gaussian Splatting
(3DGS) has gained significant attention in the field of surface reconstruction
due to its impressive geometric quality and computational efficiency. While
recent relevant advancements in novel view synthesis under inconsistent
illumination using 3DGS have shown promise, the challenge of robust surface
reconstruction under such conditions is still being explored. To address this
challenge, we propose a method called GS-3I. Specifically, to mitigate 3D
Gaussian optimization bias caused by underexposed regions in single-view
images, based on Convolutional Neural Network (CNN), a tone mapping correction
framework is introduced. Furthermore, inconsistent lighting across multi-view
images, resulting from variations in camera settings and complex scene
illumination, often leads to geometric constraint mismatches and deviations in
the reconstructed surface. To overcome this, we propose a normal compensation
mechanism that integrates reference normals extracted from single-view image
with normals computed from multi-view observations to effectively constrain
geometric inconsistencies. Extensive experimental evaluations demonstrate that
GS-3I can achieve robust and accurate surface reconstruction across complex
illumination scenarios, highlighting its effectiveness and versatility in this
critical challenge. https://github.com/TFwang-9527/GS-3I
</details>

### [TopoGaussian: Inferring Internal Topology Structures from Visual Clues](https://arxiv.org/abs/2503.12343)
*Xiaoyu Xiong,Changyu Hu,Chunru Lin,Pingchuan Ma,Chuang Gan,Tao Du*
<details>
  <summary>Abstract</summary>
We present TopoGaussian, a holistic, particle-based pipeline for inferring
the interior structure of an opaque object from easily accessible photos and
videos as input. Traditional mesh-based approaches require tedious and
error-prone mesh filling and fixing process, while typically output rough
boundary surface. Our pipeline combines Gaussian Splatting with a novel,
versatile particle-based differentiable simulator that simultaneously
accommodates constitutive model, actuator, and collision, without interference
with mesh. Based on the gradients from this simulator, we provide flexible
choice of topology representation for optimization, including particle, neural
implicit surface, and quadratic surface. The resultant pipeline takes easily
accessible photos and videos as input and outputs the topology that matches the
physical characteristics of the input. We demonstrate the efficacy of our
pipeline on a synthetic dataset and four real-world tasks with 3D-printed
prototypes. Compared with existing mesh-based method, our pipeline is 5.26x
faster on average with improved shape quality. These results highlight the
potential of our pipeline in 3D vision, soft robotics, and manufacturing
applications.
</details>

### [ProbDiffFlow: An Efficient Learning-Free Framework for Probabilistic Single-Image Optical Flow Estimation](https://arxiv.org/abs/2503.12348)
*Mo Zhou,Jianwei Wang,Xuanmeng Zhang,Dylan Campbell,Kai Wang,Long Yuan,Wenjie Zhang,Xuemin Lin*
<details>
  <summary>Abstract</summary>
This paper studies optical flow estimation, a critical task in motion
analysis with applications in autonomous navigation, action recognition, and
film production. Traditional optical flow methods require consecutive frames,
which are often unavailable due to limitations in data acquisition or
real-world scene disruptions. Thus, single-frame optical flow estimation is
emerging in the literature. However, existing single-frame approaches suffer
from two major limitations: (1) they rely on labeled training data, making them
task-specific, and (2) they produce deterministic predictions, failing to
capture motion uncertainty. To overcome these challenges, we propose
ProbDiffFlow, a training-free framework that estimates optical flow
distributions from a single image. Instead of directly predicting motion,
ProbDiffFlow follows an estimation-by-synthesis paradigm: it first generates
diverse plausible future frames using a diffusion-based model, then estimates
motion from these synthesized samples using a pre-trained optical flow model,
and finally aggregates the results into a probabilistic flow distribution. This
design eliminates the need for task-specific training while capturing multiple
plausible motions. Experiments on both synthetic and real-world datasets
demonstrate that ProbDiffFlow achieves superior accuracy, diversity, and
efficiency, outperforming existing single-image and two-frame baselines.
</details>

### [ResLPR: A LiDAR Data Restoration Network and Benchmark for Robust Place Recognition Against Weather Corruptions](https://arxiv.org/abs/2503.12350)
*Wenqing Kuang,Xiongwei Zhao,Yehui Shen,Congcong Wen,Huimin Lu,Zongtan Zhou,Xieyuanli Chen*
<details>
  <summary>Abstract</summary>
LiDAR-based place recognition (LPR) is a key component for autonomous
driving, and its resilience to environmental corruption is critical for safety
in high-stakes applications. While state-of-the-art (SOTA) LPR methods perform
well in clean weather, they still struggle with weather-induced corruption
commonly encountered in driving scenarios. To tackle this, we propose
ResLPRNet, a novel LiDAR data restoration network that largely enhances LPR
performance under adverse weather by restoring corrupted LiDAR scans using a
wavelet transform-based network. ResLPRNet is efficient, lightweight and can be
integrated plug-and-play with pretrained LPR models without substantial
additional computational cost. Given the lack of LPR datasets under adverse
weather, we introduce ResLPR, a novel benchmark that examines SOTA LPR methods
under a wide range of LiDAR distortions induced by severe snow, fog, and rain
conditions. Experiments on our proposed WeatherKITTI and WeatherNCLT datasets
demonstrate the resilience and notable gains achieved by using our restoration
method with multiple LPR approaches in challenging weather scenarios. Our code
and benchmark are publicly available here:
https://github.com/nubot-nudt/ResLPR.
</details>

### [Atlas: Multi-Scale Attention Improves Long Context Image Modeling](https://arxiv.org/abs/2503.12355)
*Kumar Krishna Agrawal,Long Lian,Longchao Liu,Natalia Harguindeguy,Boyi Li,Alexander Bick,Maggie Chung,Trevor Darrell,Adam Yala*
<details>
  <summary>Abstract</summary>
Efficiently modeling massive images is a long-standing challenge in machine
learning. To this end, we introduce Multi-Scale Attention (MSA). MSA relies on
two key ideas, (i) multi-scale representations (ii) bi-directional cross-scale
communication. MSA creates O(log N) scales to represent the image across
progressively coarser features and leverages cross-attention to propagate
information across scales. We then introduce Atlas, a novel neural network
architecture based on MSA. We demonstrate that Atlas significantly improves the
compute-performance tradeoff of long-context image modeling in a
high-resolution variant of ImageNet 100. At 1024px resolution, Atlas-B achieves
91.04% accuracy, comparable to ConvNext-B (91.92%) while being 4.3x faster.
Atlas is 2.95x faster and 7.38% better than FasterViT, 2.25x faster and 4.96%
better than LongViT. In comparisons against MambaVision-S, we find Atlas-S
achieves 5%, 16% and 32% higher accuracy at 1024px, 2048px and 4096px
respectively, while obtaining similar runtimes. Code for reproducing our
experiments and pretrained models is available at
https://github.com/yalalab/atlas.
</details>

### [Localized Concept Erasure for Text-to-Image Diffusion Models Using Training-Free Gated Low-Rank Adaptation](https://arxiv.org/abs/2503.12356)
*Byung Hyun Lee,Sungjin Lim,Se Young Chun*
<details>
  <summary>Abstract</summary>
Fine-tuning based concept erasing has demonstrated promising results in
preventing generation of harmful contents from text-to-image diffusion models
by removing target concepts while preserving remaining concepts. To maintain
the generation capability of diffusion models after concept erasure, it is
necessary to remove only the image region containing the target concept when it
locally appears in an image, leaving other regions intact. However, prior arts
often compromise fidelity of the other image regions in order to erase the
localized target concept appearing in a specific area, thereby reducing the
overall performance of image generation. To address these limitations, we first
introduce a framework called localized concept erasure, which allows for the
deletion of only the specific area containing the target concept in the image
while preserving the other regions. As a solution for the localized concept
erasure, we propose a training-free approach, dubbed Gated Low-rank adaptation
for Concept Erasure (GLoCE), that injects a lightweight module into the
diffusion model. GLoCE consists of low-rank matrices and a simple gate,
determined only by several generation steps for concepts without training. By
directly applying GLoCE to image embeddings and designing the gate to activate
only for target concepts, GLoCE can selectively remove only the region of the
target concepts, even when target and remaining concepts coexist within an
image. Extensive experiments demonstrated GLoCE not only improves the image
fidelity to text prompts after erasing the localized target concepts, but also
outperforms prior arts in efficacy, specificity, and robustness by large margin
and can be extended to mass concept erasure.
</details>

### [L2COcc: Lightweight Camera-Centric Semantic Scene Completion via Distillation of LiDAR Model](https://arxiv.org/abs/2503.12369)
*Ruoyu Wang,Yukai Ma,Yi Yao,Sheng Tao,Haoang Li,Zongzhi Zhu,Yong Liu,Xingxing Zuo*
<details>
  <summary>Abstract</summary>
Semantic Scene Completion (SSC) constitutes a pivotal element in autonomous
driving perception systems, tasked with inferring the 3D semantic occupancy of
a scene from sensory data. To improve accuracy, prior research has implemented
various computationally demanding and memory-intensive 3D operations, imposing
significant computational requirements on the platform during training and
testing. This paper proposes L2COcc, a lightweight camera-centric SSC framework
that also accommodates LiDAR inputs. With our proposed efficient voxel
transformer (EVT) and cross-modal knowledge modules, including feature
similarity distillation (FSD), TPV distillation (TPVD) and prediction alignment
distillation (PAD), our method substantially reduce computational burden while
maintaining high accuracy. The experimental evaluations demonstrate that our
proposed method surpasses the current state-of-the-art vision-based SSC methods
regarding accuracy on both the SemanticKITTI and SSCBench-KITTI-360 benchmarks,
respectively. Additionally, our method is more lightweight, exhibiting a
reduction in both memory consumption and inference time by over 23% compared to
the current state-of-the-arts method. Code is available at our project
page:https://studyingfufu.github.io/L2COcc/.
</details>

### [Deepfake Detection with Optimized Hybrid Model: EAR Biometric Descriptor via Improved RCNN](https://arxiv.org/abs/2503.12381)
*Ruchika Sharma,Rudresh Dwivedi*
<details>
  <summary>Abstract</summary>
Deepfake is a widely used technology employed in recent years to create
pernicious content such as fake news, movies, and rumors by altering and
substituting facial information from various sources. Given the ongoing
evolution of deepfakes investigation of continuous identification and
prevention is crucial. Due to recent technological advancements in AI
(Artificial Intelligence) distinguishing deepfakes and artificially altered
images has become challenging. This approach introduces the robust detection of
subtle ear movements and shape changes to generate ear descriptors. Further, we
also propose a novel optimized hybrid deepfake detection model that considers
the ear biometric descriptors via enhanced RCNN (Region-Based Convolutional
Neural Network). Initially, the input video is converted into frames and
preprocessed through resizing, normalization, grayscale conversion, and
filtering processes followed by face detection using the Viola-Jones technique.
Next, a hybrid model comprising DBN (Deep Belief Network) and Bi-GRU
(Bidirectional Gated Recurrent Unit) is utilized for deepfake detection based
on ear descriptors. The output from the detection phase is determined through
improved score-level fusion. To enhance the performance, the weights of both
detection models are optimally tuned using the SU-JFO (Self-Upgraded Jellyfish
Optimization method). Experimentation is conducted based on four scenarios:
compression, noise, rotation, pose, and illumination on three different
datasets. The performance results affirm that our proposed method outperforms
traditional models such as CNN (Convolution Neural Network), SqueezeNet, LeNet,
LinkNet, LSTM (Long Short-Term Memory), DFP (Deepfake Predictor) [1], and
ResNext+CNN+LSTM [2] in terms of various performance metrics viz. accuracy,
specificity, and precision.
</details>

### [RENO: Real-Time Neural Compression for 3D LiDAR Point Clouds](https://arxiv.org/abs/2503.12382)
*Kang You,Tong Chen,Dandan Ding,M. Salman Asif,Zhan Ma*
<details>
  <summary>Abstract</summary>
Despite the substantial advancements demonstrated by learning-based neural
models in the LiDAR Point Cloud Compression (LPCC) task, realizing real-time
compression - an indispensable criterion for numerous industrial applications -
remains a formidable challenge. This paper proposes RENO, the first real-time
neural codec for 3D LiDAR point clouds, achieving superior performance with a
lightweight model. RENO skips the octree construction and directly builds upon
the multiscale sparse tensor representation. Instead of the multi-stage
inferring, RENO devises sparse occupancy codes, which exploit cross-scale
correlation and derive voxels' occupancy in a one-shot manner, greatly saving
processing time. Experimental results demonstrate that the proposed RENO
achieves real-time coding speed, 10 fps at 14-bit depth on a desktop platform
(e.g., one RTX 3090 GPU) for both encoding and decoding processes, while
providing 12.25% and 48.34% bit-rate savings compared to G-PCCv23 and Draco,
respectively, at a similar quality. RENO model size is merely 1MB, making it
attractive for practical applications. The source code is available at
https://github.com/NJUVISION/RENO.
</details>

### [VRsketch2Gaussian: 3D VR Sketch Guided 3D Object Generation with Gaussian Splatting](https://arxiv.org/abs/2503.12383)
*Songen Gu,Haoxuan Song,Binjie Liu,Qian Yu,Sanyi Zhang,Haiyong Jiang,Jin Huang,Feng Tian*
<details>
  <summary>Abstract</summary>
We propose VRSketch2Gaussian, a first VR sketch-guided, multi-modal, native
3D object generation framework that incorporates a 3D Gaussian Splatting
representation. As part of our work, we introduce VRSS, the first large-scale
paired dataset containing VR sketches, text, images, and 3DGS, bridging the gap
in multi-modal VR sketch-based generation. Our approach features the following
key innovations: 1) Sketch-CLIP feature alignment. We propose a two-stage
alignment strategy that bridges the domain gap between sparse VR sketch
embeddings and rich CLIP embeddings, facilitating both VR sketch-based
retrieval and generation tasks. 2) Fine-Grained multi-modal conditioning. We
disentangle the 3D generation process by using explicit VR sketches for
geometric conditioning and text descriptions for appearance control. To
facilitate this, we propose a generalizable VR sketch encoder that effectively
aligns different modalities. 3) Efficient and high-fidelity 3D native
generation. Our method leverages a 3D-native generation approach that enables
fast and texture-rich 3D object synthesis. Experiments conducted on our VRSS
dataset demonstrate that our method achieves high-quality, multi-modal VR
sketch-based 3D generation. We believe our VRSS dataset and VRsketch2Gaussian
method will be beneficial for the 3D generation community.
</details>

### [Car-1000: A New Large Scale Fine-Grained Visual Categorization Dataset](https://arxiv.org/abs/2503.12385)
*Yutao Hu,Sen Li,Jincheng Yan,Wenqi Shao,Xiaoyan Luo*
<details>
  <summary>Abstract</summary>
Fine-grained visual categorization (FGVC) is a challenging but significant
task in computer vision, which aims to recognize different sub-categories of
birds, cars, airplanes, etc. Among them, recognizing models of different cars
has significant application value in autonomous driving, traffic surveillance
and scene understanding, which has received considerable attention in the past
few years. However, Stanford-Car, the most widely used fine-grained dataset for
car recognition, only has 196 different categories and only includes vehicle
models produced earlier than 2013. Due to the rapid advancements in the
automotive industry during recent years, the appearances of various car models
have become increasingly intricate and sophisticated. Consequently, the
previous Stanford-Car dataset fails to capture this evolving landscape and
cannot satisfy the requirements of automotive industry. To address these
challenges, in our paper, we introduce Car-1000, a large-scale dataset designed
specifically for fine-grained visual categorization of diverse car models.
Car-1000 encompasses vehicles from 165 different automakers, spanning a wide
range of 1000 distinct car models. Additionally, we have reproduced several
state-of-the-art FGVC methods on the Car-1000 dataset, establishing a new
benchmark for research in this field. We hope that our work will offer a fresh
perspective for future FGVC researchers. Our dataset is available at
https://github.com/toggle1995/Car-1000.
</details>

### [Pathology Image Restoration via Mixture of Prompts](https://arxiv.org/abs/2503.12399)
*Jiangdong Cai,Yan Chen,Zhenrong Shen,Haotian Jiang,Honglin Xiong,Kai Xuan,Lichi Zhang,Qian Wang*
<details>
  <summary>Abstract</summary>
In digital pathology, acquiring all-in-focus images is essential to
high-quality imaging and high-efficient clinical workflow. Traditional scanners
achieve this by scanning at multiple focal planes of varying depths and then
merging them, which is relatively slow and often struggles with complex tissue
defocus. Recent prevailing image restoration technique provides a means to
restore high-quality pathology images from scans of single focal planes.
However, existing image restoration methods are inadequate, due to intricate
defocus patterns in pathology images and their domain-specific semantic
complexities. In this work, we devise a two-stage restoration solution
cascading a transformer and a diffusion model, to benefit from their powers in
preserving image fidelity and perceptual quality, respectively. We particularly
propose a novel mixture of prompts for the two-stage solution. Given initial
prompt that models defocus in microscopic imaging, we design two prompts that
describe the high-level image semantics from pathology foundation model and the
fine-grained tissue structures via edge extraction. We demonstrate that, by
feeding the prompt mixture to our method, we can restore high-quality pathology
images from single-focal-plane scans, implying high potentials of the mixture
of prompts to clinical usage. Code will be publicly available at
https://github.com/caijd2000/MoP.
</details>

### [MExD: An Expert-Infused Diffusion Model for Whole-Slide Image Classification](https://arxiv.org/abs/2503.12401)
*Jianwei Zhao,Xin Li,Fan Yang,Qiang Zhai,Ao Luo,Yang Zhao,Hong Cheng,Huazhu Fu*
<details>
  <summary>Abstract</summary>
Whole Slide Image (WSI) classification poses unique challenges due to the
vast image size and numerous non-informative regions, which introduce noise and
cause data imbalance during feature aggregation. To address these issues, we
propose MExD, an Expert-Infused Diffusion Model that combines the strengths of
a Mixture-of-Experts (MoE) mechanism with a diffusion model for enhanced
classification. MExD balances patch feature distribution through a novel
MoE-based aggregator that selectively emphasizes relevant information,
effectively filtering noise, addressing data imbalance, and extracting
essential features. These features are then integrated via a diffusion-based
generative process to directly yield the class distribution for the WSI. Moving
beyond conventional discriminative approaches, MExD represents the first
generative strategy in WSI classification, capturing fine-grained details for
robust and precise results. Our MExD is validated on three widely-used
benchmarks-Camelyon16, TCGA-NSCLC, and BRACS consistently achieving
state-of-the-art performance in both binary and multi-class tasks.
</details>

### [SAM2-ELNet: Label Enhancement and Automatic Annotation for Remote Sensing Segmentation](https://arxiv.org/abs/2503.12404)
*Jianhao Yang,Wenshuo Yu,Yuanchao Lv,Jiance Sun,Bokang Sun,Mingyang Liu*
<details>
  <summary>Abstract</summary>
Remote sensing image segmentation is crucial for environmental monitoring,
disaster assessment, and resource management, directly affecting the accuracy
and efficiency of surface information extraction. The performance of existing
supervised models in remote sensing image segmentation tasks highly depends on
the quality of label data. However, current label data mainly relies on manual
annotation, which comes with high time costs and is subject to subjective
interference, resulting in distortion of label boundaries and often a loss of
detail. To solve the above problems, our work proposes an Edge-enhanced
Labeling Network, called SAM2-ELNet, which incorporates a labeling module and
an edge attention mechanism. This model effectively addresses issues such as
label detail loss, fragmentation, and inaccurate boundaries. Due to the
scarcity of manually annotated remote sensing data, the feature extraction
capabilities of traditional neural networks are limited. Our method uses the
Hiera backbone of the pre-trained self-supervised large model segment anything
model 2 (SAM2) as the encoder, achieves high-quality and efficient feature
extraction even with small samples by fine-tuning on downstream tasks. This
study compared the training effects of original and enhanced labels on the
manually annotated Deep-SAR Oil Spill (SOS) dataset. Results showed that the
model trained with enhanced labels performed better and had a lower final loss,
indicating closer alignment with the real data distribution. Our work also
explores the potential of extending the model into an efficient automatic
annotation framework through generalization experiments, facilitating
large-scale remote sensing image interpretation and intelligent recognition.
</details>

### [A Causality-Inspired Model for Intima-Media Thickening Assessment in Ultrasound Videos](https://arxiv.org/abs/2503.12418)
*Shuo Gao,Jingyang Zhang,Jun Xue,Meng Yang,Yang Chen,Guangquan Zhou*
<details>
  <summary>Abstract</summary>
Carotid atherosclerosis represents a significant health risk, with its early
diagnosis primarily dependent on ultrasound-based assessments of carotid
intima-media thickening. However, during carotid ultrasound screening,
significant view variations cause style shifts, impairing content cues related
to thickening, such as lumen anatomy, which introduces spurious correlations
that hinder assessment. Therefore, we propose a novel causal-inspired method
for assessing carotid intima-media thickening in frame-wise ultrasound videos,
which focuses on two aspects: eliminating spurious correlations caused by style
and enhancing causal content correlations. Specifically, we introduce a novel
Spurious Correlation Elimination (SCE) module to remove non-causal style
effects by enforcing prediction invariance with style perturbations.
Simultaneously, we propose a Causal Equivalence Consolidation (CEC) module to
strengthen causal content correlation through adversarial optimization during
content randomization. Simultaneously, we design a Causal Transition
Augmentation (CTA) module to ensure smooth causal flow by integrating an
auxiliary pathway with text prompts and connecting it through contrastive
learning. The experimental results on our in-house carotid ultrasound video
dataset achieved an accuracy of 86.93\%, demonstrating the superior performance
of the proposed method. Code is available at
\href{https://github.com/xielaobanyy/causal-imt}{https://github.com/xielaobanyy/causal-imt}.
</details>

### [EgoEvGesture: Gesture Recognition Based on Egocentric Event Camera](https://arxiv.org/abs/2503.12419)
*Luming Wang,Hao Shi,Xiaoting Yin,Kailun Yang,Kaiwei Wang*
<details>
  <summary>Abstract</summary>
Egocentric gesture recognition is a pivotal technology for enhancing natural
human-computer interaction, yet traditional RGB-based solutions suffer from
motion blur and illumination variations in dynamic scenarios. While event
cameras show distinct advantages in handling high dynamic range with ultra-low
power consumption, existing RGB-based architectures face inherent limitations
in processing asynchronous event streams due to their synchronous frame-based
nature. Moreover, from an egocentric perspective, event cameras record data
that include events generated by both head movements and hand gestures, thereby
increasing the complexity of gesture recognition. To address this, we propose a
novel network architecture specifically designed for event data processing,
incorporating (1) a lightweight CNN with asymmetric depthwise convolutions to
reduce parameters while preserving spatiotemporal features, (2) a plug-and-play
state-space model as context block that decouples head movement noise from
gesture dynamics, and (3) a parameter-free Bins-Temporal Shift Module (BSTM)
that shifts features along bins and temporal dimensions to fuse sparse events
efficiently. We further build the EgoEvGesture dataset, the first large-scale
dataset for egocentric gesture recognition using event cameras. Experimental
results demonstrate that our method achieves 62.7% accuracy in heterogeneous
testing with only 7M parameters, 3.1% higher than state-of-the-art approaches.
Notable misclassifications in freestyle motions stem from high inter-personal
variability and unseen test patterns differing from training data. Moreover,
our approach achieved a remarkable accuracy of 96.97% on DVS128 Gesture,
demonstrating strong cross-dataset generalization capability. The dataset and
models are made publicly available at
https://github.com/3190105222/EgoEv_Gesture.
</details>

### [Consistent-Point: Consistent Pseudo-Points for Semi-Supervised Crowd Counting and Localization](https://arxiv.org/abs/2503.12441)
*Yuda Zou,Zelong Liu,Yuliang Gu,Bo Du,Yongchao Xu*
<details>
  <summary>Abstract</summary>
Crowd counting and localization are important in applications such as public
security and traffic management. Existing methods have achieved impressive
results thanks to extensive laborious annotations. This paper propose a novel
point-localization-based semi-supervised crowd counting and localization method
termed Consistent-Point. We identify and address two inconsistencies of
pseudo-points, which have not been adequately explored. To enhance their
position consistency, we aggregate the positions of neighboring auxiliary
proposal-points. Additionally, an instance-wise uncertainty calibration is
proposed to improve the class consistency of pseudo-points. By generating more
consistent pseudo-points, Consistent-Point provides more stable supervision to
the training process, yielding improved results. Extensive experiments across
five widely used datasets and three different labeled ratio settings
demonstrate that our method achieves state-of-the-art performance in crowd
localization while also attaining impressive crowd counting results. The code
will be available.
</details>

### [BREEN: Bridge Data-Efficient Encoder-Free Multimodal Learning with Learnable Queries](https://arxiv.org/abs/2503.12446)
*Tianle Li,Yongming Rao,Winston Hu,Yu Cheng*
<details>
  <summary>Abstract</summary>
Encoder-free multimodal large language models(MLLMs) eliminate the need for a
well-trained vision encoder by directly processing image tokens before the
language model. While this approach reduces computational overhead and model
complexity, it often requires large amounts of training data to effectively
capture the visual knowledge typically encoded by vision models like CLIP. The
absence of a vision encoder implies that the model is likely to rely on
substantial data to learn the necessary visual-semantic alignments. In this
work, we present BREEN, a data-efficient encoder-free multimodal architecture
that mitigates this issue. BREEN leverages a learnable query and image experts
to achieve comparable performance with significantly less training data. The
learnable query, positioned between image and text tokens, is supervised by the
output of a pretrained CLIP model to distill visual knowledge, bridging the gap
between visual and textual modalities. Additionally, the image expert processes
image tokens and learnable queries independently, improving efficiency and
reducing interference with the LLM's textual capabilities. BREEN achieves
comparable performance to prior encoder-free state-of-the-art models like
Mono-InternVL, using only 13 million text-image pairs in training about one
percent of the data required by existing methods. Our work highlights a
promising direction for data-efficient encoder-free multimodal learning,
offering an alternative to traditional encoder-based approaches.
</details>

### [Causality Model for Semantic Understanding on Videos](https://arxiv.org/abs/2503.12447)
*Li Yicong*
<details>
  <summary>Abstract</summary>
After a decade of prosperity, the development of video understanding has
reached a critical juncture, where the sole reliance on massive data and
complex architectures is no longer a one-size-fits-all solution to all
situations. The presence of ubiquitous data imbalance hampers DNNs from
effectively learning the underlying causal mechanisms, leading to significant
performance drops when encountering distribution shifts, such as long-tail
imbalances and perturbed imbalances. This realization has prompted researchers
to seek alternative methodologies to capture causal patterns in video data. To
tackle these challenges and increase the robustness of DNNs, causal modeling
emerged as a principle to discover the true causal patterns behind the observed
correlations. This thesis focuses on the domain of semantic video understanding
and explores the potential of causal modeling to advance two fundamental tasks:
Video Relation Detection (VidVRD) and Video Question Answering (VideoQA).
</details>

### [LazyMAR: Accelerating Masked Autoregressive Models via Feature Caching](https://arxiv.org/abs/2503.12450)
*Feihong Yan,Qingyan Wei,Jiayi Tang,Jiajun Li,Yulin Wang,Xuming Hu,Huiqi Li,Linfeng Zhang*
<details>
  <summary>Abstract</summary>
Masked Autoregressive (MAR) models have emerged as a promising approach in
image generation, expected to surpass traditional autoregressive models in
computational efficiency by leveraging the capability of parallel decoding.
However, their dependence on bidirectional self-attention inherently conflicts
with conventional KV caching mechanisms, creating unexpected computational
bottlenecks that undermine their expected efficiency. To address this problem,
this paper studies the caching mechanism for MAR by leveraging two types of
redundancy: Token Redundancy indicates that a large portion of tokens have very
similar representations in the adjacent decoding steps, which allows us to
first cache them in previous steps and then reuse them in the later steps.
Condition Redundancy indicates that the difference between conditional and
unconditional output in classifier-free guidance exhibits very similar values
in adjacent steps. Based on these two redundancies, we propose LazyMAR, which
introduces two caching mechanisms to handle them one by one. LazyMAR is
training-free and plug-and-play for all MAR models. Experimental results
demonstrate that our method achieves 2.83 times acceleration with almost no
drop in generation quality. Our codes will be released in
https://github.com/feihongyan1/LazyMAR.
</details>

### [ISLR101: an Iranian Word-Level Sign Language Recognition Dataset](https://arxiv.org/abs/2503.12451)
*Hossein Ranjbar,Alireza Taheri*
<details>
  <summary>Abstract</summary>
Sign language recognition involves modeling complex multichannel information,
such as hand shapes and movements while relying on sufficient sign
language-specific data. However, sign languages are often under-resourced,
posing a significant challenge for research and development in this field. To
address this gap, we introduce ISLR101, the first publicly available Iranian
Sign Language dataset for isolated sign language recognition. This
comprehensive dataset includes 4,614 videos covering 101 distinct signs,
recorded by 10 different signers (3 deaf individuals, 2 sign language
interpreters, and 5 L2 learners) against varied backgrounds, with a resolution
of 800x600 pixels and a frame rate of 25 frames per second. It also includes
skeleton pose information extracted using OpenPose. We establish both a visual
appearance-based and a skeleton-based framework as baseline models, thoroughly
training and evaluating them on ISLR101. These models achieve 97.01% and 94.02%
accuracy on the test set, respectively. Additionally, we publish the train,
validation, and test splits to facilitate fair comparisons.
</details>

### [Shape Bias and Robustness Evaluation via Cue Decomposition for Image Classification and Segmentation](https://arxiv.org/abs/2503.12453)
*Edgar Heinert,Thomas Gottwald,Annika M√ºtze,Matthias Rottmann*
<details>
  <summary>Abstract</summary>
Previous works studied how deep neural networks (DNNs) perceive image content
in terms of their biases towards different image cues, such as texture and
shape. Previous methods to measure shape and texture biases are typically
style-transfer-based and limited to DNNs for image classification. In this
work, we provide a new evaluation procedure consisting of 1) a
cue-decomposition method that comprises two AI-free data pre-processing methods
extracting shape and texture cues, respectively, and 2) a novel
cue-decomposition shape bias evaluation metric that leverages the
cue-decomposition data. For application purposes we introduce a corresponding
cue-decomposition robustness metric that allows for the estimation of the
robustness of a DNN w.r.t. image corruptions. In our numerical experiments, our
findings for biases in image classification DNNs align with those of previous
evaluation metrics. However, our cue-decomposition robustness metric shows
superior results in terms of estimating the robustness of DNNs. Furthermore,
our results for DNNs on the semantic segmentation datasets Cityscapes and
ADE20k for the first time shed light into the biases of semantic segmentation
DNNs.
</details>

### [Exploring Contextual Attribute Density in Referring Expression Counting](https://arxiv.org/abs/2503.12460)
*Zhicheng Wang,Zhiyu Pan,Zhan Peng,Jian Cheng,Liwen Xiao,Wei Jiang,Zhiguo Cao*
<details>
  <summary>Abstract</summary>
Referring expression counting (REC) algorithms are for more flexible and
interactive counting ability across varied fine-grained text expressions.
However, the requirement for fine-grained attribute understanding poses
challenges for prior arts, as they struggle to accurately align attribute
information with correct visual patterns. Given the proven importance of
''visual density'', it is presumed that the limitations of current REC
approaches stem from an under-exploration of ''contextual attribute density''
(CAD). In the scope of REC, we define CAD as the measure of the information
intensity of one certain fine-grained attribute in visual regions. To model the
CAD, we propose a U-shape CAD estimator in which referring expression and
multi-scale visual features from GroundingDINO can interact with each other.
With additional density supervision, we can effectively encode CAD, which is
subsequently decoded via a novel attention procedure with CAD-refined queries.
Integrating all these contributions, our framework significantly outperforms
state-of-the-art REC methods, achieves $30\%$ error reduction in counting
metrics and a $10\%$ improvement in localization accuracy. The surprising
results shed light on the significance of contextual attribute density for REC.
Code will be at github.com/Xu3XiWang/CAD-GD.
</details>

### [MambaIC: State Space Models for High-Performance Learned Image Compression](https://arxiv.org/abs/2503.12461)
*Fanhu Zeng,Hao Tang,Yihua Shao,Siyu Chen,Ling Shao,Yan Wang*
<details>
  <summary>Abstract</summary>
A high-performance image compression algorithm is crucial for real-time
information transmission across numerous fields. Despite rapid progress in
image compression, computational inefficiency and poor redundancy modeling
still pose significant bottlenecks, limiting practical applications. Inspired
by the effectiveness of state space models (SSMs) in capturing long-range
dependencies, we leverage SSMs to address computational inefficiency in
existing methods and improve image compression from multiple perspectives. In
this paper, we integrate the advantages of SSMs for better
efficiency-performance trade-off and propose an enhanced image compression
approach through refined context modeling, which we term MambaIC. Specifically,
we explore context modeling to adaptively refine the representation of hidden
states. Additionally, we introduce window-based local attention into
channel-spatial entropy modeling to reduce potential spatial redundancy during
compression, thereby increasing efficiency. Comprehensive qualitative and
quantitative results validate the effectiveness and efficiency of our approach,
particularly for high-resolution image compression. Code is released at
https://github.com/AuroraZengfh/MambaIC.
</details>

### [Learning Privacy from Visual Entities](https://arxiv.org/abs/2503.12464)
*Alessio Xompero,Andrea Cavallaro*
<details>
  <summary>Abstract</summary>
Subjective interpretation and content diversity make predicting whether an
image is private or public a challenging task. Graph neural networks combined
with convolutional neural networks (CNNs), which consist of 14,000 to 500
millions parameters, generate features for visual entities (e.g., scene and
object types) and identify the entities that contribute to the decision. In
this paper, we show that using a simpler combination of transfer learning and a
CNN to relate privacy with scene types optimises only 732 parameters while
achieving comparable performance to that of graph-based methods. On the
contrary, end-to-end training of graph-based methods can mask the contribution
of individual components to the classification performance. Furthermore, we
show that a high-dimensional feature vector, extracted with CNNs for each
visual entity, is unnecessary and complexifies the model. The graph component
has also negligible impact on performance, which is driven by fine-tuning the
CNN to optimise image features for privacy nodes.
</details>

### [DPF-Net: Physical Imaging Model Embedded Data-Driven Underwater Image Enhancement](https://arxiv.org/abs/2503.12470)
*Han Mei,Kunqian Li,Shuaixin Liu,Chengzhi Ma,Qianli Jiang*
<details>
  <summary>Abstract</summary>
Due to the complex interplay of light absorption and scattering in the
underwater environment, underwater images experience significant degradation.
This research presents a two-stage underwater image enhancement network called
the Data-Driven and Physical Parameters Fusion Network (DPF-Net), which
harnesses the robustness of physical imaging models alongside the generality
and efficiency of data-driven methods. We first train a physical parameter
estimate module using synthetic datasets to guarantee the trustworthiness of
the physical parameters, rather than solely learning the fitting relationship
between raw and reference images by the application of the imaging equation, as
is common in prior studies. This module is subsequently trained in conjunction
with an enhancement network, where the estimated physical parameters are
integrated into a data-driven model within the embedding space. To maintain the
uniformity of the restoration process amid underwater imaging degradation, we
propose a physics-based degradation consistency loss. Additionally, we suggest
an innovative weak reference loss term utilizing the entire dataset, which
alleviates our model's reliance on the quality of individual reference images.
Our proposed DPF-Net demonstrates superior performance compared to other
benchmark methods across multiple test sets, achieving state-of-the-art
results. The source code and pre-trained models are available on the project
home page: https://github.com/OUCVisionGroup/DPF-Net.
</details>

### [Diffusion-based Synthetic Data Generation for Visible-Infrared Person Re-Identification](https://arxiv.org/abs/2503.12472)
*Wenbo Dai,Lijing Lu,Zhihang Li*
<details>
  <summary>Abstract</summary>
The performance of models is intricately linked to the abundance of training
data. In Visible-Infrared person Re-IDentification (VI-ReID) tasks, collecting
and annotating large-scale images of each individual under various cameras and
modalities is tedious, time-expensive, costly and must comply with data
protection laws, posing a severe challenge in meeting dataset requirements.
Current research investigates the generation of synthetic data as an efficient
and privacy-ensuring alternative to collecting real data in the field. However,
a specific data synthesis technique tailored for VI-ReID models has yet to be
explored. In this paper, we present a novel data generation framework, dubbed
Diffusion-based VI-ReID data Expansion (DiVE), that automatically obtain
massive RGB-IR paired images with identity preserving by decoupling identity
and modality to improve the performance of VI-ReID models. Specifically,
identity representation is acquired from a set of samples sharing the same ID,
whereas the modality of images is learned by fine-tuning the Stable Diffusion
(SD) on modality-specific data. DiVE extend the text-driven image synthesis to
identity-preserving RGB-IR multimodal image synthesis. This approach
significantly reduces data collection and annotation costs by directly
incorporating synthetic data into ReID model training. Experiments have
demonstrated that VI-ReID models trained on synthetic data produced by DiVE
consistently exhibit notable enhancements. In particular, the state-of-the-art
method, CAJ, trained with synthetic images, achieves an improvement of about
$9\%$ in mAP over the baseline on the LLCM dataset. Code:
https://github.com/BorgDiven/DiVE
</details>

### [Cross-Modal Consistency Learning for Sign Language Recognition](https://arxiv.org/abs/2503.12485)
*Kepeng Wu,Zecheng Li,Weichao Zhao,Hezhen Hu,Wengang Zhou,Houqiang Li*
<details>
  <summary>Abstract</summary>
Pre-training has been proven to be effective in boosting the performance of
Isolated Sign Language Recognition (ISLR). Existing pre-training methods solely
focus on the compact pose data, which eliminate background perturbation but
inevitably suffer from insufficient semantic cues compared to raw RGB videos.
Nevertheless, direct representation learning only from RGB videos remains
challenging due to the presence of sign-independent visual features. To address
this dilemma, we propose a Cross-modal Consistency Learning framework
(CCL-SLR), which leverages the cross-modal consistency from both RGB and pose
modalities based on self-supervised pre-training. First, CCL-SLR employs
contrastive learning for instance discrimination within and across modalities.
Through the single-modal and cross-modal contrastive learning, CCL-SLR
gradually aligns the feature spaces of RGB and pose modalities, thereby
extracting consistent sign representations. Second, we further introduce
Motion-Preserving Masking (MPM) and Semantic Positive Mining (SPM) techniques
to improve cross-modal consistency from the perspective of data augmentation
and sample similarity, respectively. Extensive experiments on four ISLR
benchmarks show that CCL-SLR achieves impressive performance, demonstrating its
effectiveness. The code will be released to the public.
</details>

### [GeoRSMLLM: A Multimodal Large Language Model for Vision-Language Tasks in Geoscience and Remote Sensing](https://arxiv.org/abs/2503.12490)
*Zilun Zhang,Haozhan Shen,Tiancheng Zhao,Bin Chen,Zian Guan,Yuhao Wang,Xu Jia,Yuxiang Cai,Yongheng Shang,Jianwei Yin*
<details>
  <summary>Abstract</summary>
The application of Vision-Language Models (VLMs) in remote sensing (RS) has
demonstrated significant potential in traditional tasks such as scene
classification, object detection, and image captioning. However, current
models, which excel in Referring Expression Comprehension (REC), struggle with
tasks involving complex instructions (e.g., exists multiple conditions) or
pixel-level operations like segmentation and change detection. In this white
paper, we provide a comprehensive hierarchical summary of vision-language tasks
in RS, categorized by the varying levels of cognitive capability required. We
introduce the Remote Sensing Vision-Language Task Set (RSVLTS), which includes
Open-Vocabulary Tasks (OVT), Referring Expression Tasks (RET), and Described
Object Tasks (DOT) with increased difficulty, and Visual Question Answering
(VQA) aloneside. Moreover, we propose a novel unified data representation using
a set-of-points approach for RSVLTS, along with a condition parser and a
self-augmentation strategy based on cyclic referring. These features are
integrated into the GeoRSMLLM model, and this enhanced model is designed to
handle a broad range of tasks of RSVLTS, paving the way for a more generalized
solution for vision-language tasks in geoscience and remote sensing.
</details>

### [Geometry-Aware Face Reconstruction Under Occluded Scenes](https://arxiv.org/abs/2503.12492)
*Dapeng Zhao*
<details>
  <summary>Abstract</summary>
Recently, deep learning-based 3D face reconstruction methods have
demonstrated promising advancements in terms of quality and efficiency.
Nevertheless, these techniques face challenges in effectively handling occluded
scenes and fail to capture intricate geometric facial details. Inspired by the
principles of GANs and bump mapping, we have successfully addressed these
issues. Our approach aims to deliver comprehensive 3D facial reconstructions,
even in the presence of occlusions.While maintaining the overall shape's
robustness, we introduce a mid-level shape refinement to the fundamental
structure. Furthermore, we illustrate how our method adeptly extends to
generate plausible details for obscured facial regions. We offer numerous
examples that showcase the effectiveness of our framework in producing
realistic results, where traditional methods often struggle. To substantiate
the superior adaptability of our approach, we have conducted extensive
experiments in the context of general 3D face reconstruction tasks, serving as
concrete evidence of its regulatory prowess compared to manual occlusion
removal methods.
</details>

### [Learning Contour-Guided 3D Face Reconstruction with Occlusions](https://arxiv.org/abs/2503.12494)
*Dapeng Zhao*
<details>
  <summary>Abstract</summary>
Recently, deep learning-based 3D face reconstruction methods have
demonstrated promising advancements in terms of quality and efficiency.
Nevertheless, these techniques face challenges in effectively handling occluded
scenes and fail to capture intricate geometric facial details. Inspired by the
principles of GANs and bump mapping, we have successfully addressed these
issues. Our approach aims to deliver comprehensive 3D facial reconstructions,
even in the presence of occlusions.While maintaining the overall shape's
robustness, we introduce a mid-level shape refinement to the fundamental
structure. Furthermore, we illustrate how our method adeptly extends to
generate plausible details for obscured facial regions. We offer numerous
examples that showcase the effectiveness of our framework in producing
realistic results, where traditional methods often struggle. To substantiate
the superior adaptability of our approach, we have conducted extensive
experiments in the context of general 3D face reconstruction tasks, serving as
concrete evidence of its regulatory prowess compared to manual occlusion
removal methods.
</details>

### [BS-Mamba for Black-Soil Area Detection On the Qinghai-Tibetan Plateau](https://arxiv.org/abs/2503.12495)
*Xuan Ma,Zewen Lv,Chengcai Ma,Tao Zhang,Yuelan Xin,Kun Zhan*
<details>
  <summary>Abstract</summary>
Extremely degraded grassland on the Qinghai-Tibetan Plateau (QTP) presents a
significant environmental challenge due to overgrazing, climate change, and
rodent activity, which degrade vegetation cover and soil quality. These
extremely degraded grassland on QTP, commonly referred to as black-soil area,
require accurate assessment to guide effective restoration efforts. In this
paper, we present a newly created QTP black-soil dataset, annotated under
expert guidance. We introduce a novel neural network model, BS-Mamba,
specifically designed for the black-soil area detection using UAV remote
sensing imagery. The BS-Mamba model demonstrates higher accuracy in identifying
black-soil area across two independent test datasets than the state-of-the-art
models. This research contributes to grassland restoration by providing an
efficient method for assessing the extent of black-soil area on the QTP.
</details>

### [Does Your Vision-Language Model Get Lost in the Long Video Sampling Dilemma?](https://arxiv.org/abs/2503.12496)
*Tianyuan Qu,Longxiang Tang,Bohao Peng,Senqiao Yang,Bei Yu,Jiaya Jia*
<details>
  <summary>Abstract</summary>
The rise of Large Vision-Language Models (LVLMs) has significantly advanced
video understanding. However, efficiently processing long videos remains a
challenge due to the ``Sampling Dilemma'': low-density sampling risks missing
critical information, while high-density sampling introduces redundancy. To
address this issue, we introduce LSDBench, the first benchmark designed to
evaluate LVLMs on long-video tasks by constructing high Necessary Sampling
Density (NSD) questions, where NSD represents the minimum sampling density
required to accurately answer a given question. LSDBench focuses on dense,
short-duration actions to rigorously assess the sampling strategies employed by
LVLMs. To tackle the challenges posed by high-NSD questions, we propose a novel
Reasoning-Driven Hierarchical Sampling (RHS) framework, which combines global
localization of question-relevant cues with local dense sampling for precise
inference. Additionally, we develop a lightweight Semantic-Guided Frame
Selector to prioritize informative frames, enabling RHS to achieve comparable
or superior performance with significantly fewer sampled frames. Together, our
LSDBench and RHS framework address the unique challenges of high-NSD long-video
tasks, setting a new standard for evaluating and improving LVLMs in this
domain.
</details>

### [Segment Any-Quality Images with Generative Latent Space Enhancement](https://arxiv.org/abs/2503.12507)
*Guangqian Guo,Yoong Guo,Xuehui Yu,Wenbo Li,Yaoxing Wang,Shan Gao*
<details>
  <summary>Abstract</summary>
Despite their success, Segment Anything Models (SAMs) experience significant
performance drops on severely degraded, low-quality images, limiting their
effectiveness in real-world scenarios. To address this, we propose GleSAM,
which utilizes Generative Latent space Enhancement to boost robustness on
low-quality images, thus enabling generalization across various image
qualities. Specifically, we adapt the concept of latent diffusion to SAM-based
segmentation frameworks and perform the generative diffusion process in the
latent space of SAM to reconstruct high-quality representation, thereby
improving segmentation. Additionally, we introduce two techniques to improve
compatibility between the pre-trained diffusion model and the segmentation
framework. Our method can be applied to pre-trained SAM and SAM2 with only
minimal additional learnable parameters, allowing for efficient optimization.
We also construct the LQSeg dataset with a greater diversity of degradation
types and levels for training and evaluating the model. Extensive experiments
demonstrate that GleSAM significantly improves segmentation robustness on
complex degradations while maintaining generalization to clear images.
Furthermore, GleSAM also performs well on unseen degradations, underscoring the
versatility of our approach and dataset.
</details>

### [AI-Powered Automated Model Construction for Patient-Specific CFD Simulations of Aortic Flows](https://arxiv.org/abs/2503.12515)
*Pan Du,Delin An,Chaoli Wang,Jian-Xun Wang*
<details>
  <summary>Abstract</summary>
Image-based modeling is essential for understanding cardiovascular
hemodynamics and advancing the diagnosis and treatment of cardiovascular
diseases. Constructing patient-specific vascular models remains
labor-intensive, error-prone, and time-consuming, limiting their clinical
applications. This study introduces a deep-learning framework that automates
the creation of simulation-ready vascular models from medical images. The
framework integrates a segmentation module for accurate voxel-based vessel
delineation with a surface deformation module that performs anatomically
consistent and unsupervised surface refinements guided by medical image data.
By unifying voxel segmentation and surface deformation into a single cohesive
pipeline, the framework addresses key limitations of existing methods,
enhancing geometric accuracy and computational efficiency. Evaluated on
publicly available datasets, the proposed approach demonstrates
state-of-the-art performance in segmentation and mesh quality while
significantly reducing manual effort and processing time. This work advances
the scalability and reliability of image-based computational modeling,
facilitating broader applications in clinical and research settings.
</details>

### [Multi Activity Sequence Alignment via Implicit Clustering](https://arxiv.org/abs/2503.12519)
*Taein Kwon,Zador Pataki,Mahdi Rad,Marc Pollefeys*
<details>
  <summary>Abstract</summary>
Self-supervised temporal sequence alignment can provide rich and effective
representations for a wide range of applications. However, existing methods for
achieving optimal performance are mostly limited to aligning sequences of the
same activity only and require separate models to be trained for each activity.
We propose a novel framework that overcomes these limitations using sequence
alignment via implicit clustering. Specifically, our key idea is to perform
implicit clip-level clustering while aligning frames in sequences. This coupled
with our proposed dual augmentation technique enhances the network's ability to
learn generalizable and discriminative representations. Our experiments show
that our proposed method outperforms state-of-the-art results and highlight the
generalization capability of our framework with multi activity and different
modalities on three diverse datasets, H2O, PennAction, and IKEA ASM. We will
release our code upon acceptance.
</details>

### [EditID: Training-Free Editable ID Customization for Text-to-Image Generation](https://arxiv.org/abs/2503.12526)
*Guandong Li,Zhaobin Chu*
<details>
  <summary>Abstract</summary>
We propose EditID, a training-free approach based on the DiT architecture,
which achieves highly editable customized IDs for text to image generation.
Existing text-to-image models for customized IDs typically focus more on ID
consistency while neglecting editability. It is challenging to alter facial
orientation, character attributes, and other features through prompts. EditID
addresses this by deconstructing the text-to-image model for customized IDs
into an image generation branch and a character feature branch. The character
feature branch is further decoupled into three modules: feature extraction,
feature fusion, and feature integration. By introducing a combination of
mapping features and shift features, along with controlling the intensity of ID
feature integration, EditID achieves semantic compression of local features
across network depths, forming an editable feature space. This enables the
successful generation of high-quality images with editable IDs while
maintaining ID consistency, achieving excellent results in the IBench
evaluation, which is an editability evaluation framework for the field of
customized ID text-to-image generation that quantitatively demonstrates the
superior performance of EditID. EditID is the first text-to-image solution to
propose customizable ID editability on the DiT architecture, meeting the
demands of long prompts and high quality image generation.
</details>

### [A Plug-and-Play Learning-based IMU Bias Factor for Robust Visual-Inertial Odometry](https://arxiv.org/abs/2503.12527)
*Yang Yi,Kunqing Wang,Jinpu Zhang,Zhen Tan,Xiangke Wang,Hui Shen,Dewen Hu*
<details>
  <summary>Abstract</summary>
The bias of low-cost Inertial Measurement Units (IMU) is a critical factor
affecting the performance of Visual-Inertial Odometry (VIO). In particular,
when visual tracking encounters errors, the optimized bias results may deviate
significantly from the true values, adversely impacting the system's stability
and localization precision. In this paper, we propose a novel plug-and-play
framework featuring the Inertial Prior Network (IPNet), which is designed to
accurately estimate IMU bias. Recognizing the substantial impact of initial
bias errors in low-cost inertial devices on system performance, our network
directly leverages raw IMU data to estimate the mean bias, eliminating the
dependency on historical estimates in traditional recursive predictions and
effectively preventing error propagation. Furthermore, we introduce an
iterative approach to calculate the mean value of the bias for network
training, addressing the lack of bias labels in many visual-inertial datasets.
The framework is evaluated on two public datasets and one self-collected
dataset. Extensive experiments demonstrate that our method significantly
enhances both localization precision and robustness, with the ATE-RMSE metric
improving on average by 46\%. The source code and video will be available at
\textcolor{red}{https://github.com/yiyscut/VIO-IPNet.git}.
</details>

### [Towards Suturing World Models: Learning Predictive Models for Robotic Surgical Tasks](https://arxiv.org/abs/2503.12531)
*Mehmet Kerem Turkcan,Mattia Ballo,Filippo Filicori,Zoran Kostic*
<details>
  <summary>Abstract</summary>
We introduce specialized diffusion-based generative models that capture the
spatiotemporal dynamics of fine-grained robotic surgical sub-stitch actions
through supervised learning on annotated laparoscopic surgery footage. The
proposed models form a foundation for data-driven world models capable of
simulating the biomechanical interactions and procedural dynamics of surgical
suturing with high temporal fidelity. Annotating a dataset of $\sim2K$ clips
extracted from simulation videos, we categorize surgical actions into
fine-grained sub-stitch classes including ideal and non-ideal executions of
needle positioning, targeting, driving, and withdrawal. We fine-tune two
state-of-the-art video diffusion models, LTX-Video and HunyuanVideo, to
generate high-fidelity surgical action sequences at $\ge$768x512 resolution and
$\ge$49 frames. For training our models, we explore both Low-Rank Adaptation
(LoRA) and full-model fine-tuning approaches. Our experimental results
demonstrate that these world models can effectively capture the dynamics of
suturing, potentially enabling improved training simulators, surgical skill
assessment tools, and autonomous surgical systems. The models also display the
capability to differentiate between ideal and non-ideal technique execution,
providing a foundation for building surgical training and evaluation systems.
We release our models for testing and as a foundation for future research.
Project Page: https://mkturkcan.github.io/suturingmodels/
</details>

### [STEVE: AStep Verification Pipeline for Computer-use Agent Training](https://arxiv.org/abs/2503.12532)
*Fanbin Lu,Zhisheng Zhong,Ziqin Wei,Shu Liu,Chi-Wing Fu,Jiaya Jia*
<details>
  <summary>Abstract</summary>
Developing AI agents to autonomously manipulate graphical user interfaces is
a long challenging task. Recent advances in data scaling law inspire us to
train computer-use agents with a scaled instruction set, yet using behavior
cloning to train agents still requires immense high-quality trajectories. To
meet the scalability need, we designed STEVE, a step verification pipeline for
computer-use agent training. First, we establish a large instruction set for
computer-use agents and collect trajectory data with some suboptimal agents.
GPT-4o is used to verify the correctness of each step in the trajectories based
on the screens before and after the action execution, assigning each step with
a binary label. Last, we adopt the Kahneman and Tversky Optimization to
optimize the agent from the binary stepwise labels. Extensive experiments
manifest that our agent outperforms supervised finetuning by leveraging both
positive and negative actions within a trajectory. Also, STEVE enables us to
train a 7B vision-language model as a computer-use agent, achieving leading
performance in the challenging live desktop environment WinAgentArena with
great efficiency at a reduced cost. Code and data:
https://github.com/FanbinLu/STEVE.
</details>

### [SPC-GS: Gaussian Splatting with Semantic-Prompt Consistency for Indoor Open-World Free-view Synthesis from Sparse Inputs](https://arxiv.org/abs/2503.12535)
*Guibiao Liao,Qing Li,Zhenyu Bao,Guoping Qiu,Kanglin Liu*
<details>
  <summary>Abstract</summary>
3D Gaussian Splatting-based indoor open-world free-view synthesis approaches
have shown significant performance with dense input images. However, they
exhibit poor performance when confronted with sparse inputs, primarily due to
the sparse distribution of Gaussian points and insufficient view supervision.
To relieve these challenges, we propose SPC-GS, leveraging Scene-layout-based
Gaussian Initialization (SGI) and Semantic-Prompt Consistency (SPC)
Regularization for open-world free view synthesis with sparse inputs.
Specifically, SGI provides a dense, scene-layout-based Gaussian distribution by
utilizing view-changed images generated from the video generation model and
view-constraint Gaussian points densification. Additionally, SPC mitigates
limited view supervision by employing semantic-prompt-based consistency
constraints developed by SAM2. This approach leverages available semantics from
training views, serving as instructive prompts, to optimize visually
overlapping regions in novel views with 2D and 3D consistency constraints.
Extensive experiments demonstrate the superior performance of SPC-GS across
Replica and ScanNet benchmarks. Notably, our SPC-GS achieves a 3.06 dB gain in
PSNR for reconstruction quality and a 7.3% improvement in mIoU for open-world
semantic segmentation.
</details>

### [BFANet: Revisiting 3D Semantic Segmentation with Boundary Feature Analysis](https://arxiv.org/abs/2503.12539)
*Weiguang Zhao,Rui Zhang,Qiufeng Wang,Guangliang Cheng,Kaizhu Huang*
<details>
  <summary>Abstract</summary>
3D semantic segmentation plays a fundamental and crucial role to understand
3D scenes. While contemporary state-of-the-art techniques predominantly
concentrate on elevating the overall performance of 3D semantic segmentation
based on general metrics (e.g. mIoU, mAcc, and oAcc), they unfortunately leave
the exploration of challenging regions for segmentation mostly neglected. In
this paper, we revisit 3D semantic segmentation through a more granular lens,
shedding light on subtle complexities that are typically overshadowed by
broader performance metrics. Concretely, we have delineated 3D semantic
segmentation errors into four comprehensive categories as well as corresponding
evaluation metrics tailored to each. Building upon this categorical framework,
we introduce an innovative 3D semantic segmentation network called BFANet that
incorporates detailed analysis of semantic boundary features. First, we design
the boundary-semantic module to decouple point cloud features into semantic and
boundary features, and fuse their query queue to enhance semantic features with
attention. Second, we introduce a more concise and accelerated boundary
pseudo-label calculation algorithm, which is 3.9 times faster than the
state-of-the-art, offering compatibility with data augmentation and enabling
efficient computation in training. Extensive experiments on benchmark data
indicate the superiority of our BFANet model, confirming the significance of
emphasizing the four uniquely designed metrics. Code is available at
https://github.com/weiguangzhao/BFANet.
</details>

### [ST-Think: How Multimodal Large Language Models Reason About 4D Worlds from Ego-Centric Videos](https://arxiv.org/abs/2503.12542)
*Peiran Wu,Yunze Liu,Chonghan Liu,Miao Liu,Junxiao Shen*
<details>
  <summary>Abstract</summary>
Humans excel at spatio-temporal reasoning, effortlessly interpreting dynamic
visual events from an egocentric viewpoint. However, whether multimodal large
language models (MLLMs) can similarly comprehend the 4D world remains
uncertain. This paper explores multimodal spatio-temporal reasoning from an
egocentric perspective, aiming to equip MLLMs with human-like reasoning
capabilities. To support this objective, we introduce Ego-ST Bench, a novel
benchmark containing over 5,000 question-answer pairs across four categories,
systematically evaluating spatial, temporal, and integrated spatio-temporal
reasoning. Additionally, we propose the ST-R1 Video model, a video-based
reasoning model that incorporates reverse thinking into its reinforcement
learning process, significantly enhancing performance. We combine
long-chain-of-thought (long-CoT) supervised fine-tuning with Group Relative
Policy Optimization (GRPO) reinforcement learning, achieving notable
improvements with limited high-quality data. Ego-ST Bench and ST-R1 provide
valuable insights and resources for advancing video-based spatio-temporal
reasoning research.
</details>

### [PEBench: A Fictitious Dataset to Benchmark Machine Unlearning for Multimodal Large Language Models](https://arxiv.org/abs/2503.12545)
*Zhaopan Xu,Pengfei Zhou,Weidong Tang,Jiaxin Ai,Wangbo Zhao,Xiaojiang Peng,Kai Wang,Yang You,Wenqi Shao,Hongxun Yao,Kaipeng Zhang*
<details>
  <summary>Abstract</summary>
In recent years, Multimodal Large Language Models (MLLMs) have demonstrated
remarkable advancements in tasks such as visual question answering, visual
understanding, and reasoning. However, this impressive progress relies on vast
amounts of data collected from the internet, raising significant concerns about
privacy and security. To address these issues, machine unlearning (MU) has
emerged as a promising solution, enabling the removal of specific knowledge
from an already trained model without requiring retraining from scratch.
Although MU for MLLMs has gained attention, current evaluations of its efficacy
remain incomplete, and the underlying problem is often poorly defined, which
hinders the development of strategies for creating more secure and trustworthy
systems. To bridge this gap, we introduce a benchmark, named PEBench, which
includes a dataset of personal entities and corresponding general event scenes,
designed to comprehensively assess the performance of MU for MLLMs. Through
PEBench, we aim to provide a standardized and robust framework to advance
research in secure and privacy-preserving multimodal models. We benchmarked 6
MU methods, revealing their strengths and limitations, and shedding light on
key challenges and opportunities for MU in MLLMs.
</details>

### [MTGS: Multi-Traversal Gaussian Splatting](https://arxiv.org/abs/2503.12552)
*Tianyu Li,Yihang Qiu,Zhenhua Wu,Carl Lindstr√∂m,Peng Su,Matthias Nie√üner,Hongyang Li*
<details>
  <summary>Abstract</summary>
Multi-traversal data, commonly collected through daily commutes or by
self-driving fleets, provides multiple viewpoints for scene reconstruction
within a road block. This data offers significant potential for high-quality
novel view synthesis, which is crucial for applications such as autonomous
vehicle simulators. However, inherent challenges in multi-traversal data often
result in suboptimal reconstruction quality, including variations in appearance
and the presence of dynamic objects. To address these issues, we propose
Multi-Traversal Gaussian Splatting (MTGS), a novel approach that reconstructs
high-quality driving scenes from arbitrarily collected multi-traversal data by
modeling a shared static geometry while separately handling dynamic elements
and appearance variations. Our method employs a multi-traversal dynamic scene
graph with a shared static node and traversal-specific dynamic nodes,
complemented by color correction nodes with learnable spherical harmonics
coefficient residuals. This approach enables high-fidelity novel view synthesis
and provides flexibility to navigate any viewpoint. We conduct extensive
experiments on a large-scale driving dataset, nuPlan, with multi-traversal
data. Our results demonstrate that MTGS improves LPIPS by 23.5% and geometry
accuracy by 46.3% compared to single-traversal baselines. The code and data
would be available to the public.
</details>

### [AdaReTaKe: Adaptive Redundancy Reduction to Perceive Longer for Video-language Understanding](https://arxiv.org/abs/2503.12559)
*Xiao Wang,Qingyi Si,Jianlong Wu,Shiyu Zhu,Li Cao,Liqiang Nie*
<details>
  <summary>Abstract</summary>
Multimodal Large Language Models (MLLMs) have revolutionized video
understanding, yet are still limited by context length when processing long
videos. Recent methods compress videos by leveraging visual redundancy
uniformly, yielding promising results. Nevertheless, our quantitative analysis
shows that redundancy varies significantly across time and model layers,
necessitating a more flexible compression strategy. We propose AdaReTaKe, a
training-free method that flexibly reduces visual redundancy by allocating
compression ratios among time and layers with theoretical guarantees.
Integrated into state-of-the-art MLLMs, AdaReTaKe improves processing capacity
from 256 to 2048 frames while preserving critical information. Experiments on
VideoMME, MLVU, LongVideoBench, and LVBench datasets demonstrate that AdaReTaKe
outperforms existing methods by 2.3% and 2.8% for 7B and 72B models,
respectively, with even greater improvements of 5.9% and 6.0% on the longest
LVBench. Our code is available at
https://github.com/SCZwangxiao/video-FlexReduc.git.
</details>

### [History-Aware Transformation of ReID Features for Multiple Object Tracking](https://arxiv.org/abs/2503.12562)
*Ruopeng Gao,Yuyao Wang,Chunxu Liu,Limin Wang*
<details>
  <summary>Abstract</summary>
The aim of multiple object tracking (MOT) is to detect all objects in a video
and bind them into multiple trajectories. Generally, this process is carried
out in two steps: detecting objects and associating them across frames based on
various cues and metrics. Many studies and applications adopt object
appearance, also known as re-identification (ReID) features, for target
matching through straightforward similarity calculation. However, we argue that
this practice is overly naive and thus overlooks the unique characteristics of
MOT tasks. Unlike regular re-identification tasks that strive to distinguish
all potential targets in a general representation, multi-object tracking
typically immerses itself in differentiating similar targets within the same
video sequence. Therefore, we believe that seeking a more suitable feature
representation space based on the different sample distributions of each
sequence will enhance tracking performance. In this paper, we propose using
history-aware transformations on ReID features to achieve more discriminative
appearance representations. Specifically, we treat historical trajectory
features as conditions and employ a tailored Fisher Linear Discriminant (FLD)
to find a spatial projection matrix that maximizes the differentiation between
different trajectories. Our extensive experiments reveal that this
training-free projection can significantly boost feature-only trackers to
achieve competitive, even superior tracking performance compared to
state-of-the-art methods while also demonstrating impressive zero-shot transfer
capabilities. This demonstrates the effectiveness of our proposal and further
encourages future investigation into the importance and customization of ReID
models in multiple object tracking. The code will be released at
https://github.com/HELLORPG/HATReID-MOT.
</details>

### [GAN-Based Single-Stage Defense for Traffic Sign Classification Under Adversarial Patch Attack](https://arxiv.org/abs/2503.12567)
*Abyad Enan,Mashrur Chowdhury*
<details>
  <summary>Abstract</summary>
Computer Vision plays a critical role in ensuring the safe navigation of
autonomous vehicles (AVs). An AV perception module is responsible for capturing
and interpreting the surrounding environment to facilitate safe navigation.
This module enables AVs to recognize traffic signs, traffic lights, and various
road users. However, the perception module is vulnerable to adversarial
attacks, which can compromise their accuracy and reliability. One such attack
is the adversarial patch attack (APA), a physical attack in which an adversary
strategically places a specially crafted sticker on an object to deceive object
classifiers. In APA, an adversarial patch is positioned on a target object,
leading the classifier to misidentify it. Such an APA can cause AVs to
misclassify traffic signs, leading to catastrophic incidents. To enhance the
security of an AV perception system against APAs, this study develops a
Generative Adversarial Network (GAN)-based single-stage defense strategy for
traffic sign classification. This approach is tailored to defend against APAs
on different classes of traffic signs without prior knowledge of a patch's
design. This study found this approach to be effective against patches of
varying sizes. Our experimental analysis demonstrates that the defense strategy
presented in this paper improves the classifier's accuracy under APA conditions
by up to 80.8% and enhances overall classification accuracy for all the traffic
signs considered in this study by 58%, compared to a classifier without any
defense mechanism. Our defense strategy is model-agnostic, making it applicable
to any traffic sign classifier, regardless of the underlying classification
model.
</details>

### [Deblur Gaussian Splatting SLAM](https://arxiv.org/abs/2503.12572)
*Francesco Girlanda,Denys Rozumnyi,Marc Pollefeys,Martin R. Oswald*
<details>
  <summary>Abstract</summary>
We present Deblur-SLAM, a robust RGB SLAM pipeline designed to recover sharp
reconstructions from motion-blurred inputs. The proposed method bridges the
strengths of both frame-to-frame and frame-to-model approaches to model
sub-frame camera trajectories that lead to high-fidelity reconstructions in
motion-blurred settings. Moreover, our pipeline incorporates techniques such as
online loop closure and global bundle adjustment to achieve a dense and precise
global trajectory. We model the physical image formation process of
motion-blurred images and minimize the error between the observed blurry images
and rendered blurry images obtained by averaging sharp virtual sub-frame
images. Additionally, by utilizing a monocular depth estimator alongside the
online deformation of Gaussians, we ensure precise mapping and enhanced image
deblurring. The proposed SLAM pipeline integrates all these components to
improve the results. We achieve state-of-the-art results for sharp map
estimation and sub-frame trajectory recovery both on synthetic and real-world
blurry input data.
</details>

### [BalancedDPO: Adaptive Multi-Metric Alignment](https://arxiv.org/abs/2503.12575)
*Dipesh Tamboli,Souradip Chakraborty,Aditya Malusare,Biplab Banerjee,Amrit Singh Bedi,Vaneet Aggarwal*
<details>
  <summary>Abstract</summary>
Text-to-image (T2I) diffusion models have made remarkable advancements, yet
aligning them with diverse preferences remains a persistent challenge. Current
methods often optimize single metrics or depend on narrowly curated datasets,
leading to overfitting and limited generalization across key visual quality
metrics. We present BalancedDPO, a novel extension of Direct Preference
Optimization (DPO) that addresses these limitations by simultaneously aligning
T2I diffusion models with multiple metrics, including human preference, CLIP
score, and aesthetic quality. Our key novelty lies in aggregating consensus
labels from diverse metrics in the preference distribution space as compared to
existing reward mixing approaches, enabling robust and scalable multi-metric
alignment while maintaining the simplicity of the standard DPO pipeline that we
refer to as BalancedDPO. Our evaluations on the Pick-a-Pic, PartiPrompt and HPD
datasets show that BalancedDPO achieves state-of-the-art results, outperforming
existing approaches across all major metrics. BalancedDPO improves the average
win rates by 15%, 7.1%, and 10.3% on Pick-a-pic, PartiPrompt and HPD,
respectively, from the DiffusionDPO.
</details>

### [Progressive Limb-Aware Virtual Try-On](https://arxiv.org/abs/2503.12588)
*Xiaoyu Han,Shengping Zhang,Qinglin Liu,Zonglin Li,Chenyang Wang*
<details>
  <summary>Abstract</summary>
Existing image-based virtual try-on methods directly transfer specific
clothing to a human image without utilizing clothing attributes to refine the
transferred clothing geometry and textures, which causes incomplete and blurred
clothing appearances. In addition, these methods usually mask the limb textures
of the input for the clothing-agnostic person representation, which results in
inaccurate predictions for human limb regions (i.e., the exposed arm skin),
especially when transforming between long-sleeved and short-sleeved garments.
To address these problems, we present a progressive virtual try-on framework,
named PL-VTON, which performs pixel-level clothing warping based on multiple
attributes of clothing and embeds explicit limb-aware features to generate
photo-realistic try-on results. Specifically, we design a Multi-attribute
Clothing Warping (MCW) module that adopts a two-stage alignment strategy based
on multiple attributes to progressively estimate pixel-level clothing
displacements. A Human Parsing Estimator (HPE) is then introduced to
semantically divide the person into various regions, which provides structural
constraints on the human body and therefore alleviates texture bleeding between
clothing and limb regions. Finally, we propose a Limb-aware Texture Fusion
(LTF) module to estimate high-quality details in limb regions by fusing
textures of the clothing and the human body with the guidance of explicit
limb-aware features. Extensive experiments demonstrate that our proposed method
outperforms the state-of-the-art virtual try-on methods both qualitatively and
quantitatively. The code is available at https://github.com/xyhanHIT/PL-VTON.
</details>

### [Personalize Anything for Free with Diffusion Transformer](https://arxiv.org/abs/2503.12590)
*Haoran Feng,Zehuan Huang,Lin Li,Hairong Lv,Lu Sheng*
<details>
  <summary>Abstract</summary>
Personalized image generation aims to produce images of user-specified
concepts while enabling flexible editing. Recent training-free approaches,
while exhibit higher computational efficiency than training-based methods,
struggle with identity preservation, applicability, and compatibility with
diffusion transformers (DiTs). In this paper, we uncover the untapped potential
of DiT, where simply replacing denoising tokens with those of a reference
subject achieves zero-shot subject reconstruction. This simple yet effective
feature injection technique unlocks diverse scenarios, from personalization to
image editing. Building upon this observation, we propose \textbf{Personalize
Anything}, a training-free framework that achieves personalized image
generation in DiT through: 1) timestep-adaptive token replacement that enforces
subject consistency via early-stage injection and enhances flexibility through
late-stage regularization, and 2) patch perturbation strategies to boost
structural diversity. Our method seamlessly supports layout-guided generation,
multi-subject personalization, and mask-controlled editing. Evaluations
demonstrate state-of-the-art performance in identity preservation and
versatility. Our work establishes new insights into DiTs while delivering a
practical paradigm for efficient personalization.
</details>

### [Point Cloud Based Scene Segmentation: A Survey](https://arxiv.org/abs/2503.12595)
*Dan Halperin,Niklas Eisl*
<details>
  <summary>Abstract</summary>
Autonomous driving is a safety-critical application, and it is therefore a
top priority that the accompanying assistance systems are able to provide
precise information about the surrounding environment of the vehicle. Tasks
such as 3D Object Detection deliver an insufficiently detailed understanding of
the surrounding scene because they only predict a bounding box for foreground
objects. In contrast, 3D Semantic Segmentation provides richer and denser
information about the environment by assigning a label to each individual
point, which is of paramount importance for autonomous driving tasks, such as
navigation or lane changes. To inspire future research, in this review paper,
we provide a comprehensive overview of the current state-of-the-art methods in
the field of Point Cloud Semantic Segmentation for autonomous driving. We
categorize the approaches into projection-based, 3D-based and hybrid methods.
Moreover, we discuss the most important and commonly used datasets for this
task and also emphasize the importance of synthetic data to support research
when real-world data is limited. We further present the results of the
different methods and compare them with respect to their segmentation accuracy
and efficiency.
</details>

### [Multimodal Chain-of-Thought Reasoning: A Comprehensive Survey](https://arxiv.org/abs/2503.12605)
*Yaoting Wang,Shengqiong Wu,Yuecheng Zhang,William Wang,Ziwei Liu,Jiebo Luo,Hao Fei*
<details>
  <summary>Abstract</summary>
By extending the advantage of chain-of-thought (CoT) reasoning in human-like
step-by-step processes to multimodal contexts, multimodal CoT (MCoT) reasoning
has recently garnered significant research attention, especially in the
integration with multimodal large language models (MLLMs). Existing MCoT
studies design various methodologies and innovative reasoning paradigms to
address the unique challenges of image, video, speech, audio, 3D, and
structured data across different modalities, achieving extensive success in
applications such as robotics, healthcare, autonomous driving, and multimodal
generation. However, MCoT still presents distinct challenges and opportunities
that require further focus to ensure consistent thriving in this field, where,
unfortunately, an up-to-date review of this domain is lacking. To bridge this
gap, we present the first systematic survey of MCoT reasoning, elucidating the
relevant foundational concepts and definitions. We offer a comprehensive
taxonomy and an in-depth analysis of current methodologies from diverse
perspectives across various application scenarios. Furthermore, we provide
insights into existing challenges and future research directions, aiming to
foster innovation toward multimodal AGI.
</details>

### [Industrial-Grade Sensor Simulation via Gaussian Splatting: A Modular Framework for Scalable Editing and Full-Stack Validation](https://arxiv.org/abs/2503.11731)
*Xianming Zeng,Sicong Du,Qifeng Chen,Lizhe Liu,Haoyu Shu,Jiaxuan Gao,Jiarun Liu,Jiulong Xu,Jianyun Xu,Mingxia Chen,Yiru Zhao,Peng Chen,Yapeng Xue,Chunming Zhao,Sheng Yang,Qiang Li*
<details>
  <summary>Abstract</summary>
Sensor simulation is pivotal for scalable validation of autonomous driving
systems, yet existing Neural Radiance Fields (NeRF) based methods face
applicability and efficiency challenges in industrial workflows. This paper
introduces a Gaussian Splatting (GS) based system to address these challenges:
We first break down sensor simulator components and analyze the possible
advantages of GS over NeRF. Then in practice, we refactor three crucial
components through GS, to leverage its explicit scene representation and
real-time rendering: (1) choosing the 2D neural Gaussian representation for
physics-compliant scene and sensor modeling, (2) proposing a scene editing
pipeline to leverage Gaussian primitives library for data augmentation, and (3)
coupling a controllable diffusion model for scene expansion and harmonization.
We implement this framework on a proprietary autonomous driving dataset
supporting cameras and LiDAR sensors. We demonstrate through ablation studies
that our approach reduces frame-wise simulation latency, achieves better
geometric and photometric consistency, and enables interpretable explicit scene
editing and expansion. Furthermore, we showcase how integrating such a GS-based
sensor simulator with traffic and dynamic simulators enables full-stack testing
of end-to-end autonomy algorithms. Our work provides both algorithmic insights
and practical validation, establishing GS as a cornerstone for industrial-grade
sensor simulation.
</details>

### [Safe Vision-Language Models via Unsafe Weights Manipulation](https://arxiv.org/abs/2503.11742)
*Moreno D'Inc√†,Elia Peruzzo,Xingqian Xu,Humphrey Shi,Nicu Sebe,Massimiliano Mancini*
<details>
  <summary>Abstract</summary>
Vision-language models (VLMs) often inherit the biases and unsafe
associations present within their large-scale training dataset. While recent
approaches mitigate unsafe behaviors, their evaluation focuses on how safe the
model is on unsafe inputs, ignoring potential shortcomings on safe ones. In
this paper, we first revise safety evaluation by introducing SafeGround, a new
set of metrics that evaluate safety at different levels of granularity. With
this metric, we uncover a surprising issue of training-based methods: they make
the model less safe on safe inputs. From this finding, we take a different
direction and explore whether it is possible to make a model safer without
training, introducing Unsafe Weights Manipulation (UWM). UWM uses a calibration
set of safe and unsafe instances to compare activations between safe and unsafe
content, identifying the most important parameters for processing the latter.
Their values are then manipulated via negation. Experiments show that UWM
achieves the best tradeoff between safety and knowledge preservation,
consistently improving VLMs on unsafe queries while outperforming even
training-based state-of-the-art methods on safe ones.
</details>

### [Making Every Step Effective: Jailbreaking Large Vision-Language Models Through Hierarchical KV Equalization](https://arxiv.org/abs/2503.11750)
*Shuyang Hao,Yiwei Wang,Bryan Hooi,Jun Liu,Muhao Chen,Zi Huang,Yujun Cai*
<details>
  <summary>Abstract</summary>
In the realm of large vision-language models (LVLMs), adversarial jailbreak
attacks serve as a red-teaming approach to identify safety vulnerabilities of
these models and their associated defense mechanisms. However, we identify a
critical limitation: not every adversarial optimization step leads to a
positive outcome, and indiscriminately accepting optimization results at each
step may reduce the overall attack success rate. To address this challenge, we
introduce HKVE (Hierarchical Key-Value Equalization), an innovative
jailbreaking framework that selectively accepts gradient optimization results
based on the distribution of attention scores across different layers, ensuring
that every optimization step positively contributes to the attack. Extensive
experiments demonstrate HKVE's significant effectiveness, achieving attack
success rates of 75.08% on MiniGPT4, 85.84% on LLaVA and 81.00% on Qwen-VL,
substantially outperforming existing methods by margins of 20.43\%, 21.01\% and
26.43\% respectively. Furthermore, making every step effective not only leads
to an increase in attack success rate but also allows for a reduction in the
number of iterations, thereby lowering computational costs. Warning: This paper
contains potentially harmful example data.
</details>

### [Rethinking Multi-modal Object Detection from the Perspective of Mono-Modality Feature Learning](https://arxiv.org/abs/2503.11780)
*Tianyi Zhao,Boyang Liu,Yanglei Gao,Yiming Sun,Maoxun Yuan,Xingxing Wei*
<details>
  <summary>Abstract</summary>
Multi-Modal Object Detection (MMOD), due to its stronger adaptability to
various complex environments, has been widely applied in various applications.
Extensive research is dedicated to the RGB-IR object detection, primarily
focusing on how to integrate complementary features from RGB-IR modalities.
However, they neglect the mono-modality insufficient learning problem that the
decreased feature extraction capability in multi-modal joint learning. This
leads to an unreasonable but prevalent phenomenon--Fusion Degradation, which
hinders the performance improvement of the MMOD model. Motivated by this, in
this paper, we introduce linear probing evaluation to the multi-modal detectors
and rethink the multi-modal object detection task from the mono-modality
learning perspective. Therefore, we construct an novel framework called
M$^2$D-LIF, which consists of the Mono-Modality Distillation (M$^2$D) method
and the Local Illumination-aware Fusion (LIF) module. The M$^2$D-LIF framework
facilitates the sufficient learning of mono-modality during multi-modal joint
training and explores a lightweight yet effective feature fusion manner to
achieve superior object detection performance. Extensive experiments conducted
on three MMOD datasets demonstrate that our M$^2$D-LIF effectively mitigates
the Fusion Degradation phenomenon and outperforms the previous SOTA detectors.
</details>

### [Color Matching Using Hypernetwork-Based Kolmogorov-Arnold Networks](https://arxiv.org/abs/2503.11781)
*Artem Nikonorov,Georgy Perevozchikov,Andrei Korepanov,Nancy Mehta,Mahmoud Afifi,Egor Ershov,Radu Timofte*
<details>
  <summary>Abstract</summary>
We present cmKAN, a versatile framework for color matching. Given an input
image with colors from a source color distribution, our method effectively and
accurately maps these colors to match a target color distribution in both
supervised and unsupervised settings. Our framework leverages the spline
capabilities of Kolmogorov-Arnold Networks (KANs) to model the color matching
between source and target distributions. Specifically, we developed a
hypernetwork that generates spatially varying weight maps to control the
nonlinear splines of a KAN, enabling accurate color matching. As part of this
work, we introduce a first large-scale dataset of paired images captured by two
distinct cameras and evaluate the efficacy of our and existing methods in
matching colors. We evaluated our approach across various color-matching tasks,
including: (1) raw-to-raw mapping, where the source color distribution is in
one camera's raw color space and the target in another camera's raw space; (2)
raw-to-sRGB mapping, where the source color distribution is in a camera's raw
space and the target is in the display sRGB space, emulating the color
rendering of a camera ISP; and (3) sRGB-to-sRGB mapping, where the goal is to
transfer colors from a source sRGB space (e.g., produced by a source camera
ISP) to a target sRGB space (e.g., from a different camera ISP). The results
show that our method outperforms existing approaches by 37.3% on average for
supervised and unsupervised cases while remaining lightweight compared to other
methods. The codes, dataset, and pre-trained models are available at:
https://github.com/gosha20777/cmKAN
</details>

### [ECLARE: Efficient cross-planar learning for anisotropic resolution enhancement](https://arxiv.org/abs/2503.11787)
*Samuel W. Remedios,Shuwen Wei,Shuo Han,Jinwei Zhang,Aaron Carass,Kurt G. Schilling,Dzung L. Pham,Jerry L. Prince,Blake E. Dewey*
<details>
  <summary>Abstract</summary>
In clinical imaging, magnetic resonance (MR) image volumes are often acquired
as stacks of 2D slices, permitting decreased scan times, improved
signal-to-noise ratio, and image contrasts unique to 2D MR pulse sequences.
While this is sufficient for clinical evaluation, automated algorithms designed
for 3D analysis perform sub-optimally on 2D-acquired scans, especially those
with thick slices and gaps between slices. Super-resolution (SR) methods aim to
address this problem, but previous methods do not address all of the following:
slice profile shape estimation, slice gap, domain shift, and non-integer /
arbitrary upsampling factors. In this paper, we propose ECLARE (Efficient
Cross-planar Learning for Anisotropic Resolution Enhancement), a self-SR method
that addresses each of these factors. ECLARE estimates the slice profile from
the 2D-acquired multi-slice MR volume, trains a network to learn the mapping
from low-resolution to high-resolution in-plane patches from the same volume,
and performs SR with anti-aliasing. We compared ECLARE to cubic B-spline
interpolation, SMORE, and other contemporary SR methods. We used realistic and
representative simulations so that quantitative performance against a ground
truth could be computed, and ECLARE outperformed all other methods in both
signal recovery and downstream tasks. On real data for which there is no ground
truth, ECLARE demonstrated qualitative superiority over other methods as well.
Importantly, as ECLARE does not use external training data it cannot suffer
from domain shift between training and testing. Our code is open-source and
available at https://www.github.com/sremedios/eclare.
</details>

### [StyleMorpheus: A Style-Based 3D-Aware Morphable Face Model](https://arxiv.org/abs/2503.11792)
*Peizhi Yan,Rabab K. Ward,Dan Wang,Qiang Tang,Shan Du*
<details>
  <summary>Abstract</summary>
For 3D face modeling, the recently developed 3D-aware neural rendering
methods are able to render photorealistic face images with arbitrary viewing
directions. The training of the parametric controllable 3D-aware face models,
however, still relies on a large-scale dataset that is lab-collected. To
address this issue, this paper introduces "StyleMorpheus", the first
style-based neural 3D Morphable Face Model (3DMM) that is trained on
in-the-wild images. It inherits 3DMM's disentangled controllability (over face
identity, expression, and appearance) but without the need for accurately
reconstructed explicit 3D shapes. StyleMorpheus employs an auto-encoder
structure. The encoder aims at learning a representative disentangled
parametric code space and the decoder improves the disentanglement using shape
and appearance-related style codes in the different sub-modules of the network.
Furthermore, we fine-tune the decoder through style-based generative
adversarial learning to achieve photorealistic 3D rendering quality. The
proposed style-based design enables StyleMorpheus to achieve state-of-the-art
3D-aware face reconstruction results, while also allowing disentangled control
of the reconstructed face. Our model achieves real-time rendering speed,
allowing its use in virtual reality applications. We also demonstrate the
capability of the proposed style-based design in face editing applications such
as style mixing and color editing. Project homepage:
https://github.com/ubc-3d-vision-lab/StyleMorpheus.
</details>

### [Semantic-Clipping: Efficient Vision-Language Modeling with Semantic-Guidedd Visual Selection](https://arxiv.org/abs/2503.11794)
*Bangzheng Li,Fei Wang,Wenxuan Zhou,Nan Xu,Ben Zhou,Sheng Zhang,Hoifung Poon,Muhao Chen*
<details>
  <summary>Abstract</summary>
Vision-Language Models (VLMs) leverage aligned visual encoders to transform
images into visual tokens, allowing them to be processed similarly to text by
the backbone large language model (LLM). This unified input paradigm enables
VLMs to excel in vision-language tasks such as visual question answering (VQA).
To improve fine-grained visual reasoning, recent advancements in
vision-language modeling introduce image cropping techniques that feed all
encoded sub-images into the model. However, this approach significantly
increases the number of visual tokens, leading to inefficiency and potential
distractions for the LLM. To address the generalization challenges of image
representation in VLMs, we propose a lightweight, universal framework that
seamlessly integrates with existing VLMs to enhance their ability to process
finegrained details. Our method leverages textual semantics to identify key
visual areas, improving VQA performance without requiring any retraining of the
VLM. Additionally, it incorporates textual signals into the visual encoding
process, enhancing both efficiency and effectiveness. The proposed method,
SEMCLIP, strengthens the visual understanding of a 7B VLM, LLaVA-1.5 by 3.3% on
average across 7 benchmarks, and particularly by 5.3% on the challenging
detailed understanding benchmark V*.
</details>

### [Human-in-the-Loop Local Corrections of 3D Scene Layouts via Infilling](https://arxiv.org/abs/2503.11806)
*Christopher Xie,Armen Avetisyan,Henry Howard-Jenkins,Yawar Siddiqui,Julian Straub,Richard Newcombe,Vasileios Balntas,Jakob Engel*
<details>
  <summary>Abstract</summary>
We present a novel human-in-the-loop approach to estimate 3D scene layout
that uses human feedback from an egocentric standpoint. We study this approach
through introduction of a novel local correction task, where users identify
local errors and prompt a model to automatically correct them. Building on
SceneScript, a state-of-the-art framework for 3D scene layout estimation that
leverages structured language, we propose a solution that structures this
problem as "infilling", a task studied in natural language processing. We train
a multi-task version of SceneScript that maintains performance on global
predictions while significantly improving its local correction ability. We
integrate this into a human-in-the-loop system, enabling a user to iteratively
refine scene layout estimates via a low-friction "one-click fix'' workflow. Our
system enables the final refined layout to diverge from the training
distribution, allowing for more accurate modelling of complex layouts.
</details>

### [Mitigating Bad Ground Truth in Supervised Machine Learning based Crop Classification: A Multi-Level Framework with Sentinel-2 Images](https://arxiv.org/abs/2503.11807)
*Sanayya A,Amoolya Shetty,Abhijeet Sharma,Venkatesh Ravichandran,Masthan Wali Gosuvarapalli,Sarthak Jain,Priyamvada Nanjundiah,Ujjal Kr Dutta,Divya Sharma*
<details>
  <summary>Abstract</summary>
In agricultural management, precise Ground Truth (GT) data is crucial for
accurate Machine Learning (ML) based crop classification. Yet, issues like crop
mislabeling and incorrect land identification are common. We propose a
multi-level GT cleaning framework while utilizing multi-temporal Sentinel-2
data to address these issues. Specifically, this framework utilizes generating
embeddings for farmland, clustering similar crop profiles, and identification
of outliers indicating GT errors. We validated clusters with False Colour
Composite (FCC) checks and used distance-based metrics to scale and automate
this verification process. The importance of cleaning the GT data became
apparent when the models were trained on the clean and unclean data. For
instance, when we trained a Random Forest model with the clean GT data, we
achieved upto 70\% absolute percentage points higher for the F1 score metric.
This approach advances crop classification methodologies, with potential for
applications towards improving loan underwriting and agricultural
decision-making.
</details>

### [Towards a Unified Copernicus Foundation Model for Earth Vision](https://arxiv.org/abs/2503.11849)
*Yi Wang,Zhitong Xiong,Chenying Liu,Adam J. Stewart,Thomas Dujardin,Nikolaos Ioannis Bountos,Angelos Zavras,Franziska Gerken,Ioannis Papoutsis,Laura Leal-Taix√©,Xiao Xiang Zhu*
<details>
  <summary>Abstract</summary>
Advances in Earth observation (EO) foundation models have unlocked the
potential of big satellite data to learn generic representations from space,
benefiting a wide range of downstream applications crucial to our planet.
However, most existing efforts remain limited to fixed spectral sensors, focus
solely on the Earth's surface, and overlook valuable metadata beyond imagery.
In this work, we take a step towards next-generation EO foundation models with
three key components: 1) Copernicus-Pretrain, a massive-scale pretraining
dataset that integrates 18.7M aligned images from all major Copernicus Sentinel
missions, spanning from the Earth's surface to its atmosphere; 2)
Copernicus-FM, a unified foundation model capable of processing any spectral or
non-spectral sensor modality using extended dynamic hypernetworks and flexible
metadata encoding; and 3) Copernicus-Bench, a systematic evaluation benchmark
with 15 hierarchical downstream tasks ranging from preprocessing to specialized
applications for each Sentinel mission. Our dataset, model, and benchmark
greatly improve the scalability, versatility, and multimodal adaptability of EO
foundation models, while also creating new opportunities to connect EO,
weather, and climate research. Codes, datasets and models are available at
https://github.com/zhu-xlab/Copernicus-FM.
</details>

### [DecAlign: Hierarchical Cross-Modal Alignment for Decoupled Multimodal Representation Learning](https://arxiv.org/abs/2503.11892)
*Chengxuan Qian,Shuo Xing,Shawn Li,Yue Zhao,Zhengzhong Tu*
<details>
  <summary>Abstract</summary>
Multimodal representation learning aims to capture both shared and
complementary semantic information across multiple modalities. However, the
intrinsic heterogeneity of diverse modalities presents substantial challenges
to achieve effective cross-modal collaboration and integration. To address
this, we introduce DecAlign, a novel hierarchical cross-modal alignment
framework designed to decouple multimodal representations into modality-unique
(heterogeneous) and modality-common (homogeneous) features. For handling
heterogeneity, we employ a prototype-guided optimal transport alignment
strategy leveraging gaussian mixture modeling and multi-marginal transport
plans, thus mitigating distribution discrepancies while preserving
modality-unique characteristics. To reinforce homogeneity, we ensure semantic
consistency across modalities by aligning latent distribution matching with
Maximum Mean Discrepancy regularization. Furthermore, we incorporate a
multimodal transformer to enhance high-level semantic feature fusion, thereby
further reducing cross-modal inconsistencies. Our extensive experiments on four
widely used multimodal benchmarks demonstrate that DecAlign consistently
outperforms existing state-of-the-art methods across five metrics. These
results highlight the efficacy of DecAlign in enhancing superior cross-modal
alignment and semantic consistency while preserving modality-unique features,
marking a significant advancement in multimodal representation learning
scenarios. Our project page is at https://taco-group.github.io/DecAlign and the
code is available at https://github.com/taco-group/DecAlign.
</details>

### [UStyle: Waterbody Style Transfer of Underwater Scenes by Depth-Guided Feature Synthesis](https://arxiv.org/abs/2503.11893)
*Md Abu Bakr Siddique,Junliang Liu,Piyush Singh,Md Jahidul Islam*
<details>
  <summary>Abstract</summary>
The concept of waterbody style transfer remains largely unexplored in the
underwater imaging and vision literature. Traditional image style transfer
(STx) methods primarily focus on artistic and photorealistic blending, often
failing to preserve object and scene geometry in images captured in
high-scattering mediums such as underwater. The wavelength-dependent nonlinear
attenuation and depth-dependent backscattering artifacts further complicate
learning underwater image STx from unpaired data. This paper introduces UStyle,
the first data-driven learning framework for transferring waterbody styles
across underwater images without requiring prior reference images or scene
information. We propose a novel depth-aware whitening and coloring transform
(DA-WCT) mechanism that integrates physics-based waterbody synthesis to ensure
perceptually consistent stylization while preserving scene structure. To
enhance style transfer quality, we incorporate carefully designed loss
functions that guide UStyle to maintain colorfulness, lightness, structural
integrity, and frequency-domain characteristics, as well as high-level content
in VGG and CLIP (contrastive language-image pretraining) feature spaces. By
addressing domain-specific challenges, UStyle provides a robust framework for
no-reference underwater image STx, surpassing state-of-the-art (SOTA) methods
that rely solely on end-to-end reconstruction loss. Furthermore, we introduce
the UF7D dataset, a curated collection of high-resolution underwater images
spanning seven distinct waterbody styles, establishing a benchmark to support
future research in underwater image STx. The UStyle inference pipeline and UF7D
dataset are released at: https://github.com/uf-robopi/UStyle.
</details>

### [Upcycling Text-to-Image Diffusion Models for Multi-Task Capabilities](https://arxiv.org/abs/2503.11905)
*Ruchika Chavhan,Abhinav Mehrotra,Malcolm Chadwick,Alberto Gil Ramos,Luca Morreale,Mehdi Noroozi,Sourav Bhattacharya*
<details>
  <summary>Abstract</summary>
Text-to-image synthesis has witnessed remarkable advancements in recent
years. Many attempts have been made to adopt text-to-image models to support
multiple tasks. However, existing approaches typically require
resource-intensive re-training or additional parameters to accommodate for the
new tasks, which makes the model inefficient for on-device deployment. We
propose Multi-Task Upcycling (MTU), a simple yet effective recipe that extends
the capabilities of a pre-trained text-to-image diffusion model to support a
variety of image-to-image generation tasks. MTU replaces Feed-Forward Network
(FFN) layers in the diffusion model with smaller FFNs, referred to as experts,
and combines them with a dynamic routing mechanism. To the best of our
knowledge, MTU is the first multi-task diffusion modeling approach that
seamlessly blends multi-tasking with on-device compatibility, by mitigating the
issue of parameter inflation. We show that the performance of MTU is on par
with the single-task fine-tuned diffusion models across several tasks including
image editing, super-resolution, and inpainting, while maintaining similar
latency and computational load (GFLOPs) as the single-task fine-tuned models.
</details>

### [A Survey on SAR ship classification using Deep Learning](https://arxiv.org/abs/2503.11906)
*Ch Muhammad Awais,Marco Reggiannini,Davide Moroni,Emanuele Salerno*
<details>
  <summary>Abstract</summary>
Deep learning (DL) has emerged as a powerful tool for Synthetic Aperture
Radar (SAR) ship classification. This survey comprehensively analyzes the
diverse DL techniques employed in this domain. We identify critical trends and
challenges, highlighting the importance of integrating handcrafted features,
utilizing public datasets, data augmentation, fine-tuning, explainability
techniques, and fostering interdisciplinary collaborations to improve DL model
performance. This survey establishes a first-of-its-kind taxonomy for
categorizing relevant research based on DL models, handcrafted feature use, SAR
attribute utilization, and the impact of fine-tuning. We discuss the
methodologies used in SAR ship classification tasks and the impact of different
techniques. Finally, the survey explores potential avenues for future research,
including addressing data scarcity, exploring novel DL architectures,
incorporating interpretability techniques, and establishing standardized
performance metrics. By addressing these challenges and leveraging advancements
in DL, researchers can contribute to developing more accurate and efficient
ship classification systems, ultimately enhancing maritime surveillance and
related applications.
</details>

### [k-fold Subsampling based Sequential Backward Feature Elimination](https://arxiv.org/abs/2503.11919)
*Jeonghwan Park,Kang Li,Huiyu Zhou*
<details>
  <summary>Abstract</summary>
We present a new wrapper feature selection algorithm for human detection.
This algorithm is a hybrid feature selection approach combining the benefits of
filter and wrapper methods. It allows the selection of an optimal feature
vector that well represents the shapes of the subjects in the images. In
detail, the proposed feature selection algorithm adopts the k-fold subsampling
and sequential backward elimination approach, while the standard linear support
vector machine (SVM) is used as the classifier for human detection. We apply
the proposed algorithm to the publicly accessible INRIA and ETH pedestrian full
image datasets with the PASCAL VOC evaluation criteria. Compared to other state
of the arts algorithms, our feature selection based approach can improve the
detection speed of the SVM classifier by over 50% with up to 2% better
detection accuracy. Our algorithm also outperforms the equivalent systems
introduced in the deformable part model approach with around 9% improvement in
the detection accuracy.
</details>

### [Generating a Biometrically Unique and Realistic Iris Database](https://arxiv.org/abs/2503.11930)
*Jingxuan Zhang,Robert J. Hart,Ziqian Bi,Shiaofen Fang,Susan Walsh*
<details>
  <summary>Abstract</summary>
The use of the iris as a biometric identifier has increased dramatically over
the last 30 years, prompting privacy and security concerns about the use of
iris images in research. It can be difficult to acquire iris image databases
due to ethical concerns, and this can be a barrier for those performing
biometrics research. In this paper, we describe and show how to create a
database of realistic, biometrically unidentifiable colored iris images by
training a diffusion model within an open-source diffusion framework. Not only
were we able to verify that our model is capable of creating iris textures that
are biometrically unique from the training data, but we were also able to
verify that our model output creates a full distribution of realistic iris
pigmentations. We highlight the fact that the utility of diffusion networks to
achieve these criteria with relative ease, warrants additional research in its
use within the context of iris database generation and presentation attack
security.
</details>

### [SPRINT: Script-agnostic Structure Recognition in Tables](https://arxiv.org/abs/2503.11932)
*Dhruv Kudale,Badri Vishal Kasuba,Venkatapathy Subramanian,Parag Chaudhuri,Ganesh Ramakrishnan*
<details>
  <summary>Abstract</summary>
Table Structure Recognition (TSR) is vital for various downstream tasks like
information retrieval, table reconstruction, and document understanding. While
most state-of-the-art (SOTA) research predominantly focuses on TSR in English
documents, the need for similar capabilities in other languages is evident,
considering the global diversity of data. Moreover, creating substantial
labeled data in non-English languages and training these SOTA models from
scratch is costly and time-consuming. We propose TSR as a language-agnostic
cell arrangement prediction and introduce SPRINT, Script-agnostic Structure
Recognition in Tables. SPRINT uses recently introduced Optimized Table
Structure Language (OTSL) sequences to predict table structures. We show that
when coupled with a pre-trained table grid estimator, SPRINT can improve the
overall tree edit distance-based similarity structure scores of tables even for
non-English documents. We experimentally evaluate our performance across
benchmark TSR datasets including PubTabNet, FinTabNet, and PubTables-1M. Our
findings reveal that SPRINT not only matches SOTA models in performance on
standard datasets but also demonstrates lower latency. Additionally, SPRINT
excels in accurately identifying table structures in non-English documents,
surpassing current leading models by showing an absolute average increase of
11.12%. We also present an algorithm for converting valid OTSL predictions into
a widely used HTML-based table representation. To encourage further research,
we release our code and Multilingual Scanned and Scene Table Structure
Recognition Dataset, MUSTARD labeled with OTSL sequences for 1428 tables in
thirteen languages encompassing several scripts at
https://github.com/IITB-LEAP-OCR/SPRINT
</details>

### [Design of an Expression Recognition Solution Employing the Global Channel-Spatial Attention Mechanism](https://arxiv.org/abs/2503.11935)
*Jun Yu,Yang Zheng,Lei Wang,Yongqi Wang,Shengfan Xu*
<details>
  <summary>Abstract</summary>
Facial expression recognition is a challenging classification task with broad
application prospects in the field of human - computer interaction. This paper
aims to introduce the methods of our upcoming 8th Affective Behavior Analysis
in the Wild (ABAW) competition to be held at CVPR2025. To address issues such
as low recognition accuracy caused by subtle expression changes and multi -
scales in facial expression recognition in videos, we propose global channel -
spatial attention and median - enhanced spatial - channel attention to
strengthen feature processing for speech and images respectively. Secondly, to
fully utilize the complementarity between the speech and facial expression
modalities, a speech - and - facial - expression key - frame alignment
technique is adopted to calculate the weights of speech and facial expressions.
These weights are input into the feature fusion layer for multi - scale dilated
fusion, which effectively improves the recognition rate of facial expression
recognition. In the facial expression recognition task of the 6th ABAW
competition, our method achieved excellent results on the official validation
set, which fully demonstrates the effectiveness and competitiveness of the
proposed method.
</details>

### [Att-Adapter: A Robust and Precise Domain-Specific Multi-Attributes T2I Diffusion Adapter via Conditional Variational Autoencoder](https://arxiv.org/abs/2503.11937)
*Wonwoong Cho,Yan-Ying Chen,Matthew Klenk,David I. Inouye,Yanxia Zhang*
<details>
  <summary>Abstract</summary>
Text-to-Image (T2I) Diffusion Models have achieved remarkable performance in
generating high quality images. However, enabling precise control of continuous
attributes, especially multiple attributes simultaneously, in a new domain
(e.g., numeric values like eye openness or car width) with text-only guidance
remains a significant challenge. To address this, we introduce the Attribute
(Att) Adapter, a novel plug-and-play module designed to enable fine-grained,
multi-attributes control in pretrained diffusion models. Our approach learns a
single control adapter from a set of sample images that can be unpaired and
contain multiple visual attributes. The Att-Adapter leverages the decoupled
cross attention module to naturally harmonize the multiple domain attributes
with text conditioning. We further introduce Conditional Variational
Autoencoder (CVAE) to the Att-Adapter to mitigate overfitting, matching the
diverse nature of the visual world. Evaluations on two public datasets show
that Att-Adapter outperforms all LoRA-based baselines in controlling continuous
attributes. Additionally, our method enables a broader control range and also
improves disentanglement across multiple attributes, surpassing StyleGAN-based
techniques. Notably, Att-Adapter is flexible, requiring no paired synthetic
data for training, and is easily scalable to multiple attributes within a
single model.
</details>

### [Your Text Encoder Can Be An Object-Level Watermarking Controller](https://arxiv.org/abs/2503.11945)
*Naresh Kumar Devulapally,Mingzhen Huang,Vishal Asnani,Shruti Agarwal,Siwei Lyu,Vishnu Suresh Lokhande*
<details>
  <summary>Abstract</summary>
Invisible watermarking of AI-generated images can help with copyright
protection, enabling detection and identification of AI-generated media. In
this work, we present a novel approach to watermark images of T2I Latent
Diffusion Models (LDMs). By only fine-tuning text token embeddings $W_*$, we
enable watermarking in selected objects or parts of the image, offering greater
flexibility compared to traditional full-image watermarking. Our method
leverages the text encoder's compatibility across various LDMs, allowing
plug-and-play integration for different LDMs. Moreover, introducing the
watermark early in the encoding stage improves robustness to adversarial
perturbations in later stages of the pipeline. Our approach achieves $99\%$ bit
accuracy ($48$ bits) with a $10^5 \times$ reduction in model parameters,
enabling efficient watermarking.
</details>

### [SPOC: Spatially-Progressing Object State Change Segmentation in Video](https://arxiv.org/abs/2503.11953)
*Priyanka Mandikal,Tushar Nagarajan,Alex Stoken,Zihui Xue,Kristen Grauman*
<details>
  <summary>Abstract</summary>
Object state changes in video reveal critical information about human and
agent activity. However, existing methods are limited to temporal localization
of when the object is in its initial state (e.g., the unchopped avocado) versus
when it has completed a state change (e.g., the chopped avocado), which limits
applicability for any task requiring detailed information about the progress of
the actions and its spatial localization. We propose to deepen the problem by
introducing the spatially-progressing object state change segmentation task.
The goal is to segment at the pixel-level those regions of an object that are
actionable and those that are transformed. We introduce the first model to
address this task, designing a VLM-based pseudo-labeling approach, state-change
dynamics constraints, and a novel WhereToChange benchmark built on in-the-wild
Internet videos. Experiments on two datasets validate both the challenge of the
new task as well as the promise of our model for localizing exactly where and
how fast objects are changing in video. We further demonstrate useful
implications for tracking activity progress to benefit robotic agents. Project
page: https://vision.cs.utexas.edu/projects/spoc-spatially-progressing-osc
</details>

### [CHOrD: Generation of Collision-Free, House-Scale, and Organized Digital Twins for 3D Indoor Scenes with Controllable Floor Plans and Optimal Layouts](https://arxiv.org/abs/2503.11958)
*Chong Su,Yingbin Fu,Zheyuan Hu,Jing Yang,Param Hanji,Shaojun Wang,Xuan Zhao,Cengiz √ñztireli,Fangcheng Zhong*
<details>
  <summary>Abstract</summary>
We introduce CHOrD, a novel framework for scalable synthesis of 3D indoor
scenes, designed to create house-scale, collision-free, and hierarchically
structured indoor digital twins. In contrast to existing methods that directly
synthesize the scene layout as a scene graph or object list, CHOrD incorporates
a 2D image-based intermediate layout representation, enabling effective
prevention of collision artifacts by successfully capturing them as
out-of-distribution (OOD) scenarios during generation. Furthermore, unlike
existing methods, CHOrD is capable of generating scene layouts that adhere to
complex floor plans with multi-modal controls, enabling the creation of
coherent, house-wide layouts robust to both geometric and semantic variations
in room structures. Additionally, we propose a novel dataset with expanded
coverage of household items and room configurations, as well as significantly
improved data quality. CHOrD demonstrates state-of-the-art performance on both
the 3D-FRONT and our proposed datasets, delivering photorealistic, spatially
coherent indoor scene synthesis adaptable to arbitrary floor plan variations.
</details>

### [Evaluation of Intra-operative Patient-specific Methods for Point Cloud Completion for Minimally Invasive Liver Interventions](https://arxiv.org/abs/2503.11969)
*Nakul Poudel,Zixin Yang,Kelly Merrell,Richard Simon,Cristian A. Linte*
<details>
  <summary>Abstract</summary>
The registration between the pre-operative model and the intra-operative
surface is crucial in image-guided liver surgery, as it facilitates the
effective use of pre-operative information during the procedure. However, the
intra-operative surface, usually represented as a point cloud, often has
limited coverage, especially in laparoscopic surgery, and is prone to holes and
noise, posing significant challenges for registration methods. Point cloud
completion methods have the potential to alleviate these issues. Thus, we
explore six state-of-the-art point cloud completion methods to identify the
optimal completion method for liver surgery applications. We focus on a
patient-specific approach for liver point cloud completion from a partial liver
surface under three cases: canonical pose, non-canonical pose, and canonical
pose with noise. The transformer-based method, AdaPoinTr, outperforms all other
methods to generate a complete point cloud from the given partial liver point
cloud under the canonical pose. On the other hand, our findings reveal
substantial performance degradation of these methods under non-canonical poses
and noisy settings, highlighting the limitations of these methods, which
suggests the need for a robust point completion method for its application in
image-guided liver surgery.
</details>

### [DynaGSLAM: Real-Time Gaussian-Splatting SLAM for Online Rendering, Tracking, Motion Predictions of Moving Objects in Dynamic Scenes](https://arxiv.org/abs/2503.11979)
*Runfa Blark Li,Mahdi Shaghaghi,Keito Suzuki,Xinshuang Liu,Varun Moparthi,Bang Du,Walker Curtis,Martin Renschler,Ki Myung Brian Lee,Nikolay Atanasov,Truong Nguyen*
<details>
  <summary>Abstract</summary>
Simultaneous Localization and Mapping (SLAM) is one of the most important
environment-perception and navigation algorithms for computer vision, robotics,
and autonomous cars/drones. Hence, high quality and fast mapping becomes a
fundamental problem. With the advent of 3D Gaussian Splatting (3DGS) as an
explicit representation with excellent rendering quality and speed,
state-of-the-art (SOTA) works introduce GS to SLAM. Compared to classical
pointcloud-SLAM, GS-SLAM generates photometric information by learning from
input camera views and synthesize unseen views with high-quality textures.
However, these GS-SLAM fail when moving objects occupy the scene that violate
the static assumption of bundle adjustment. The failed updates of moving GS
affects the static GS and contaminates the full map over long frames. Although
some efforts have been made by concurrent works to consider moving objects for
GS-SLAM, they simply detect and remove the moving regions from GS rendering
("anti'' dynamic GS-SLAM), where only the static background could benefit from
GS. To this end, we propose the first real-time GS-SLAM, "DynaGSLAM'', that
achieves high-quality online GS rendering, tracking, motion predictions of
moving objects in dynamic scenes while jointly estimating accurate ego motion.
Our DynaGSLAM outperforms SOTA static & "Anti'' dynamic GS-SLAM on three
dynamic real datasets, while keeping speed and memory efficiency in practice.
</details>

### [DecompDreamer: Advancing Structured 3D Asset Generation with Multi-Object Decomposition and Gaussian Splatting](https://arxiv.org/abs/2503.11981)
*Utkarsh Nath,Rajeev Goel,Rahul Khurana,Kyle Min,Mark Ollila,Pavan Turaga,Varun Jampani,Tejaswi Gowda*
<details>
  <summary>Abstract</summary>
Text-to-3D generation saw dramatic advances in recent years by leveraging
Text-to-Image models. However, most existing techniques struggle with
compositional prompts, which describe multiple objects and their spatial
relationships. They often fail to capture fine-grained inter-object
interactions. We introduce DecompDreamer, a Gaussian splatting-based training
routine designed to generate high-quality 3D compositions from such complex
prompts. DecompDreamer leverages Vision-Language Models (VLMs) to decompose
scenes into structured components and their relationships. We propose a
progressive optimization strategy that first prioritizes joint relationship
modeling before gradually shifting toward targeted object refinement. Our
qualitative and quantitative evaluations against state-of-the-art text-to-3D
models demonstrate that DecompDreamer effectively generates intricate 3D
compositions with superior object disentanglement, offering enhanced control
and flexibility in 3D generation. Project page :
https://decompdreamer3d.github.io
</details>

### [Fraesormer: Learning Adaptive Sparse Transformer for Efficient Food Recognition](https://arxiv.org/abs/2503.11995)
*Shun Zou,Yi Zou,Mingya Zhang,Shipeng Luo,Zhihao Chen,Guangwei Gao*
<details>
  <summary>Abstract</summary>
In recent years, Transformer has witnessed significant progress in food
recognition. However, most existing approaches still face two critical
challenges in lightweight food recognition: (1) the quadratic complexity and
redundant feature representation from interactions with irrelevant tokens; (2)
static feature recognition and single-scale representation, which overlook the
unstructured, non-fixed nature of food images and the need for multi-scale
features. To address these, we propose an adaptive and efficient sparse
Transformer architecture (Fraesormer) with two core designs: Adaptive Top-k
Sparse Partial Attention (ATK-SPA) and Hierarchical Scale-Sensitive Feature
Gating Network (HSSFGN). ATK-SPA uses a learnable Gated Dynamic Top-K Operator
(GDTKO) to retain critical attention scores, filtering low query-key matches
that hinder feature aggregation. It also introduces a partial channel mechanism
to reduce redundancy and promote expert information flow, enabling local-global
collaborative modeling. HSSFGN employs gating mechanism to achieve multi-scale
feature representation, enhancing contextual semantic information. Extensive
experiments show that Fraesormer outperforms state-of-the-art methods. code is
available at https://zs1314.github.io/Fraesormer.
</details>

### [3D Gaussian Splatting against Moving Objects for High-Fidelity Street Scene Reconstruction](https://arxiv.org/abs/2503.12001)
*Peizhen Zheng,Longfei Wei,Dongjing Jiang,Jianfei Zhang*
<details>
  <summary>Abstract</summary>
The accurate reconstruction of dynamic street scenes is critical for
applications in autonomous driving, augmented reality, and virtual reality.
Traditional methods relying on dense point clouds and triangular meshes
struggle with moving objects, occlusions, and real-time processing constraints,
limiting their effectiveness in complex urban environments. While multi-view
stereo and neural radiance fields have advanced 3D reconstruction, they face
challenges in computational efficiency and handling scene dynamics. This paper
proposes a novel 3D Gaussian point distribution method for dynamic street scene
reconstruction. Our approach introduces an adaptive transparency mechanism that
eliminates moving objects while preserving high-fidelity static scene details.
Additionally, iterative refinement of Gaussian point distribution enhances
geometric accuracy and texture representation. We integrate directional
encoding with spatial position optimization to optimize storage and rendering
efficiency, reducing redundancy while maintaining scene integrity. Experimental
results demonstrate that our method achieves high reconstruction quality,
improved rendering performance, and adaptability in large-scale dynamic
environments. These contributions establish a robust framework for real-time,
high-precision 3D reconstruction, advancing the practicality of dynamic scene
modeling across multiple applications. The source code for this work is
available to the public at https://github.com/deepcoxcom/3dgs
</details>

### [ROS-SAM: High-Quality Interactive Segmentation for Remote Sensing Moving Object](https://arxiv.org/abs/2503.12006)
*Zhe Shan,Yang Liu,Lei Zhou,Cheng Yan,Heng Wang,Xia Xie*
<details>
  <summary>Abstract</summary>
The availability of large-scale remote sensing video data underscores the
importance of high-quality interactive segmentation. However, challenges such
as small object sizes, ambiguous features, and limited generalization make it
difficult for current methods to achieve this goal. In this work, we propose
ROS-SAM, a method designed to achieve high-quality interactive segmentation
while preserving generalization across diverse remote sensing data. The ROS-SAM
is built upon three key innovations: 1) LoRA-based fine-tuning, which enables
efficient domain adaptation while maintaining SAM's generalization ability, 2)
Enhancement of deep network layers to improve the discriminability of extracted
features, thereby reducing misclassifications, and 3) Integration of global
context with local boundary details in the mask decoder to generate
high-quality segmentation masks. Additionally, we design the data pipeline to
ensure the model learns to better handle objects at varying scales during
training while focusing on high-quality predictions during inference.
Experiments on remote sensing video datasets show that the redesigned data
pipeline boosts the IoU by 6%, while ROS-SAM increases the IoU by 13%. Finally,
when evaluated on existing remote sensing object tracking datasets, ROS-SAM
demonstrates impressive zero-shot capabilities, generating masks that closely
resemble manual annotations. These results confirm ROS-SAM as a powerful tool
for fine-grained segmentation in remote sensing applications. Code is available
at https://github.com/ShanZard/ROS-SAM.
</details>

### [UniMamba: Unified Spatial-Channel Representation Learning with Group-Efficient Mamba for LiDAR-based 3D Object Detection](https://arxiv.org/abs/2503.12009)
*Xin Jin,Haisheng Su,Kai Liu,Cong Ma,Wei Wu,Fei Hui,Junchi Yan*
<details>
  <summary>Abstract</summary>
Recent advances in LiDAR 3D detection have demonstrated the effectiveness of
Transformer-based frameworks in capturing the global dependencies from point
cloud spaces, which serialize the 3D voxels into the flattened 1D sequence for
iterative self-attention. However, the spatial structure of 3D voxels will be
inevitably destroyed during the serialization process. Besides, due to the
considerable number of 3D voxels and quadratic complexity of Transformers,
multiple sequences are grouped before feeding to Transformers, leading to a
limited receptive field. Inspired by the impressive performance of State Space
Models (SSM) achieved in the field of 2D vision tasks, in this paper, we
propose a novel Unified Mamba (UniMamba), which seamlessly integrates the
merits of 3D convolution and SSM in a concise multi-head manner, aiming to
perform "local and global" spatial context aggregation efficiently and
simultaneously. Specifically, a UniMamba block is designed which mainly
consists of spatial locality modeling, complementary Z-order serialization and
local-global sequential aggregator. The spatial locality modeling module
integrates 3D submanifold convolution to capture the dynamic spatial position
embedding before serialization. Then the efficient Z-order curve is adopted for
serialization both horizontally and vertically. Furthermore, the local-global
sequential aggregator adopts the channel grouping strategy to efficiently
encode both "local and global" spatial inter-dependencies using multi-head SSM.
Additionally, an encoder-decoder architecture with stacked UniMamba blocks is
formed to facilitate multi-scale spatial learning hierarchically. Extensive
experiments are conducted on three popular datasets: nuScenes, Waymo and
Argoverse 2. Particularly, our UniMamba achieves 70.2 mAP on the nuScenes
dataset.
</details>

### [Learning Dual-Domain Multi-Scale Representations for Single Image Deraining](https://arxiv.org/abs/2503.12014)
*Shun Zou,Yi Zou,Mingya Zhang,Shipeng Luo,Guangwei Gao,Guojun Qi*
<details>
  <summary>Abstract</summary>
Existing image deraining methods typically rely on single-input,
single-output, and single-scale architectures, which overlook the joint
multi-scale information between external and internal features. Furthermore,
single-domain representations are often too restrictive, limiting their ability
to handle the complexities of real-world rain scenarios. To address these
challenges, we propose a novel Dual-Domain Multi-Scale Representation Network
(DMSR). The key idea is to exploit joint multi-scale representations from both
external and internal domains in parallel while leveraging the strengths of
both spatial and frequency domains to capture more comprehensive properties.
Specifically, our method consists of two main components: the Multi-Scale
Progressive Spatial Refinement Module (MPSRM) and the Frequency Domain Scale
Mixer (FDSM). The MPSRM enables the interaction and coupling of multi-scale
expert information within the internal domain using a hierarchical modulation
and fusion strategy. The FDSM extracts multi-scale local information in the
spatial domain, while also modeling global dependencies in the frequency
domain. Extensive experiments show that our model achieves state-of-the-art
performance across six benchmark datasets.
</details>

### [QDM: Quadtree-Based Region-Adaptive Sparse Diffusion Models for Efficient Image Super-Resolution](https://arxiv.org/abs/2503.12015)
*Donglin Yang,Paul Vicol,Xiaojuan Qi,Renjie Liao,Xiaofan Zhang*
<details>
  <summary>Abstract</summary>
Deep learning-based super-resolution (SR) methods often perform pixel-wise
computations uniformly across entire images, even in homogeneous regions where
high-resolution refinement is redundant. We propose the Quadtree Diffusion
Model (QDM), a region-adaptive diffusion framework that leverages a quadtree
structure to selectively enhance detail-rich regions while reducing
computations in homogeneous areas. By guiding the diffusion with a quadtree
derived from the low-quality input, QDM identifies key regions-represented by
leaf nodes-where fine detail is essential and applies minimal refinement
elsewhere. This mask-guided, two-stream architecture adaptively balances
quality and efficiency, producing high-fidelity outputs with low computational
redundancy. Experiments demonstrate QDM's effectiveness in high-resolution SR
tasks across diverse image types, particularly in medical imaging (e.g., CT
scans), where large homogeneous regions are prevalent. Furthermore, QDM
outperforms or is comparable to state-of-the-art SR methods on standard
benchmarks while significantly reducing computational costs, highlighting its
efficiency and suitability for resource-limited environments. Our code is
available at https://github.com/linYDTHU/QDM.
</details>

### [Compose Your Aesthetics: Empowering Text-to-Image Models with the Principles of Art](https://arxiv.org/abs/2503.12018)
*Zhe Jin,Tat-Seng Chua*
<details>
  <summary>Abstract</summary>
Text-to-Image (T2I) diffusion models (DM) have garnered widespread adoption
due to their capability in generating high-fidelity outputs and accessibility
to anyone able to put imagination into words. However, DMs are often
predisposed to generate unappealing outputs, much like the random images on the
internet they were trained on. Existing approaches to address this are founded
on the implicit premise that visual aesthetics is universal, which is limiting.
Aesthetics in the T2I context should be about personalization and we propose
the novel task of aesthetics alignment which seeks to align user-specified
aesthetics with the T2I generation output. Inspired by how artworks provide an
invaluable perspective to approach aesthetics, we codify visual aesthetics
using the compositional framework artists employ, known as the Principles of
Art (PoA). To facilitate this study, we introduce CompArt, a large-scale
compositional art dataset building on top of WikiArt with PoA analysis
annotated by a capable Multimodal LLM. Leveraging the expressive power of LLMs
and training a lightweight and transferrable adapter, we demonstrate that T2I
DMs can effectively offer 10 compositional controls through user-specified PoA
conditions. Additionally, we design an appropriate evaluation framework to
assess the efficacy of our approach.
</details>

### [SteerX: Creating Any Camera-Free 3D and 4D Scenes with Geometric Steering](https://arxiv.org/abs/2503.12024)
*Byeongjun Park,Hyojun Go,Hyelin Nam,Byung-Hoon Kim,Hyungjin Chung,Changick Kim*
<details>
  <summary>Abstract</summary>
Recent progress in 3D/4D scene generation emphasizes the importance of
physical alignment throughout video generation and scene reconstruction.
However, existing methods improve the alignment separately at each stage,
making it difficult to manage subtle misalignments arising from another stage.
Here, we present SteerX, a zero-shot inference-time steering method that
unifies scene reconstruction into the generation process, tilting data
distributions toward better geometric alignment. To this end, we introduce two
geometric reward functions for 3D/4D scene generation by using pose-free
feed-forward scene reconstruction models. Through extensive experiments, we
demonstrate the effectiveness of SteerX in improving 3D/4D scene generation.
</details>

### [Leveraging Motion Information for Better Self-Supervised Video Correspondence Learning](https://arxiv.org/abs/2503.12026)
*Zihan Zhoua,Changrui Daia,Aibo Songa,Xiaolin Fang*
<details>
  <summary>Abstract</summary>
Self-supervised video correspondence learning depends on the ability to
accurately associate pixels between video frames that correspond to the same
visual object. However, achieving reliable pixel matching without supervision
remains a major challenge. To address this issue, recent research has focused
on feature learning techniques that aim to encode unique pixel representations
for matching. Despite these advances, existing methods still struggle to
achieve exact pixel correspondences and often suffer from false matches,
limiting their effectiveness in self-supervised settings.
  To this end, we explore an efficient self-supervised Video Correspondence
Learning framework (MER) that aims to accurately extract object details from
unlabeled videos. First, we design a dedicated Motion Enhancement Engine that
emphasizes capturing the dynamic motion of objects in videos. In addition, we
introduce a flexible sampling strategy for inter-pixel correspondence
information (Multi-Cluster Sampler) that enables the model to pay more
attention to the pixel changes of important objects in motion. Through
experiments, our algorithm outperforms the state-of-the-art competitors on
video correspondence learning tasks such as video object segmentation and video
object keypoint tracking.
</details>

### [Challenges in Plane Symmetry: From Theory to Perception](https://arxiv.org/abs/2503.12028)
*F. √áengel,V. Adanova,S. Tari*
<details>
  <summary>Abstract</summary>
The planar ornaments are created by repeating a base unit using a combination
of four primitive geometric operations: translation, rotation, reflection, and
glide reflection. According to group theory, different combinations of these
four geometric operations lead to different symmetry groups. In this work, we
select a single challenging ornament, and analyze it both from the theoretical
point of view and perceptual point of view. We present the perceptual
experiment results, where one can see that the symmetries that the participants
perceived from the ornaments do not match to what the theory dictates.
</details>

### [Real-Time Manipulation Action Recognition with a Factorized Graph Sequence Encoder](https://arxiv.org/abs/2503.12034)
*Enes Erdogan,Eren Erdal Aksoy,Sanem Sariel*
<details>
  <summary>Abstract</summary>
Recognition of human manipulation actions in real-time is essential for safe
and effective human-robot interaction and collaboration. The challenge lies in
developing a model that is both lightweight enough for real-time execution and
capable of generalization. While some existing methods in the literature can
run in real-time, they struggle with temporal scalability, i.e., they fail to
adapt to long-duration manipulations effectively. To address this, leveraging
the generalizable scene graph representations, we propose a new Factorized
Graph Sequence Encoder network that not only runs in real-time but also scales
effectively in the temporal dimension, thanks to its factorized encoder
architecture. Additionally, we introduce Hand Pooling operation, a simple
pooling operation for more focused extraction of the graph-level embeddings.
Our model outperforms the previous state-of-the-art real-time approach,
achieving a 14.3\% and 5.6\% improvement in F1-macro score on the KIT Bimanual
Action (Bimacs) Dataset and Collaborative Action (CoAx) Dataset, respectively.
Moreover, we conduct an extensive ablation study to validate our network design
choices. Finally, we compare our model with its architecturally similar
RGB-based model on the Bimacs dataset and show the limitations of this model in
contrast to ours on such an object-centric manipulation dataset.
</details>

### [MOS: Modeling Object-Scene Associations in Generalized Category Discovery](https://arxiv.org/abs/2503.12035)
*Zhengyuan Peng,Jinpeng Ma,Zhimin Sun,Ran Yi,Haichuan Song,Xin Tan,Lizhuang Ma*
<details>
  <summary>Abstract</summary>
Generalized Category Discovery (GCD) is a classification task that aims to
classify both base and novel classes in unlabeled images, using knowledge from
a labeled dataset. In GCD, previous research overlooks scene information or
treats it as noise, reducing its impact during model training. However, in this
paper, we argue that scene information should be viewed as a strong prior for
inferring novel classes. We attribute the misinterpretation of scene
information to a key factor: the Ambiguity Challenge inherent in GCD.
Specifically, novel objects in base scenes might be wrongly classified into
base categories, while base objects in novel scenes might be mistakenly
recognized as novel categories. Once the ambiguity challenge is addressed,
scene information can reach its full potential, significantly enhancing the
performance of GCD models. To more effectively leverage scene information, we
propose the Modeling Object-Scene Associations (MOS) framework, which utilizes
a simple MLP-based scene-awareness module to enhance GCD performance. It
achieves an exceptional average accuracy improvement of 4% on the challenging
fine-grained datasets compared to state-of-the-art methods, emphasizing its
superior performance in fine-grained GCD. The code is publicly available at
https://github.com/JethroPeng/MOS.
</details>

### [PSGait: Multimodal Gait Recognition using Parsing Skeleton](https://arxiv.org/abs/2503.12047)
*Hangrui Xu,Chuanrui Zhang,Zhengxian Wu,Peng Jiao,Haoqian Wang*
<details>
  <summary>Abstract</summary>
Gait recognition has emerged as a robust biometric modality due to its
non-intrusive nature and resilience to occlusion. Conventional gait recognition
methods typically rely on silhouettes or skeletons. Despite their success in
gait recognition for controlled laboratory environments, they usually fail in
real-world scenarios due to their limited information entropy for gait
representations. To achieve accurate gait recognition in the wild, we propose a
novel gait representation, named Parsing Skeleton. This representation
innovatively introduces the skeleton-guided human parsing method to capture
fine-grained body dynamics, so they have much higher information entropy to
encode the shapes and dynamics of fine-grained human parts during walking.
Moreover, to effectively explore the capability of the parsing skeleton
representation, we propose a novel parsing skeleton-based gait recognition
framework, named PSGait, which takes parsing skeletons and silhouettes as
input. By fusing these two modalities, the resulting image sequences are fed
into gait recognition models for enhanced individual differentiation. We
conduct comprehensive benchmarks on various datasets to evaluate our model.
PSGait outperforms existing state-of-the-art multimodal methods. Furthermore,
as a plug-and-play method, PSGait leads to a maximum improvement of 10.9% in
Rank-1 accuracy across various gait recognition models. These results
demonstrate the effectiveness and versatility of parsing skeletons for gait
recognition in the wild, establishing PSGait as a new state-of-the-art approach
for multimodal gait recognition.
</details>

### [TACO: Taming Diffusion for in-the-wild Video Amodal Completion](https://arxiv.org/abs/2503.12049)
*Ruijie Lu,Yixin Chen,Yu Liu,Jiaxiang Tang,Junfeng Ni,Diwen Wan,Gang Zeng,Siyuan Huang*
<details>
  <summary>Abstract</summary>
Humans can infer complete shapes and appearances of objects from limited
visual cues, relying on extensive prior knowledge of the physical world.
However, completing partially observable objects while ensuring consistency
across video frames remains challenging for existing models, especially for
unstructured, in-the-wild videos. This paper tackles the task of Video Amodal
Completion (VAC), which aims to generate the complete object consistently
throughout the video given a visual prompt specifying the object of interest.
Leveraging the rich, consistent manifolds learned by pre-trained video
diffusion models, we propose a conditional diffusion model, TACO, that
repurposes these manifolds for VAC. To enable its effective and robust
generalization to challenging in-the-wild scenarios, we curate a large-scale
synthetic dataset with multiple difficulty levels by systematically imposing
occlusions onto un-occluded videos. Building on this, we devise a progressive
fine-tuning paradigm that starts with simpler recovery tasks and gradually
advances to more complex ones. We demonstrate TACO's versatility on a wide
range of in-the-wild videos from Internet, as well as on diverse, unseen
datasets commonly used in autonomous driving, robotic manipulation, and scene
understanding. Moreover, we show that TACO can be effectively applied to
various downstream tasks like object reconstruction and pose estimation,
highlighting its potential to facilitate physical world understanding and
reasoning. Our project page is available at https://jason-aplp.github.io/TACO.
</details>

### [Tailor: An Integrated Text-Driven CG-Ready Human and Garment Generation System](https://arxiv.org/abs/2503.12052)
*Zhiyao Sun,Yu-Hui Wen,Matthieu Lin,Ho-Jui Fang,Sheng Ye,Tian Lv,Yong-Jin Liu*
<details>
  <summary>Abstract</summary>
Creating detailed 3D human avatars with garments typically requires
specialized expertise and labor-intensive processes. Although recent advances
in generative AI have enabled text-to-3D human/clothing generation, current
methods fall short in offering accessible, integrated pipelines for producing
ready-to-use clothed avatars. To solve this, we introduce Tailor, an integrated
text-to-avatar system that generates high-fidelity, customizable 3D humans with
simulation-ready garments. Our system includes a three-stage pipeline. We first
employ a large language model to interpret textual descriptions into
parameterized body shapes and semantically matched garment templates. Next, we
develop topology-preserving deformation with novel geometric losses to adapt
garments precisely to body geometries. Furthermore, an enhanced texture
diffusion module with a symmetric local attention mechanism ensures both view
consistency and photorealistic details. Quantitative and qualitative
evaluations demonstrate that Tailor outperforms existing SoTA methods in terms
of fidelity, usability, and diversity. Code will be available for academic use.
</details>

### [EHNet: An Efficient Hybrid Network for Crowd Counting and Localization](https://arxiv.org/abs/2503.12061)
*Yuqing Yan,Yirui Wu*
<details>
  <summary>Abstract</summary>
In recent years, crowd counting and localization have become crucial
techniques in computer vision, with applications spanning various domains. The
presence of multi-scale crowd distributions within a single image remains a
fundamental challenge in crowd counting tasks. To address these challenges, we
introduce the Efficient Hybrid Network (EHNet), a novel framework for efficient
crowd counting and localization. By reformulating crowd counting into a point
regression framework, EHNet leverages the Spatial-Position Attention Module
(SPAM) to capture comprehensive spatial contexts and long-range dependencies.
Additionally, we develop an Adaptive Feature Aggregation Module (AFAM) to
effectively fuse and harmonize multi-scale feature representations. Building
upon these, we introduce the Multi-Scale Attentive Decoder (MSAD). Experimental
results on four benchmark datasets demonstrate that EHNet achieves competitive
performance with reduced computational overhead, outperforming existing methods
on ShanghaiTech Part \_A, ShanghaiTech Part \_B, UCF-CC-50, and UCF-QNRF. Our
code is in https://anonymous.4open.science/r/EHNet.
</details>

### [DLA-Count: Dynamic Label Assignment Network for Dense Cell Distribution Counting](https://arxiv.org/abs/2503.12063)
*Yuqing Yan,Yirui Wu*
<details>
  <summary>Abstract</summary>
Cell counting remains a fundamental yet challenging task in medical and
biological research due to the diverse morphology of cells, their dense
distribution, and variations in image quality. We present DLA-Count, a
breakthrough approach to cell counting that introduces three key innovations:
(1) K-adjacent Hungarian Matching (KHM), which dramatically improves cell
matching in dense regions, (2) Multi-scale Deformable Gaussian Convolution
(MDGC), which adapts to varying cell morphologies, and (3) Gaussian-enhanced
Feature Decoder (GFD) for efficient multi-scale feature fusion. Our extensive
experiments on four challenging cell counting datasets (ADI, MBM, VGG, and DCC)
demonstrate that our method outperforms previous methods across diverse
datasets, with improvements in Mean Absolute Error of up to 46.7\% on ADI and
42.5\% on MBM datasets. Our code is available at
https://anonymous.4open.science/r/DLA-Count.
</details>

### [A Comprehensive Survey on Knowledge Distillation](https://arxiv.org/abs/2503.12067)
*Amir M. Mansourian,Rozhan Ahmadi,Masoud Ghafouri,Amir Mohammad Babaei,Elaheh Badali Golezani,Zeynab Yasamani Ghamchi,Vida Ramezanian,Alireza Taherian,Kimia Dinashi,Amirali Miri,Shohreh Kasaei*
<details>
  <summary>Abstract</summary>
Deep Neural Networks (DNNs) have achieved notable performance in the fields
of computer vision and natural language processing with various applications in
both academia and industry. However, with recent advancements in DNNs and
transformer models with a tremendous number of parameters, deploying these
large models on edge devices causes serious issues such as high runtime and
memory consumption. This is especially concerning with the recent large-scale
foundation models, Vision-Language Models (VLMs), and Large Language Models
(LLMs). Knowledge Distillation (KD) is one of the prominent techniques proposed
to address the aforementioned problems using a teacher-student architecture.
More specifically, a lightweight student model is trained using additional
knowledge from a cumbersome teacher model. In this work, a comprehensive survey
of knowledge distillation methods is proposed. This includes reviewing KD from
different aspects: distillation sources, distillation schemes, distillation
algorithms, distillation by modalities, applications of distillation, and
comparison among existing methods. In contrast to most existing surveys, which
are either outdated or simply update former surveys, this work proposes a
comprehensive survey with a new point of view and representation structure that
categorizes and investigates the most recent methods in knowledge distillation.
This survey considers various critically important subcategories, including KD
for diffusion models, 3D inputs, foundational models, transformers, and LLMs.
Furthermore, existing challenges in KD and possible future research directions
are discussed. Github page of the project:
https://github.com/IPL-Sharif/KD_Survey
</details>

### [Prototype-Based Image Prompting for Weakly Supervised Histopathological Image Segmentation](https://arxiv.org/abs/2503.12068)
*Qingchen Tang,Lei Fan,Maurice Pagnucco,Yang Song*
<details>
  <summary>Abstract</summary>
Weakly supervised image segmentation with image-level labels has drawn
attention due to the high cost of pixel-level annotations. Traditional methods
using Class Activation Maps (CAMs) often highlight only the most discriminative
regions, leading to incomplete masks. Recent approaches that introduce textual
information struggle with histopathological images due to inter-class
homogeneity and intra-class heterogeneity. In this paper, we propose a
prototype-based image prompting framework for histopathological image
segmentation. It constructs an image bank from the training set using
clustering, extracting multiple prototype features per class to capture
intra-class heterogeneity. By designing a matching loss between input features
and class-specific prototypes using contrastive learning, our method addresses
inter-class homogeneity and guides the model to generate more accurate CAMs.
Experiments on four datasets (LUAD-HistoSeg, BCSS-WSSS, GCSS, and BCSS) show
that our method outperforms existing weakly supervised segmentation approaches,
setting new benchmarks in histopathological image segmentation.
</details>

### [Robust Dataset Distillation by Matching Adversarial Trajectories](https://arxiv.org/abs/2503.12069)
*Wei Lai,Tianyu Ding,ren dongdong,Lei Wang,Jing Huo,Yang Gao,Wenbin Li*
<details>
  <summary>Abstract</summary>
Dataset distillation synthesizes compact datasets that enable models to
achieve performance comparable to training on the original large-scale
datasets. However, existing distillation methods overlook the robustness of the
model, resulting in models that are vulnerable to adversarial attacks when
trained on distilled data. To address this limitation, we introduce the task of
``robust dataset distillation", a novel paradigm that embeds adversarial
robustness into the synthetic datasets during the distillation process. We
propose Matching Adversarial Trajectories (MAT), a method that integrates
adversarial training into trajectory-based dataset distillation. MAT
incorporates adversarial samples during trajectory generation to obtain robust
training trajectories, which are then used to guide the distillation process.
As experimentally demonstrated, even through natural training on our distilled
dataset, models can achieve enhanced adversarial robustness while maintaining
competitive accuracy compared to existing distillation methods. Our work
highlights robust dataset distillation as a new and important research
direction and provides a strong baseline for future research to bridge the gap
between efficient training and adversarial robustness.
</details>

### [V-Stylist: Video Stylization via Collaboration and Reflection of MLLM Agents](https://arxiv.org/abs/2503.12077)
*Zhengrong Yue,Shaobin Zhuang,Kunchang Li,Yanbo Ding,Yali Wang*
<details>
  <summary>Abstract</summary>
Despite the recent advancement in video stylization, most existing methods
struggle to render any video with complex transitions, based on an open style
description of user query. To fill this gap, we introduce a generic multi-agent
system for video stylization, V-Stylist, by a novel collaboration and
reflection paradigm of multi-modal large language models. Specifically, our
V-Stylist is a systematical workflow with three key roles: (1) Video Parser
decomposes the input video into a number of shots and generates their text
prompts of key shot content. Via a concise video-to-shot prompting paradigm, it
allows our V-Stylist to effectively handle videos with complex transitions. (2)
Style Parser identifies the style in the user query and progressively search
the matched style model from a style tree. Via a robust tree-of-thought
searching paradigm, it allows our V-Stylist to precisely specify vague style
preference in the open user query. (3) Style Artist leverages the matched model
to render all the video shots into the required style. Via a novel multi-round
self-reflection paradigm, it allows our V-Stylist to adaptively adjust detail
control, according to the style requirement. With such a distinct design of
mimicking human professionals, our V-Stylist achieves a major breakthrough over
the primary challenges for effective and automatic video stylization.
Moreover,we further construct a new benchmark Text-driven Video Stylization
Benchmark (TVSBench), which fills the gap to assess stylization of complex
videos on open user queries. Extensive experiments show that, V-Stylist
achieves the state-of-the-art, e.g.,V-Stylist surpasses FRESCO and ControlVideo
by 6.05% and 4.51% respectively in overall average metrics, marking a
significant advance in video stylization.
</details>

### [FA-BARF: Frequency Adapted Bundle-Adjusting Neural Radiance Fields](https://arxiv.org/abs/2503.12086)
*Rui Qian,Chenyangguang Zhang,Yan Di,Guangyao Zhai,Ruida Zhang,Jiayu Guo,Benjamin Busam,Jian Pu*
<details>
  <summary>Abstract</summary>
Neural Radiance Fields (NeRF) have exhibited highly effective performance for
photorealistic novel view synthesis recently. However, the key limitation it
meets is the reliance on a hand-crafted frequency annealing strategy to recover
3D scenes with imperfect camera poses. The strategy exploits a temporal
low-pass filter to guarantee convergence while decelerating the joint
optimization of implicit scene reconstruction and camera registration. In this
work, we introduce the Frequency Adapted Bundle Adjusting Radiance Field
(FA-BARF), substituting the temporal low-pass filter for a frequency-adapted
spatial low-pass filter to address the decelerating problem. We establish a
theoretical framework to interpret the relationship between position encoding
of NeRF and camera registration and show that our frequency-adapted filter can
mitigate frequency fluctuation caused by the temporal filter. Furthermore, we
show that applying a spatial low-pass filter in NeRF can optimize camera poses
productively through radial uncertainty overlaps among various views. Extensive
experiments show that FA-BARF can accelerate the joint optimization process
under little perturbations in object-centric scenes and recover real-world
scenes with unknown camera poses. This implies wider possibilities for NeRF
applied in dense 3D mapping and reconstruction under real-time requirements.
The code will be released upon paper acceptance.
</details>

### [Temporally Consistent Mitral Annulus Measurements from Sparse Annotations in Echocardiographic Videos](https://arxiv.org/abs/2503.12087)
*Gino E. Jansen,Mark J. Schuuring,Berto J. Bouma,Ivana I≈°gum*
<details>
  <summary>Abstract</summary>
This work presents a novel approach to achieving temporally consistent mitral
annulus landmark localization in echocardiography videos using sparse
annotations. Our method introduces a self-supervised loss term that enforces
temporal consistency between neighboring frames, which smooths the position of
landmarks and enhances measurement accuracy over time. Additionally, we
incorporate realistic field-of-view augmentations to improve the recognition of
missing anatomical landmarks. We evaluate our approach on both a public and
private dataset, and demonstrate significant improvements in Mitral Annular
Plane Systolic Excursion (MAPSE) calculations and overall landmark tracking
stability. The method achieves a mean absolute MAPSE error of 1.81 $\pm$ 0.14
mm, an annulus size error of 2.46 $\pm$ 0.31 mm, and a landmark localization
error of 2.48 $\pm$ 0.07 mm. Finally, it achieves a 0.99 ROC-AUC for
recognition of missing landmarks.
</details>

### [SFMNet: Sparse Focal Modulation for 3D Object Detection](https://arxiv.org/abs/2503.12093)
*Oren Shrout,Ayellet Tal*
<details>
  <summary>Abstract</summary>
We propose SFMNet, a novel 3D sparse detector that combines the efficiency of
sparse convolutions with the ability to model long-range dependencies. While
traditional sparse convolution techniques efficiently capture local structures,
they struggle with modeling long-range relationships. However, capturing
long-range dependencies is fundamental for 3D object detection. In contrast,
transformers are designed to capture these long-range dependencies through
attention mechanisms. But, they come with high computational costs, due to
their quadratic query-key-value interactions. Furthermore, directly applying
attention to non-empty voxels is inefficient due to the sparse nature of 3D
scenes. Our SFMNet is built on a novel Sparse Focal Modulation (SFM) module,
which integrates short- and long-range contexts with linear complexity by
leveraging a new hierarchical sparse convolution design. This approach enables
SFMNet to achieve high detection performance with improved efficiency, making
it well-suited for large-scale LiDAR scenes. We show that our detector achieves
state-of-the-art performance on autonomous driving datasets.
</details>

### [E-SAM: Training-Free Segment Every Entity Model](https://arxiv.org/abs/2503.12094)
*Weiming Zhang,Dingwen Xiao,Lei Chen,Lin Wang*
<details>
  <summary>Abstract</summary>
Entity Segmentation (ES) aims at identifying and segmenting distinct entities
within an image without the need for predefined class labels. This
characteristic makes ES well-suited to open-world applications with adaptation
to diverse and dynamically changing environments, where new and previously
unseen entities may appear frequently. Existing ES methods either require large
annotated datasets or high training costs, limiting their scalability and
adaptability. Recently, the Segment Anything Model (SAM), especially in its
Automatic Mask Generation (AMG) mode, has shown potential for holistic image
segmentation. However, it struggles with over-segmentation and
under-segmentation, making it less effective for ES. In this paper, we
introduce E-SAM, a novel training-free framework that exhibits exceptional ES
capability. Specifically, we first propose Multi-level Mask Generation (MMG)
that hierarchically processes SAM's AMG outputs to generate reliable
object-level masks while preserving fine details at other levels. Entity-level
Mask Refinement (EMR) then refines these object-level masks into accurate
entity-level masks. That is, it separates overlapping masks to address the
redundancy issues inherent in SAM's outputs and merges similar masks by
evaluating entity-level consistency. Lastly, Under-Segmentation Refinement
(USR) addresses under-segmentation by generating additional high-confidence
masks fused with EMR outputs to produce the final ES map. These three modules
are seamlessly optimized to achieve the best ES without additional training
overhead. Extensive experiments demonstrate that E-SAM achieves
state-of-the-art performance compared to prior ES methods, demonstrating a
significant improvement by +30.1 on benchmark metrics.
</details>

### [Towards Vision Zero: The Accid3nD Dataset](https://arxiv.org/abs/2503.12095)
*Walter Zimmer,Ross Greer,Daniel Lehmberg,Marc Pavel,Holger Caesar,Xingcheng Zhou,Ahmed Ghita,Mohan Trivedi,Rui Song,Hu Cao,Akshay Gopalkrishnan,Alois C. Knoll*
<details>
  <summary>Abstract</summary>
Even though a significant amount of work has been done to increase the safety
of transportation networks, accidents still occur regularly. They must be
understood as unavoidable and sporadic outcomes of traffic networks. No public
dataset contains 3D annotations of real-world accidents recorded from roadside
sensors. We present the Accid3nD dataset, a collection of real-world highway
accidents in different weather and lighting conditions. It contains vehicle
crashes at high-speed driving with 2,634,233 labeled 2D bounding boxes,
instance masks, and 3D bounding boxes with track IDs. In total, the dataset
contains 111,945 labeled frames recorded from four roadside cameras and LiDARs
at 25 Hz. The dataset contains six object classes and is provided in the
OpenLABEL format. We propose an accident detection model that combines a
rule-based approach with a learning-based one. Experiments and ablation studies
on our dataset show the robustness of our proposed method. The dataset, model,
and code are available on our website: https://accident-dataset.github.io.
</details>

### [O-TPT: Orthogonality Constraints for Calibrating Test-time Prompt Tuning in Vision-Language Models](https://arxiv.org/abs/2503.12096)
*Ashshak Sharifdeen,Muhammad Akhtar Munir,Sanoojan Baliah,Salman Khan,Muhammad Haris Khan*
<details>
  <summary>Abstract</summary>
Test-time prompt tuning for vision-language models (VLMs) is getting
attention because of their ability to learn with unlabeled data without
fine-tuning. Although test-time prompt tuning methods for VLMs can boost
accuracy, the resulting models tend to demonstrate poor calibration, which
casts doubts on the reliability and trustworthiness of these models. Notably,
more attention needs to be devoted to calibrating the test-time prompt tuning
in vision-language models. To this end, we propose a new approach, called O-TPT
that introduces orthogonality constraints on the textual features corresponding
to the learnable prompts for calibrating test-time prompt tuning in VLMs.
Towards introducing orthogonality constraints, we make the following
contributions. First, we uncover new insights behind the suboptimal calibration
performance of existing methods relying on textual feature dispersion. Second,
we show that imposing a simple orthogonalization of textual features is a more
effective approach towards obtaining textual dispersion. We conduct extensive
experiments on various datasets with different backbones and baselines. The
results indicate that our method consistently outperforms the prior state of
the art in significantly reducing the overall average calibration error. Also,
our method surpasses the zero-shot calibration performance on fine-grained
classification tasks.
</details>

### [A Speech-to-Video Synthesis Approach Using Spatio-Temporal Diffusion for Vocal Tract MRI](https://arxiv.org/abs/2503.12102)
*Paula Andrea P√©rez-Toro,Tom√°s Arias-Vergara,Fangxu Xing,Xiaofeng Liu,Maureen Stone,Jiachen Zhuo,Juan Rafael Orozco-Arroyave,Elmar N√∂th,Jana Hutter,Jerry L. Prince,Andreas Maier,Jonghye Woo*
<details>
  <summary>Abstract</summary>
Understanding the relationship between vocal tract motion during speech and
the resulting acoustic signal is crucial for aided clinical assessment and
developing personalized treatment and rehabilitation strategies. Toward this
goal, we introduce an audio-to-video generation framework for creating Real
Time/cine-Magnetic Resonance Imaging (RT-/cine-MRI) visuals of the vocal tract
from speech signals. Our framework first preprocesses RT-/cine-MRI sequences
and speech samples to achieve temporal alignment, ensuring synchronization
between visual and audio data. We then employ a modified stable diffusion
model, integrating structural and temporal blocks, to effectively capture
movement characteristics and temporal dynamics in the synchronized data. This
process enables the generation of MRI sequences from new speech inputs,
improving the conversion of audio into visual data. We evaluated our framework
on healthy controls and tongue cancer patients by analyzing and comparing the
vocal tract movements in synthesized videos. Our framework demonstrated
adaptability to new speech inputs and effective generalization. In addition,
positive human evaluations confirmed its effectiveness, with realistic and
accurate visualizations, suggesting its potential for outpatient therapy and
personalized simulation of vocal tract visualizations.
</details>

### [Z-Magic: Zero-shot Multiple Attributes Guided Image Creator](https://arxiv.org/abs/2503.12124)
*Yingying Deng,Xiangyu He,Fan Tang,Weiming Dong*
<details>
  <summary>Abstract</summary>
The customization of multiple attributes has gained popularity with the
rising demand for personalized content creation. Despite promising empirical
results, the contextual coherence between different attributes has been largely
overlooked. In this paper, we argue that subsequent attributes should follow
the multivariable conditional distribution introduced by former attribute
creation. In light of this, we reformulate multi-attribute creation from a
conditional probability theory perspective and tackle the challenging zero-shot
setting. By explicitly modeling the dependencies between attributes, we further
enhance the coherence of generated images across diverse attribute
combinations. Furthermore, we identify connections between multi-attribute
customization and multi-task learning, effectively addressing the high
computing cost encountered in multi-attribute synthesis. Extensive experiments
demonstrate that Z-Magic outperforms existing models in zero-shot image
generation, with broad implications for AI-driven design and creative
applications.
</details>

### [Hyperbolic Safety-Aware Vision-Language Models](https://arxiv.org/abs/2503.12127)
*Tobia Poppi,Tejaswi Kasarla,Pascal Mettes,Lorenzo Baraldi,Rita Cucchiara*
<details>
  <summary>Abstract</summary>
Addressing the retrieval of unsafe content from vision-language models such
as CLIP is an important step towards real-world integration. Current efforts
have relied on unlearning techniques that try to erase the model's knowledge of
unsafe concepts. While effective in reducing unwanted outputs, unlearning
limits the model's capacity to discern between safe and unsafe content. In this
work, we introduce a novel approach that shifts from unlearning to an awareness
paradigm by leveraging the inherent hierarchical properties of the hyperbolic
space. We propose to encode safe and unsafe content as an entailment hierarchy,
where both are placed in different regions of hyperbolic space. Our HySAC,
Hyperbolic Safety-Aware CLIP, employs entailment loss functions to model the
hierarchical and asymmetrical relations between safe and unsafe image-text
pairs. This modelling, ineffective in standard vision-language models due to
their reliance on Euclidean embeddings, endows the model with awareness of
unsafe content, enabling it to serve as both a multimodal unsafe classifier and
a flexible content retriever, with the option to dynamically redirect unsafe
queries toward safer alternatives or retain the original output. Extensive
experiments show that our approach not only enhances safety recognition but
also establishes a more adaptable and interpretable framework for content
moderation in vision-language models. Our source code is available at
https://github.com/aimagelab/HySAC.
</details>

### [DiffGAP: A Lightweight Diffusion Module in Contrastive Space for Bridging Cross-Model Gap](https://arxiv.org/abs/2503.12131)
*Shentong Mo,Zehua Chen,Fan Bao,Jun Zhu*
<details>
  <summary>Abstract</summary>
Recent works in cross-modal understanding and generation, notably through
models like CLAP (Contrastive Language-Audio Pretraining) and CAVP (Contrastive
Audio-Visual Pretraining), have significantly enhanced the alignment of text,
video, and audio embeddings via a single contrastive loss. However, these
methods often overlook the bidirectional interactions and inherent noises
present in each modality, which can crucially impact the quality and efficacy
of cross-modal integration. To address this limitation, we introduce DiffGAP, a
novel approach incorporating a lightweight generative module within the
contrastive space. Specifically, our DiffGAP employs a bidirectional diffusion
process tailored to bridge the cross-modal gap more effectively. This involves
a denoising process on text and video embeddings conditioned on audio
embeddings and vice versa, thus facilitating a more nuanced and robust
cross-modal interaction. Our experimental results on VGGSound and AudioCaps
datasets demonstrate that DiffGAP significantly improves performance in
video/text-audio generation and retrieval tasks, confirming its effectiveness
in enhancing cross-modal understanding and generation capabilities.
</details>

### [Point-Cache: Test-time Dynamic and Hierarchical Cache for Robust and Generalizable Point Cloud Analysis](https://arxiv.org/abs/2503.12150)
*Hongyu Sun,Qiuhong Ke,Ming Cheng,Yongcai Wang,Deying Li,Chenhui Gou,Jianfei Cai*
<details>
  <summary>Abstract</summary>
This paper proposes a general solution to enable point cloud recognition
models to handle distribution shifts at test time. Unlike prior methods, which
rely heavily on training data-often inaccessible during online inference-and
are limited to recognizing a fixed set of point cloud classes predefined during
training, we explore a more practical and challenging scenario: adapting the
model solely based on online test data to recognize both previously seen
classes and novel, unseen classes at test time. To this end, we develop
Point-Cache, a hierarchical cache model that captures essential clues of online
test samples, particularly focusing on the global structure of point clouds and
their local-part details. Point-Cache, which serves as a rich 3D knowledge
base, is dynamically managed to prioritize the inclusion of high-quality
samples. Designed as a plug-and-play module, our method can be flexibly
integrated into large multimodal 3D models to support open-vocabulary point
cloud recognition. Notably, our solution operates with efficiency comparable to
zero-shot inference, as it is entirely training-free. Point-Cache demonstrates
substantial gains across 8 challenging benchmarks and 4 representative large 3D
models, highlighting its effectiveness. Code is available at
https://github.com/auniquesun/Point-Cache.
</details>

### [VTON 360: High-Fidelity Virtual Try-On from Any Viewing Direction](https://arxiv.org/abs/2503.12165)
*Zijian He,Yuwei Ning,Yipeng Qin,Wangrun Wang,Sibei Yang,Liang Lin,Guanbin Li*
<details>
  <summary>Abstract</summary>
Virtual Try-On (VTON) is a transformative technology in e-commerce and
fashion design, enabling realistic digital visualization of clothing on
individuals. In this work, we propose VTON 360, a novel 3D VTON method that
addresses the open challenge of achieving high-fidelity VTON that supports
any-view rendering. Specifically, we leverage the equivalence between a 3D
model and its rendered multi-view 2D images, and reformulate 3D VTON as an
extension of 2D VTON that ensures 3D consistent results across multiple views.
To achieve this, we extend 2D VTON models to include multi-view garments and
clothing-agnostic human body images as input, and propose several novel
techniques to enhance them, including: i) a pseudo-3D pose representation using
normal maps derived from the SMPL-X 3D human model, ii) a multi-view spatial
attention mechanism that models the correlations between features from
different viewing angles, and iii) a multi-view CLIP embedding that enhances
the garment CLIP features used in 2D VTON with camera information. Extensive
experiments on large-scale real datasets and clothing images from e-commerce
platforms demonstrate the effectiveness of our approach. Project page:
https://scnuhealthy.github.io/VTON360.
</details>

### [Learning Extremely High Density Crowds as Active Matters](https://arxiv.org/abs/2503.12168)
*Feixiang He,Jiangbei Yue,Jialin Zhu,Armin Seyfried,Dan Casas,Julien Pettr√©,He Wang*
<details>
  <summary>Abstract</summary>
Video-based high-density crowd analysis and prediction has been a
long-standing topic in computer vision. It is notoriously difficult due to, but
not limited to, the lack of high-quality data and complex crowd dynamics.
Consequently, it has been relatively under studied. In this paper, we propose a
new approach that aims to learn from in-the-wild videos, often with low quality
where it is difficult to track individuals or count heads. The key novelty is a
new physics prior to model crowd dynamics. We model high-density crowds as
active matter, a continumm with active particles subject to stochastic forces,
named 'crowd material'. Our physics model is combined with neural networks,
resulting in a neural stochastic differential equation system which can mimic
the complex crowd dynamics. Due to the lack of similar research, we adapt a
range of existing methods which are close to ours for comparison. Through
exhaustive evaluation, we show our model outperforms existing methods in
analyzing and forecasting extremely high-density crowds. Furthermore, since our
model is a continuous-time physics model, it can be used for simulation and
analysis, providing strong interpretability. This is categorically different
from most deep learning methods, which are discrete-time models and
black-boxes.
</details>

### [LAPIG: Language Guided Projector Image Generation with Surface Adaptation and Stylization](https://arxiv.org/abs/2503.12173)
*Yuchen Deng,Haibin Ling,Bingyao Huang*
<details>
  <summary>Abstract</summary>
We propose LAPIG, a language guided projector image generation method with
surface adaptation and stylization. LAPIG consists of a projector-camera system
and a target textured projection surface. LAPIG takes the user text prompt as
input and aims to transform the surface style using the projector. LAPIG's key
challenge is that due to the projector's physical brightness limitation and the
surface texture, the viewer's perceived projection may suffer from color
saturation and artifacts in both dark and bright regions, such that even with
the state-of-the-art projector compensation techniques, the viewer may see
clear surface texture-related artifacts. Therefore, how to generate a projector
image that follows the user's instruction while also displaying minimum surface
artifacts is an open problem. To address this issue, we propose projection
surface adaptation (PSA) that can generate compensable surface stylization. We
first train two networks to simulate the projector compensation and
project-and-capture processes, this allows us to find a satisfactory projector
image without real project-and-capture and utilize gradient descent for fast
convergence. Then, we design content and saturation losses to guide the
projector image generation, such that the generated image shows no clearly
perceivable artifacts when projected. Finally, the generated image is projected
for visually pleasing surface style morphing effects. The source code and video
are available on the project page: https://Yu-chen-Deng.github.io/LAPIG/.
</details>

### [Breaking the Box: Enhancing Remote Sensing Image Segmentation with Freehand Sketches](https://arxiv.org/abs/2503.12191)
*Ying Zang,Yuncan Gao,Jiangi Zhang,Yuangi Hu,Runlong Cao,Lanyun Zhu,Qi Zhu,Deyi Ji,Renjun Xu,Tianrun Chen*
<details>
  <summary>Abstract</summary>
This work advances zero-shot interactive segmentation for remote sensing
imagery through three key contributions. First, we propose a novel sketch-based
prompting method, enabling users to intuitively outline objects, surpassing
traditional point or box prompts. Second, we introduce LTL-Sensing, the first
dataset pairing human sketches with remote sensing imagery, setting a benchmark
for future research. Third, we present LTL-Net, a model featuring a multi-input
prompting transport module tailored for freehand sketches. Extensive
experiments show our approach significantly improves segmentation accuracy and
robustness over state-of-the-art methods like SAM, fostering more intuitive
human-AI collaboration in remote sensing analysis and enhancing its
applications.
</details>

### [S2IL: Structurally Stable Incremental Learning](https://arxiv.org/abs/2503.12193)
*S Balasubramanian,Yedu Krishna P,Talasu Sai Sriram,M Sai Subramaniam,Manepalli Pranav Phanindra Sai,Darshan Gera*
<details>
  <summary>Abstract</summary>
Feature Distillation (FD) strategies are proven to be effective in mitigating
Catastrophic Forgetting (CF) seen in Class Incremental Learning (CIL). However,
current FD approaches enforce strict alignment of feature magnitudes and
directions across incremental steps, limiting the model's ability to adapt to
new knowledge. In this paper we propose Structurally Stable Incremental
Learning(S22IL), a FD method for CIL that mitigates CF by focusing on
preserving the overall spatial patterns of features which promote flexible
(plasticity) yet stable representations that preserve old knowledge
(stability). We also demonstrate that our proposed method S2IL achieves strong
incremental accuracy and outperforms other FD methods on SOTA benchmark
datasets CIFAR-100, ImageNet-100 and ImageNet-1K. Notably, S2IL outperforms
other methods by a significant margin in scenarios that have a large number of
incremental tasks.
</details>

### [TLAC: Two-stage LMM Augmented CLIP for Zero-Shot Classification](https://arxiv.org/abs/2503.12206)
*Ans Munir,Faisal Z. Qureshi,Muhammad Haris Khan,Mohsen Ali*
<details>
  <summary>Abstract</summary>
Contrastive Language-Image Pretraining (CLIP) has shown impressive zero-shot
performance on image classification. However, state-of-the-art methods often
rely on fine-tuning techniques like prompt learning and adapter-based tuning to
optimize CLIP's performance. The necessity for fine-tuning significantly limits
CLIP's adaptability to novel datasets and domains. This requirement mandates
substantial time and computational resources for each new dataset. To overcome
this limitation, we introduce simple yet effective training-free approaches,
Single-stage LMM Augmented CLIP (SLAC) and Two-stage LMM Augmented CLIP (TLAC),
that leverages powerful Large Multimodal Models (LMMs), such as Gemini, for
image classification. The proposed methods leverages the capabilities of
pre-trained LMMs, allowing for seamless adaptation to diverse datasets and
domains without the need for additional training. Our approaches involve
prompting the LMM to identify objects within an image. Subsequently, the CLIP
text encoder determines the image class by identifying the dataset class with
the highest semantic similarity to the LLM predicted object. We evaluated our
models on 11 base-to-novel datasets and they achieved superior accuracy on 9 of
these, including benchmarks like ImageNet, SUN397 and Caltech101, while
maintaining a strictly training-free paradigm. Our overall accuracy of 83.44%
surpasses the previous state-of-the-art few-shot methods by a margin of 6.75%.
Our method achieved 83.6% average accuracy across 13 datasets, a 9.7%
improvement over the previous 73.9% state-of-the-art for training-free
approaches. Our method improves domain generalization, with a 3.6% gain on
ImageNetV2, 16.96% on ImageNet-S, and 12.59% on ImageNet-R, over prior few-shot
methods.
</details>

### [STAY Diffusion: Styled Layout Diffusion Model for Diverse Layout-to-Image Generation](https://arxiv.org/abs/2503.12213)
*Ruyu Wang,Xuefeng Hou,Sabrina Schmedding,Marco F. Huber*
<details>
  <summary>Abstract</summary>
In layout-to-image (L2I) synthesis, controlled complex scenes are generated
from coarse information like bounding boxes. Such a task is exciting to many
downstream applications because the input layouts offer strong guidance to the
generation process while remaining easily reconfigurable by humans. In this
paper, we proposed STyled LAYout Diffusion (STAY Diffusion), a diffusion-based
model that produces photo-realistic images and provides fine-grained control of
stylized objects in scenes. Our approach learns a global condition for each
layout, and a self-supervised semantic map for weight modulation using a novel
Edge-Aware Normalization (EA Norm). A new Styled-Mask Attention (SM Attention)
is also introduced to cross-condition the global condition and image feature
for capturing the objects' relationships. These measures provide consistent
guidance through the model, enabling more accurate and controllable image
generation. Extensive benchmarking demonstrates that our STAY Diffusion
presents high-quality images while surpassing previous state-of-the-art methods
in generation diversity, accuracy, and controllability.
</details>

### [Gun Detection Using Combined Human Pose and Weapon Appearance](https://arxiv.org/abs/2503.12215)
*Amulya Reddy Maligireddy,Manohar Reddy Uppula,Nidhi Rastogi,Yaswanth Reddy Parla*
<details>
  <summary>Abstract</summary>
The increasing frequency of firearm-related incidents has necessitated
advancements in security and surveillance systems, particularly in firearm
detection within public spaces. Traditional gun detection methods rely on
manual inspections and continuous human monitoring of CCTV footage, which are
labor-intensive and prone to high false positive and negative rates. To address
these limitations, we propose a novel approach that integrates human pose
estimation with weapon appearance recognition using deep learning techniques.
Unlike prior studies that focus on either body pose estimation or firearm
detection in isolation, our method jointly analyzes posture and weapon presence
to enhance detection accuracy in real-world, dynamic environments. To train our
model, we curated a diverse dataset comprising images from open-source
repositories such as IMFDB and Monash Guns, supplemented with AI-generated and
manually collected images from web sources. This dataset ensures robust
generalization and realistic performance evaluation under various surveillance
conditions. Our research aims to improve the precision and reliability of
firearm detection systems, contributing to enhanced public safety and threat
mitigation in high-risk areas.
</details>

### [Adaptive Label Correction for Robust Medical Image Segmentation with Noisy Labels](https://arxiv.org/abs/2503.12218)
*Chengxuan Qian,Kai Han,Siqi Ma,Chongwen Lyu,Zhenlong Yuan,Jun Chen,Zhe Liu*
<details>
  <summary>Abstract</summary>
Deep learning has shown remarkable success in medical image analysis, but its
reliance on large volumes of high-quality labeled data limits its
applicability. While noisy labeled data are easier to obtain, directly
incorporating them into training can degrade model performance. To address this
challenge, we propose a Mean Teacher-based Adaptive Label Correction (ALC)
self-ensemble framework for robust medical image segmentation with noisy
labels. The framework leverages the Mean Teacher architecture to ensure
consistent learning under noise perturbations. It includes an adaptive label
refinement mechanism that dynamically captures and weights differences across
multiple disturbance versions to enhance the quality of noisy labels.
Additionally, a sample-level uncertainty-based label selection algorithm is
introduced to prioritize high-confidence samples for network updates,
mitigating the impact of noisy annotations. Consistency learning is integrated
to align the predictions of the student and teacher networks, further enhancing
model robustness. Extensive experiments on two public datasets demonstrate the
effectiveness of the proposed framework, showing significant improvements in
segmentation performance. By fully exploiting the strengths of the Mean Teacher
structure, the ALC framework effectively processes noisy labels, adapts to
challenging scenarios, and achieves competitive results compared to
state-of-the-art methods.
</details>

### [LIAM: Multimodal Transformer for Language Instructions, Images, Actions and Semantic Maps](https://arxiv.org/abs/2503.12230)
*Yihao Wang,Raphael Memmesheimer,Sven Behnke*
<details>
  <summary>Abstract</summary>
The availability of large language models and open-vocabulary object
perception methods enables more flexibility for domestic service robots. The
large variability of domestic tasks can be addressed without implementing each
task individually by providing the robot with a task description along with
appropriate environment information. In this work, we propose LIAM - an
end-to-end model that predicts action transcripts based on language, image,
action, and map inputs. Language and image inputs are encoded with a CLIP
backbone, for which we designed two pre-training tasks to fine-tune its weights
and pre-align the latent spaces. We evaluate our method on the ALFRED dataset,
a simulator-generated benchmark for domestic tasks. Our results demonstrate the
importance of pre-aligning embedding spaces from different modalities and the
efficacy of incorporating semantic maps.
</details>

### [From Laboratory to Real World: A New Benchmark Towards Privacy-Preserved Visible-Infrared Person Re-Identification](https://arxiv.org/abs/2503.12232)
*Yan Jiang,Hao Yu,Xu Cheng,Haoyu Chen,Zhaodong Sun,Guoying Zhao*
<details>
  <summary>Abstract</summary>
Aiming to match pedestrian images captured under varying lighting conditions,
visible-infrared person re-identification (VI-ReID) has drawn intensive
research attention and achieved promising results. However, in real-world
surveillance contexts, data is distributed across multiple devices/entities,
raising privacy and ownership concerns that make existing centralized training
impractical for VI-ReID. To tackle these challenges, we propose L2RW, a
benchmark that brings VI-ReID closer to real-world applications. The rationale
of L2RW is that integrating decentralized training into VI-ReID can address
privacy concerns in scenarios with limited data-sharing regulation.
Specifically, we design protocols and corresponding algorithms for different
privacy sensitivity levels. In our new benchmark, we ensure the model training
is done in the conditions that: 1) data from each camera remains completely
isolated, or 2) different data entities (e.g., data controllers of a certain
region) can selectively share the data. In this way, we simulate scenarios with
strict privacy constraints which is closer to real-world conditions. Intensive
experiments with various server-side federated algorithms are conducted,
showing the feasibility of decentralized VI-ReID training. Notably, when
evaluated in unseen domains (i.e., new data entities), our L2RW, trained with
isolated data (privacy-preserved), achieves performance comparable to SOTAs
trained with shared data (privacy-unrestricted). We hope this work offers a
novel research entry for deploying VI-ReID that fits real-world scenarios and
can benefit the community.
</details>

### [RePerformer: Immersive Human-centric Volumetric Videos from Playback to Photoreal Reperformance](https://arxiv.org/abs/2503.12242)
*Yuheng Jiang,Zhehao Shen,Chengcheng Guo,Yu Hong,Zhuo Su,Yingliang Zhang,Marc Habermann,Lan Xu*
<details>
  <summary>Abstract</summary>
Human-centric volumetric videos offer immersive free-viewpoint experiences,
yet existing methods focus either on replaying general dynamic scenes or
animating human avatars, limiting their ability to re-perform general dynamic
scenes. In this paper, we present RePerformer, a novel Gaussian-based
representation that unifies playback and re-performance for high-fidelity
human-centric volumetric videos. Specifically, we hierarchically disentangle
the dynamic scenes into motion Gaussians and appearance Gaussians which are
associated in the canonical space. We further employ a Morton-based
parameterization to efficiently encode the appearance Gaussians into 2D
position and attribute maps. For enhanced generalization, we adopt 2D CNNs to
map position maps to attribute maps, which can be assembled into appearance
Gaussians for high-fidelity rendering of the dynamic scenes. For
re-performance, we develop a semantic-aware alignment module and apply
deformation transfer on motion Gaussians, enabling photo-real rendering under
novel motions. Extensive experiments validate the robustness and effectiveness
of RePerformer, setting a new benchmark for playback-then-reperformance
paradigm in human-centric volumetric videos.
</details>

### [Minuscule Cell Detection in AS-OCT Images with Progressive Field-of-View Focusing](https://arxiv.org/abs/2503.12249)
*Boyu Chen,Ameenat L. Solebo,Daqian Shi,Jinge Wu,Paul Taylor*
<details>
  <summary>Abstract</summary>
Anterior Segment Optical Coherence Tomography (AS-OCT) is an emerging imaging
technique with great potential for diagnosing anterior uveitis, a
vision-threatening ocular inflammatory condition. A hallmark of this condition
is the presence of inflammatory cells in the eye's anterior chamber, and
detecting these cells using AS-OCT images has attracted research interest.
While recent efforts aim to replace manual cell detection with automated
computer vision approaches, detecting extremely small (minuscule) objects in
high-resolution images, such as AS-OCT, poses substantial challenges: (1) each
cell appears as a minuscule particle, representing less than 0.005\% of the
image, making the detection difficult, and (2) OCT imaging introduces
pixel-level noise that can be mistaken for cells, leading to false positive
detections. To overcome these challenges, we propose a minuscule cell detection
framework through a progressive field-of-view focusing strategy. This strategy
systematically refines the detection scope from the whole image to a target
region where cells are likely to be present, and further to minuscule regions
potentially containing individual cells. Our framework consists of two modules.
First, a Field-of-Focus module uses a vision foundation model to segment the
target region. Subsequently, a Fine-grained Object Detection module introduces
a specialized Minuscule Region Proposal followed by a Spatial Attention Network
to distinguish individual cells from noise within the segmented region.
Experimental results demonstrate that our framework outperforms
state-of-the-art methods for cell detection, providing enhanced efficacy for
clinical applications. Our code is publicly available at:
https://github.com/joeybyc/MCD.
</details>

### [Enhancing Facial Expression Recognition through Dual-Direction Attention Mixed Feature Networks and CLIP: Application to 8th ABAW Challenge](https://arxiv.org/abs/2503.12260)
*Josep Cabacas-Maso,Elena Ortega-Beltr√°n,Ismael Benito-Altamirano,Carles Ventura*
<details>
  <summary>Abstract</summary>
We present our contribution to the 8th ABAW challenge at CVPR 2025, where we
tackle valence-arousal estimation, emotion recognition, and facial action unit
detection as three independent challenges. Our approach leverages the
well-known Dual-Direction Attention Mixed Feature Network (DDAMFN) for all
three tasks, achieving results that surpass the proposed baselines.
Additionally, we explore the use of CLIP for the emotion recognition challenge
as an additional experiment. We provide insights into the architectural choices
that contribute to the strong performance of our methods.
</details>

### [Handling Weak Complementary Relationships for Audio-Visual Emotion Recognition](https://arxiv.org/abs/2503.12261)
*R. Gnana Praveen,Jahangir Alam*
<details>
  <summary>Abstract</summary>
Multimodal emotion recognition has recently drawn a lot of interest in
affective computing as it has immense potential to outperform isolated unimodal
approaches. Audio and visual modalities are two predominant contact-free
channels in videos, which are often expected to carry a complementary
relationship with each other. However, audio and visual channels may not always
be complementary with each other, resulting in poor audio-visual feature
representations, thereby degrading the performance of the system. In this
paper, we propose a flexible audio-visual fusion model that can adapt to weak
complementary relationships using a gated attention mechanism. Specifically, we
extend the recursive joint cross-attention model by introducing gating
mechanism in every iteration to control the flow of information between the
input features and the attended features depending on the strength of their
complementary relationship. For instance, if the modalities exhibit strong
complementary relationships, the gating mechanism chooses cross-attended
features, otherwise non-attended features. To further improve the performance
of the system, we further introduce stage gating mechanism, which is used to
control the flow of information across the gated outputs of each iteration.
Therefore, the proposed model improves the performance of the system even when
the audio and visual modalities do not have a strong complementary relationship
with each other by adding more flexibility to the recursive joint cross
attention mechanism. The proposed model has been evaluated on the challenging
Affwild2 dataset and significantly outperforms the state-of-the-art fusion
approaches.
</details>

### [An Efficient Deep Learning-Based Approach to Automating Invoice Document Validation](https://arxiv.org/abs/2503.12267)
*Aziz Amari,Mariem Makni,Wissal Fnaich,Akram Lahmar,Fedi Koubaa,Oumayma Charrad,Mohamed Ali Zormati,Rabaa Youssef Douss*
<details>
  <summary>Abstract</summary>
In large organizations, the number of financial transactions can grow
rapidly, driving the need for fast and accurate multi-criteria invoice
validation. Manual processing remains error-prone and time-consuming, while
current automated solutions are limited by their inability to support a variety
of constraints, such as documents that are partially handwritten or
photographed with a mobile phone. In this paper, we propose to automate the
validation of machine written invoices using document layout analysis and
object detection techniques based on recent deep learning (DL) models. We
introduce a novel dataset consisting of manually annotated real-world invoices
and a multi-criteria validation process. We fine-tune and benchmark the most
relevant DL models on our dataset. Experimental results show the effectiveness
of the proposed pipeline and selected DL models in terms of achieving fast and
accurate validation of invoices.
</details>

### [Cracking the PUMA Challenge in 24 Hours with CellViT++ and nnU-Net](https://arxiv.org/abs/2503.12269)
*Negar Shahamiri,Moritz Rempe,Lukas Heine,Jens Kleesiek,Fabian H√∂rst*
<details>
  <summary>Abstract</summary>
Automatic tissue segmentation and nuclei detection is an important task in
pathology, aiding in biomarker extraction and discovery. The panoptic
segmentation of nuclei and tissue in advanced melanoma (PUMA) challenge aims to
improve tissue segmentation and nuclei detection in melanoma histopathology.
Unlike many challenge submissions focusing on extensive model tuning, our
approach emphasizes delivering a deployable solution within a 24-hour
development timeframe, using out-of-the-box frameworks. The pipeline combines
two models, namely CellViT++ for nuclei detection and nnU-Net for tissue
segmentation. Our results demonstrate a significant improvement in tissue
segmentation, achieving a Dice score of 0.750, surpassing the baseline score of
0.629. For nuclei detection, we obtained results comparable to the baseline in
both challenge tracks. The code is publicly available at
https://github.com/TIO-IKIM/PUMA.
</details>

### [Reflect-DiT: Inference-Time Scaling for Text-to-Image Diffusion Transformers via In-Context Reflection](https://arxiv.org/abs/2503.12271)
*Shufan Li,Konstantinos Kallidromitis,Akash Gokul,Arsh Koneru,Yusuke Kato,Kazuki Kozuka,Aditya Grover*
<details>
  <summary>Abstract</summary>
The predominant approach to advancing text-to-image generation has been
training-time scaling, where larger models are trained on more data using
greater computational resources. While effective, this approach is
computationally expensive, leading to growing interest in inference-time
scaling to improve performance. Currently, inference-time scaling for
text-to-image diffusion models is largely limited to best-of-N sampling, where
multiple images are generated per prompt and a selection model chooses the best
output. Inspired by the recent success of reasoning models like DeepSeek-R1 in
the language domain, we introduce an alternative to naive best-of-N sampling by
equipping text-to-image Diffusion Transformers with in-context reflection
capabilities. We propose Reflect-DiT, a method that enables Diffusion
Transformers to refine their generations using in-context examples of
previously generated images alongside textual feedback describing necessary
improvements. Instead of passively relying on random sampling and hoping for a
better result in a future generation, Reflect-DiT explicitly tailors its
generations to address specific aspects requiring enhancement. Experimental
results demonstrate that Reflect-DiT improves performance on the GenEval
benchmark (+0.19) using SANA-1.0-1.6B as a base model. Additionally, it
achieves a new state-of-the-art score of 0.81 on GenEval while generating only
20 samples per prompt, surpassing the previous best score of 0.80, which was
obtained using a significantly larger model (SANA-1.5-4.8B) with 2048 samples
under the best-of-N approach.
</details>

### [Exploration of VLMs for Driver Monitoring Systems Applications](https://arxiv.org/abs/2503.12281)
*Paola Natalia Ca√±as,Marcos Nieto,Oihana Otaegui,Igor Rodr√≠guez*
<details>
  <summary>Abstract</summary>
In recent years, we have witnessed significant progress in emerging deep
learning models, particularly Large Language Models (LLMs) and Vision-Language
Models (VLMs). These models have demonstrated promising results, indicating a
new era of Artificial Intelligence (AI) that surpasses previous methodologies.
Their extensive knowledge and zero-shot capabilities suggest a paradigm shift
in developing deep learning solutions, moving from data capturing and algorithm
training to just writing appropriate prompts. While the application of these
technologies has been explored across various industries, including automotive,
there is a notable gap in the scientific literature regarding their use in
Driver Monitoring Systems (DMS). This paper presents our initial approach to
implementing VLMs in this domain, utilising the Driver Monitoring Dataset to
evaluate their performance and discussing their advantages and challenges when
implemented in real-world scenarios.
</details>

### [REdiSplats: Ray Tracing for Editable Gaussian Splatting](https://arxiv.org/abs/2503.12284)
*Krzysztof Byrski,Grzegorz Wilczy≈Ñski,Weronika Smolak-Dy≈ºewska,Piotr Borycki,Dawid Baran,S≈Çawomir Tadeja,Przemys≈Çaw Spurek*
<details>
  <summary>Abstract</summary>
Gaussian Splatting (GS) has become one of the most important neural rendering
algorithms. GS represents 3D scenes using Gaussian components with trainable
color and opacity. This representation achieves high-quality renderings with
fast inference. Regrettably, it is challenging to integrate such a solution
with varying light conditions, including shadows and light reflections, manual
adjustments, and a physical engine. Recently, a few approaches have appeared
that incorporate ray-tracing or mesh primitives into GS to address some of
these caveats. However, no such solution can simultaneously solve all the
existing limitations of the classical GS. Consequently, we introduce
REdiSplats, which employs ray tracing and a mesh-based representation of flat
3D Gaussians. In practice, we model the scene using flat Gaussian distributions
parameterized by the mesh. We can leverage fast ray tracing and control
Gaussian modification by adjusting the mesh vertices. Moreover, REdiSplats
allows modeling of light conditions, manual adjustments, and physical
simulation. Furthermore, we can render our models using 3D tools such as
Blender or Nvdiffrast, which opens the possibility of integrating them with all
existing 3D graphics techniques dedicated to mesh representations.
</details>

### [Towards Self-Improving Systematic Cognition for Next-Generation Foundation MLLMs](https://arxiv.org/abs/2503.12303)
*Xiaoying Zhang,Da Peng,Yipeng Zhang,Zonghao Guo,Chengyue Wu,Chi Chen,Wei Ke,Helen Meng,Maosong Sun*
<details>
  <summary>Abstract</summary>
Despite their impressive capabilities, Multimodal Large Language Models
(MLLMs) face challenges with fine-grained perception and complex reasoning.
Prevalent pre-training approaches focus on enhancing perception by training on
high-quality image captions due to the extremely high cost of collecting
chain-of-thought (CoT) reasoning data for improving reasoning. While leveraging
advanced MLLMs for caption generation enhances scalability, the outputs often
lack comprehensiveness and accuracy. In this paper, we introduce Self-Improving
Cognition (SIcog), a self-learning framework designed to construct
next-generation foundation MLLMs by enhancing their systematic cognitive
capabilities through multimodal pre-training with self-generated data.
Specifically, we propose chain-of-description, an approach that improves an
MLLM's systematic perception by enabling step-by-step visual understanding,
ensuring greater comprehensiveness and accuracy. Additionally, we adopt a
structured CoT reasoning technique to enable MLLMs to integrate in-depth
multimodal reasoning. To construct a next-generation foundation MLLM with
self-improved cognition, SIcog first equips an MLLM with systematic perception
and reasoning abilities using minimal external annotations. The enhanced models
then generate detailed captions and CoT reasoning data, which are further
curated through self-consistency. This curated data is ultimately used to
refine the MLLM during multimodal pre-training, facilitating next-generation
foundation MLLM construction. Extensive experiments on both low- and
high-resolution MLLMs across diverse benchmarks demonstrate that, with merely
213K self-generated pre-training samples, SIcog produces next-generation
foundation MLLMs with significantly improved cognition, achieving
benchmark-leading performance compared to prevalent pre-training approaches.
</details>

### [Swift4D:Adaptive divide-and-conquer Gaussian Splatting for compact and efficient reconstruction of dynamic scene](https://arxiv.org/abs/2503.12307)
*Jiahao Wu,Rui Peng,Zhiyan Wang,Lu Xiao,Luyang Tang,Jinbo Yan,Kaiqiang Xiong,Ronggang Wang*
<details>
  <summary>Abstract</summary>
Novel view synthesis has long been a practical but challenging task, although
the introduction of numerous methods to solve this problem, even combining
advanced representations like 3D Gaussian Splatting, they still struggle to
recover high-quality results and often consume too much storage memory and
training time. In this paper we propose Swift4D, a divide-and-conquer 3D
Gaussian Splatting method that can handle static and dynamic primitives
separately, achieving a good trade-off between rendering quality and
efficiency, motivated by the fact that most of the scene is the static
primitive and does not require additional dynamic properties. Concretely, we
focus on modeling dynamic transformations only for the dynamic primitives which
benefits both efficiency and quality. We first employ a learnable decomposition
strategy to separate the primitives, which relies on an additional parameter to
classify primitives as static or dynamic. For the dynamic primitives, we employ
a compact multi-resolution 4D Hash mapper to transform these primitives from
canonical space into deformation space at each timestamp, and then mix the
static and dynamic primitives to produce the final output. This
divide-and-conquer method facilitates efficient training and reduces storage
redundancy. Our method not only achieves state-of-the-art rendering quality
while being 20X faster in training than previous SOTA methods with a minimum
storage requirement of only 30MB on real-world datasets. Code is available at
https://github.com/WuJH2001/swift4d.
</details>

### [Leveraging Vision Capabilities of Multimodal LLMs for Automated Data Extraction from Plots](https://arxiv.org/abs/2503.12326)
*Maciej P. Polak,Dane Morgan*
<details>
  <summary>Abstract</summary>
Automated data extraction from research texts has been steadily improving,
with the emergence of large language models (LLMs) accelerating progress even
further. Extracting data from plots in research papers, however, has been such
a complex task that it has predominantly been confined to manual data
extraction. We show that current multimodal large language models, with proper
instructions and engineered workflows, are capable of accurately extracting
data from plots. This capability is inherent to the pretrained models and can
be achieved with a chain-of-thought sequence of zero-shot engineered prompts we
call PlotExtract, without the need to fine-tune. We demonstrate PlotExtract
here and assess its performance on synthetic and published plots. We consider
only plots with two axes in this analysis. For plots identified as extractable,
PlotExtract finds points with over 90% precision (and around 90% recall) and
errors in x and y position of around 5% or lower. These results prove that
multimodal LLMs are a viable path for high-throughput data extraction for plots
and in many circumstances can replace the current manual methods of data
extraction.
</details>

### [CapArena: Benchmarking and Analyzing Detailed Image Captioning in the LLM Era](https://arxiv.org/abs/2503.12329)
*Kanzhi Cheng,Wenpo Song,Jiaxin Fan,Zheng Ma,Qiushi Sun,Fangzhi Xu,Chenyang Yan,Nuo Chen,Jianbing Zhang,Jiajun Chen*
<details>
  <summary>Abstract</summary>
Image captioning has been a longstanding challenge in vision-language
research. With the rise of LLMs, modern Vision-Language Models (VLMs) generate
detailed and comprehensive image descriptions. However, benchmarking the
quality of such captions remains unresolved. This paper addresses two key
questions: (1) How well do current VLMs actually perform on image captioning,
particularly compared to humans? We built CapArena, a platform with over 6000
pairwise caption battles and high-quality human preference votes. Our
arena-style evaluation marks a milestone, showing that leading models like
GPT-4o achieve or even surpass human performance, while most open-source models
lag behind. (2) Can automated metrics reliably assess detailed caption quality?
Using human annotations from CapArena, we evaluate traditional and recent
captioning metrics, as well as VLM-as-a-Judge. Our analysis reveals that while
some metrics (e.g., METEOR) show decent caption-level agreement with humans,
their systematic biases lead to inconsistencies in model ranking. In contrast,
VLM-as-a-Judge demonstrates robust discernment at both the caption and model
levels. Building on these insights, we release CapArena-Auto, an accurate and
efficient automated benchmark for detailed captioning, achieving 94.3%
correlation with human rankings at just $4 per test. Data and resources will be
open-sourced at https://caparena.github.io.
</details>

### [VideoMAP: Toward Scalable Mamba-based Video Autoregressive Pretraining](https://arxiv.org/abs/2503.12332)
*Yunze Liu,Peiran Wu,Cheng Liang,Junxiao Shen,Limin Wang,Li Yi*
<details>
  <summary>Abstract</summary>
Recent Mamba-based architectures for video understanding demonstrate
promising computational efficiency and competitive performance, yet struggle
with overfitting issues that hinder their scalability. To overcome this
challenge, we introduce VideoMAP, a Hybrid Mamba-Transformer framework
featuring a novel pre-training approach. VideoMAP uses a 4:1
Mamba-to-Transformer ratio, effectively balancing computational cost and model
capacity. This architecture, combined with our proposed frame-wise masked
autoregressive pre-training strategy, delivers significant performance gains
when scaling to larger models. Additionally, VideoMAP exhibits impressive
sample efficiency, significantly outperforming existing methods with less
training data. Experiments show that VideoMAP outperforms existing models
across various datasets, including Kinetics-400, Something-Something V2,
Breakfast, and COIN. Furthermore, we demonstrate the potential of VideoMAP as a
visual encoder for multimodal large language models, highlighting its ability
to reduce memory usage and enable the processing of longer video sequences. The
code is open-source at https://github.com/yunzeliu/MAP
</details>

### [GS-3I: Gaussian Splatting for Surface Reconstruction from Illumination-Inconsistent Images](https://arxiv.org/abs/2503.12335)
*Tengfei Wang,Yongmao Hou,Zhaoning Zhang,Yiwei Xu,Zongqian Zhan,Xin Wang*
<details>
  <summary>Abstract</summary>
Accurate geometric surface reconstruction, providing essential environmental
information for navigation and manipulation tasks, is critical for enabling
robotic self-exploration and interaction. Recently, 3D Gaussian Splatting
(3DGS) has gained significant attention in the field of surface reconstruction
due to its impressive geometric quality and computational efficiency. While
recent relevant advancements in novel view synthesis under inconsistent
illumination using 3DGS have shown promise, the challenge of robust surface
reconstruction under such conditions is still being explored. To address this
challenge, we propose a method called GS-3I. Specifically, to mitigate 3D
Gaussian optimization bias caused by underexposed regions in single-view
images, based on Convolutional Neural Network (CNN), a tone mapping correction
framework is introduced. Furthermore, inconsistent lighting across multi-view
images, resulting from variations in camera settings and complex scene
illumination, often leads to geometric constraint mismatches and deviations in
the reconstructed surface. To overcome this, we propose a normal compensation
mechanism that integrates reference normals extracted from single-view image
with normals computed from multi-view observations to effectively constrain
geometric inconsistencies. Extensive experimental evaluations demonstrate that
GS-3I can achieve robust and accurate surface reconstruction across complex
illumination scenarios, highlighting its effectiveness and versatility in this
critical challenge. https://github.com/TFwang-9527/GS-3I
</details>

### [TopoGaussian: Inferring Internal Topology Structures from Visual Clues](https://arxiv.org/abs/2503.12343)
*Xiaoyu Xiong,Changyu Hu,Chunru Lin,Pingchuan Ma,Chuang Gan,Tao Du*
<details>
  <summary>Abstract</summary>
We present TopoGaussian, a holistic, particle-based pipeline for inferring
the interior structure of an opaque object from easily accessible photos and
videos as input. Traditional mesh-based approaches require tedious and
error-prone mesh filling and fixing process, while typically output rough
boundary surface. Our pipeline combines Gaussian Splatting with a novel,
versatile particle-based differentiable simulator that simultaneously
accommodates constitutive model, actuator, and collision, without interference
with mesh. Based on the gradients from this simulator, we provide flexible
choice of topology representation for optimization, including particle, neural
implicit surface, and quadratic surface. The resultant pipeline takes easily
accessible photos and videos as input and outputs the topology that matches the
physical characteristics of the input. We demonstrate the efficacy of our
pipeline on a synthetic dataset and four real-world tasks with 3D-printed
prototypes. Compared with existing mesh-based method, our pipeline is 5.26x
faster on average with improved shape quality. These results highlight the
potential of our pipeline in 3D vision, soft robotics, and manufacturing
applications.
</details>

### [ProbDiffFlow: An Efficient Learning-Free Framework for Probabilistic Single-Image Optical Flow Estimation](https://arxiv.org/abs/2503.12348)
*Mo Zhou,Jianwei Wang,Xuanmeng Zhang,Dylan Campbell,Kai Wang,Long Yuan,Wenjie Zhang,Xuemin Lin*
<details>
  <summary>Abstract</summary>
This paper studies optical flow estimation, a critical task in motion
analysis with applications in autonomous navigation, action recognition, and
film production. Traditional optical flow methods require consecutive frames,
which are often unavailable due to limitations in data acquisition or
real-world scene disruptions. Thus, single-frame optical flow estimation is
emerging in the literature. However, existing single-frame approaches suffer
from two major limitations: (1) they rely on labeled training data, making them
task-specific, and (2) they produce deterministic predictions, failing to
capture motion uncertainty. To overcome these challenges, we propose
ProbDiffFlow, a training-free framework that estimates optical flow
distributions from a single image. Instead of directly predicting motion,
ProbDiffFlow follows an estimation-by-synthesis paradigm: it first generates
diverse plausible future frames using a diffusion-based model, then estimates
motion from these synthesized samples using a pre-trained optical flow model,
and finally aggregates the results into a probabilistic flow distribution. This
design eliminates the need for task-specific training while capturing multiple
plausible motions. Experiments on both synthetic and real-world datasets
demonstrate that ProbDiffFlow achieves superior accuracy, diversity, and
efficiency, outperforming existing single-image and two-frame baselines.
</details>

### [ResLPR: A LiDAR Data Restoration Network and Benchmark for Robust Place Recognition Against Weather Corruptions](https://arxiv.org/abs/2503.12350)
*Wenqing Kuang,Xiongwei Zhao,Yehui Shen,Congcong Wen,Huimin Lu,Zongtan Zhou,Xieyuanli Chen*
<details>
  <summary>Abstract</summary>
LiDAR-based place recognition (LPR) is a key component for autonomous
driving, and its resilience to environmental corruption is critical for safety
in high-stakes applications. While state-of-the-art (SOTA) LPR methods perform
well in clean weather, they still struggle with weather-induced corruption
commonly encountered in driving scenarios. To tackle this, we propose
ResLPRNet, a novel LiDAR data restoration network that largely enhances LPR
performance under adverse weather by restoring corrupted LiDAR scans using a
wavelet transform-based network. ResLPRNet is efficient, lightweight and can be
integrated plug-and-play with pretrained LPR models without substantial
additional computational cost. Given the lack of LPR datasets under adverse
weather, we introduce ResLPR, a novel benchmark that examines SOTA LPR methods
under a wide range of LiDAR distortions induced by severe snow, fog, and rain
conditions. Experiments on our proposed WeatherKITTI and WeatherNCLT datasets
demonstrate the resilience and notable gains achieved by using our restoration
method with multiple LPR approaches in challenging weather scenarios. Our code
and benchmark are publicly available here:
https://github.com/nubot-nudt/ResLPR.
</details>

### [Atlas: Multi-Scale Attention Improves Long Context Image Modeling](https://arxiv.org/abs/2503.12355)
*Kumar Krishna Agrawal,Long Lian,Longchao Liu,Natalia Harguindeguy,Boyi Li,Alexander Bick,Maggie Chung,Trevor Darrell,Adam Yala*
<details>
  <summary>Abstract</summary>
Efficiently modeling massive images is a long-standing challenge in machine
learning. To this end, we introduce Multi-Scale Attention (MSA). MSA relies on
two key ideas, (i) multi-scale representations (ii) bi-directional cross-scale
communication. MSA creates O(log N) scales to represent the image across
progressively coarser features and leverages cross-attention to propagate
information across scales. We then introduce Atlas, a novel neural network
architecture based on MSA. We demonstrate that Atlas significantly improves the
compute-performance tradeoff of long-context image modeling in a
high-resolution variant of ImageNet 100. At 1024px resolution, Atlas-B achieves
91.04% accuracy, comparable to ConvNext-B (91.92%) while being 4.3x faster.
Atlas is 2.95x faster and 7.38% better than FasterViT, 2.25x faster and 4.96%
better than LongViT. In comparisons against MambaVision-S, we find Atlas-S
achieves 5%, 16% and 32% higher accuracy at 1024px, 2048px and 4096px
respectively, while obtaining similar runtimes. Code for reproducing our
experiments and pretrained models is available at
https://github.com/yalalab/atlas.
</details>

### [Localized Concept Erasure for Text-to-Image Diffusion Models Using Training-Free Gated Low-Rank Adaptation](https://arxiv.org/abs/2503.12356)
*Byung Hyun Lee,Sungjin Lim,Se Young Chun*
<details>
  <summary>Abstract</summary>
Fine-tuning based concept erasing has demonstrated promising results in
preventing generation of harmful contents from text-to-image diffusion models
by removing target concepts while preserving remaining concepts. To maintain
the generation capability of diffusion models after concept erasure, it is
necessary to remove only the image region containing the target concept when it
locally appears in an image, leaving other regions intact. However, prior arts
often compromise fidelity of the other image regions in order to erase the
localized target concept appearing in a specific area, thereby reducing the
overall performance of image generation. To address these limitations, we first
introduce a framework called localized concept erasure, which allows for the
deletion of only the specific area containing the target concept in the image
while preserving the other regions. As a solution for the localized concept
erasure, we propose a training-free approach, dubbed Gated Low-rank adaptation
for Concept Erasure (GLoCE), that injects a lightweight module into the
diffusion model. GLoCE consists of low-rank matrices and a simple gate,
determined only by several generation steps for concepts without training. By
directly applying GLoCE to image embeddings and designing the gate to activate
only for target concepts, GLoCE can selectively remove only the region of the
target concepts, even when target and remaining concepts coexist within an
image. Extensive experiments demonstrated GLoCE not only improves the image
fidelity to text prompts after erasing the localized target concepts, but also
outperforms prior arts in efficacy, specificity, and robustness by large margin
and can be extended to mass concept erasure.
</details>

### [L2COcc: Lightweight Camera-Centric Semantic Scene Completion via Distillation of LiDAR Model](https://arxiv.org/abs/2503.12369)
*Ruoyu Wang,Yukai Ma,Yi Yao,Sheng Tao,Haoang Li,Zongzhi Zhu,Yong Liu,Xingxing Zuo*
<details>
  <summary>Abstract</summary>
Semantic Scene Completion (SSC) constitutes a pivotal element in autonomous
driving perception systems, tasked with inferring the 3D semantic occupancy of
a scene from sensory data. To improve accuracy, prior research has implemented
various computationally demanding and memory-intensive 3D operations, imposing
significant computational requirements on the platform during training and
testing. This paper proposes L2COcc, a lightweight camera-centric SSC framework
that also accommodates LiDAR inputs. With our proposed efficient voxel
transformer (EVT) and cross-modal knowledge modules, including feature
similarity distillation (FSD), TPV distillation (TPVD) and prediction alignment
distillation (PAD), our method substantially reduce computational burden while
maintaining high accuracy. The experimental evaluations demonstrate that our
proposed method surpasses the current state-of-the-art vision-based SSC methods
regarding accuracy on both the SemanticKITTI and SSCBench-KITTI-360 benchmarks,
respectively. Additionally, our method is more lightweight, exhibiting a
reduction in both memory consumption and inference time by over 23% compared to
the current state-of-the-arts method. Code is available at our project
page:https://studyingfufu.github.io/L2COcc/.
</details>

### [Deepfake Detection with Optimized Hybrid Model: EAR Biometric Descriptor via Improved RCNN](https://arxiv.org/abs/2503.12381)
*Ruchika Sharma,Rudresh Dwivedi*
<details>
  <summary>Abstract</summary>
Deepfake is a widely used technology employed in recent years to create
pernicious content such as fake news, movies, and rumors by altering and
substituting facial information from various sources. Given the ongoing
evolution of deepfakes investigation of continuous identification and
prevention is crucial. Due to recent technological advancements in AI
(Artificial Intelligence) distinguishing deepfakes and artificially altered
images has become challenging. This approach introduces the robust detection of
subtle ear movements and shape changes to generate ear descriptors. Further, we
also propose a novel optimized hybrid deepfake detection model that considers
the ear biometric descriptors via enhanced RCNN (Region-Based Convolutional
Neural Network). Initially, the input video is converted into frames and
preprocessed through resizing, normalization, grayscale conversion, and
filtering processes followed by face detection using the Viola-Jones technique.
Next, a hybrid model comprising DBN (Deep Belief Network) and Bi-GRU
(Bidirectional Gated Recurrent Unit) is utilized for deepfake detection based
on ear descriptors. The output from the detection phase is determined through
improved score-level fusion. To enhance the performance, the weights of both
detection models are optimally tuned using the SU-JFO (Self-Upgraded Jellyfish
Optimization method). Experimentation is conducted based on four scenarios:
compression, noise, rotation, pose, and illumination on three different
datasets. The performance results affirm that our proposed method outperforms
traditional models such as CNN (Convolution Neural Network), SqueezeNet, LeNet,
LinkNet, LSTM (Long Short-Term Memory), DFP (Deepfake Predictor) [1], and
ResNext+CNN+LSTM [2] in terms of various performance metrics viz. accuracy,
specificity, and precision.
</details>

### [RENO: Real-Time Neural Compression for 3D LiDAR Point Clouds](https://arxiv.org/abs/2503.12382)
*Kang You,Tong Chen,Dandan Ding,M. Salman Asif,Zhan Ma*
<details>
  <summary>Abstract</summary>
Despite the substantial advancements demonstrated by learning-based neural
models in the LiDAR Point Cloud Compression (LPCC) task, realizing real-time
compression - an indispensable criterion for numerous industrial applications -
remains a formidable challenge. This paper proposes RENO, the first real-time
neural codec for 3D LiDAR point clouds, achieving superior performance with a
lightweight model. RENO skips the octree construction and directly builds upon
the multiscale sparse tensor representation. Instead of the multi-stage
inferring, RENO devises sparse occupancy codes, which exploit cross-scale
correlation and derive voxels' occupancy in a one-shot manner, greatly saving
processing time. Experimental results demonstrate that the proposed RENO
achieves real-time coding speed, 10 fps at 14-bit depth on a desktop platform
(e.g., one RTX 3090 GPU) for both encoding and decoding processes, while
providing 12.25% and 48.34% bit-rate savings compared to G-PCCv23 and Draco,
respectively, at a similar quality. RENO model size is merely 1MB, making it
attractive for practical applications. The source code is available at
https://github.com/NJUVISION/RENO.
</details>

### [VRsketch2Gaussian: 3D VR Sketch Guided 3D Object Generation with Gaussian Splatting](https://arxiv.org/abs/2503.12383)
*Songen Gu,Haoxuan Song,Binjie Liu,Qian Yu,Sanyi Zhang,Haiyong Jiang,Jin Huang,Feng Tian*
<details>
  <summary>Abstract</summary>
We propose VRSketch2Gaussian, a first VR sketch-guided, multi-modal, native
3D object generation framework that incorporates a 3D Gaussian Splatting
representation. As part of our work, we introduce VRSS, the first large-scale
paired dataset containing VR sketches, text, images, and 3DGS, bridging the gap
in multi-modal VR sketch-based generation. Our approach features the following
key innovations: 1) Sketch-CLIP feature alignment. We propose a two-stage
alignment strategy that bridges the domain gap between sparse VR sketch
embeddings and rich CLIP embeddings, facilitating both VR sketch-based
retrieval and generation tasks. 2) Fine-Grained multi-modal conditioning. We
disentangle the 3D generation process by using explicit VR sketches for
geometric conditioning and text descriptions for appearance control. To
facilitate this, we propose a generalizable VR sketch encoder that effectively
aligns different modalities. 3) Efficient and high-fidelity 3D native
generation. Our method leverages a 3D-native generation approach that enables
fast and texture-rich 3D object synthesis. Experiments conducted on our VRSS
dataset demonstrate that our method achieves high-quality, multi-modal VR
sketch-based 3D generation. We believe our VRSS dataset and VRsketch2Gaussian
method will be beneficial for the 3D generation community.
</details>

### [Car-1000: A New Large Scale Fine-Grained Visual Categorization Dataset](https://arxiv.org/abs/2503.12385)
*Yutao Hu,Sen Li,Jincheng Yan,Wenqi Shao,Xiaoyan Luo*
<details>
  <summary>Abstract</summary>
Fine-grained visual categorization (FGVC) is a challenging but significant
task in computer vision, which aims to recognize different sub-categories of
birds, cars, airplanes, etc. Among them, recognizing models of different cars
has significant application value in autonomous driving, traffic surveillance
and scene understanding, which has received considerable attention in the past
few years. However, Stanford-Car, the most widely used fine-grained dataset for
car recognition, only has 196 different categories and only includes vehicle
models produced earlier than 2013. Due to the rapid advancements in the
automotive industry during recent years, the appearances of various car models
have become increasingly intricate and sophisticated. Consequently, the
previous Stanford-Car dataset fails to capture this evolving landscape and
cannot satisfy the requirements of automotive industry. To address these
challenges, in our paper, we introduce Car-1000, a large-scale dataset designed
specifically for fine-grained visual categorization of diverse car models.
Car-1000 encompasses vehicles from 165 different automakers, spanning a wide
range of 1000 distinct car models. Additionally, we have reproduced several
state-of-the-art FGVC methods on the Car-1000 dataset, establishing a new
benchmark for research in this field. We hope that our work will offer a fresh
perspective for future FGVC researchers. Our dataset is available at
https://github.com/toggle1995/Car-1000.
</details>

### [Pathology Image Restoration via Mixture of Prompts](https://arxiv.org/abs/2503.12399)
*Jiangdong Cai,Yan Chen,Zhenrong Shen,Haotian Jiang,Honglin Xiong,Kai Xuan,Lichi Zhang,Qian Wang*
<details>
  <summary>Abstract</summary>
In digital pathology, acquiring all-in-focus images is essential to
high-quality imaging and high-efficient clinical workflow. Traditional scanners
achieve this by scanning at multiple focal planes of varying depths and then
merging them, which is relatively slow and often struggles with complex tissue
defocus. Recent prevailing image restoration technique provides a means to
restore high-quality pathology images from scans of single focal planes.
However, existing image restoration methods are inadequate, due to intricate
defocus patterns in pathology images and their domain-specific semantic
complexities. In this work, we devise a two-stage restoration solution
cascading a transformer and a diffusion model, to benefit from their powers in
preserving image fidelity and perceptual quality, respectively. We particularly
propose a novel mixture of prompts for the two-stage solution. Given initial
prompt that models defocus in microscopic imaging, we design two prompts that
describe the high-level image semantics from pathology foundation model and the
fine-grained tissue structures via edge extraction. We demonstrate that, by
feeding the prompt mixture to our method, we can restore high-quality pathology
images from single-focal-plane scans, implying high potentials of the mixture
of prompts to clinical usage. Code will be publicly available at
https://github.com/caijd2000/MoP.
</details>

### [MExD: An Expert-Infused Diffusion Model for Whole-Slide Image Classification](https://arxiv.org/abs/2503.12401)
*Jianwei Zhao,Xin Li,Fan Yang,Qiang Zhai,Ao Luo,Yang Zhao,Hong Cheng,Huazhu Fu*
<details>
  <summary>Abstract</summary>
Whole Slide Image (WSI) classification poses unique challenges due to the
vast image size and numerous non-informative regions, which introduce noise and
cause data imbalance during feature aggregation. To address these issues, we
propose MExD, an Expert-Infused Diffusion Model that combines the strengths of
a Mixture-of-Experts (MoE) mechanism with a diffusion model for enhanced
classification. MExD balances patch feature distribution through a novel
MoE-based aggregator that selectively emphasizes relevant information,
effectively filtering noise, addressing data imbalance, and extracting
essential features. These features are then integrated via a diffusion-based
generative process to directly yield the class distribution for the WSI. Moving
beyond conventional discriminative approaches, MExD represents the first
generative strategy in WSI classification, capturing fine-grained details for
robust and precise results. Our MExD is validated on three widely-used
benchmarks-Camelyon16, TCGA-NSCLC, and BRACS consistently achieving
state-of-the-art performance in both binary and multi-class tasks.
</details>

### [SAM2-ELNet: Label Enhancement and Automatic Annotation for Remote Sensing Segmentation](https://arxiv.org/abs/2503.12404)
*Jianhao Yang,Wenshuo Yu,Yuanchao Lv,Jiance Sun,Bokang Sun,Mingyang Liu*
<details>
  <summary>Abstract</summary>
Remote sensing image segmentation is crucial for environmental monitoring,
disaster assessment, and resource management, directly affecting the accuracy
and efficiency of surface information extraction. The performance of existing
supervised models in remote sensing image segmentation tasks highly depends on
the quality of label data. However, current label data mainly relies on manual
annotation, which comes with high time costs and is subject to subjective
interference, resulting in distortion of label boundaries and often a loss of
detail. To solve the above problems, our work proposes an Edge-enhanced
Labeling Network, called SAM2-ELNet, which incorporates a labeling module and
an edge attention mechanism. This model effectively addresses issues such as
label detail loss, fragmentation, and inaccurate boundaries. Due to the
scarcity of manually annotated remote sensing data, the feature extraction
capabilities of traditional neural networks are limited. Our method uses the
Hiera backbone of the pre-trained self-supervised large model segment anything
model 2 (SAM2) as the encoder, achieves high-quality and efficient feature
extraction even with small samples by fine-tuning on downstream tasks. This
study compared the training effects of original and enhanced labels on the
manually annotated Deep-SAR Oil Spill (SOS) dataset. Results showed that the
model trained with enhanced labels performed better and had a lower final loss,
indicating closer alignment with the real data distribution. Our work also
explores the potential of extending the model into an efficient automatic
annotation framework through generalization experiments, facilitating
large-scale remote sensing image interpretation and intelligent recognition.
</details>

### [A Causality-Inspired Model for Intima-Media Thickening Assessment in Ultrasound Videos](https://arxiv.org/abs/2503.12418)
*Shuo Gao,Jingyang Zhang,Jun Xue,Meng Yang,Yang Chen,Guangquan Zhou*
<details>
  <summary>Abstract</summary>
Carotid atherosclerosis represents a significant health risk, with its early
diagnosis primarily dependent on ultrasound-based assessments of carotid
intima-media thickening. However, during carotid ultrasound screening,
significant view variations cause style shifts, impairing content cues related
to thickening, such as lumen anatomy, which introduces spurious correlations
that hinder assessment. Therefore, we propose a novel causal-inspired method
for assessing carotid intima-media thickening in frame-wise ultrasound videos,
which focuses on two aspects: eliminating spurious correlations caused by style
and enhancing causal content correlations. Specifically, we introduce a novel
Spurious Correlation Elimination (SCE) module to remove non-causal style
effects by enforcing prediction invariance with style perturbations.
Simultaneously, we propose a Causal Equivalence Consolidation (CEC) module to
strengthen causal content correlation through adversarial optimization during
content randomization. Simultaneously, we design a Causal Transition
Augmentation (CTA) module to ensure smooth causal flow by integrating an
auxiliary pathway with text prompts and connecting it through contrastive
learning. The experimental results on our in-house carotid ultrasound video
dataset achieved an accuracy of 86.93\%, demonstrating the superior performance
of the proposed method. Code is available at
\href{https://github.com/xielaobanyy/causal-imt}{https://github.com/xielaobanyy/causal-imt}.
</details>

### [EgoEvGesture: Gesture Recognition Based on Egocentric Event Camera](https://arxiv.org/abs/2503.12419)
*Luming Wang,Hao Shi,Xiaoting Yin,Kailun Yang,Kaiwei Wang*
<details>
  <summary>Abstract</summary>
Egocentric gesture recognition is a pivotal technology for enhancing natural
human-computer interaction, yet traditional RGB-based solutions suffer from
motion blur and illumination variations in dynamic scenarios. While event
cameras show distinct advantages in handling high dynamic range with ultra-low
power consumption, existing RGB-based architectures face inherent limitations
in processing asynchronous event streams due to their synchronous frame-based
nature. Moreover, from an egocentric perspective, event cameras record data
that include events generated by both head movements and hand gestures, thereby
increasing the complexity of gesture recognition. To address this, we propose a
novel network architecture specifically designed for event data processing,
incorporating (1) a lightweight CNN with asymmetric depthwise convolutions to
reduce parameters while preserving spatiotemporal features, (2) a plug-and-play
state-space model as context block that decouples head movement noise from
gesture dynamics, and (3) a parameter-free Bins-Temporal Shift Module (BSTM)
that shifts features along bins and temporal dimensions to fuse sparse events
efficiently. We further build the EgoEvGesture dataset, the first large-scale
dataset for egocentric gesture recognition using event cameras. Experimental
results demonstrate that our method achieves 62.7% accuracy in heterogeneous
testing with only 7M parameters, 3.1% higher than state-of-the-art approaches.
Notable misclassifications in freestyle motions stem from high inter-personal
variability and unseen test patterns differing from training data. Moreover,
our approach achieved a remarkable accuracy of 96.97% on DVS128 Gesture,
demonstrating strong cross-dataset generalization capability. The dataset and
models are made publicly available at
https://github.com/3190105222/EgoEv_Gesture.
</details>

### [Consistent-Point: Consistent Pseudo-Points for Semi-Supervised Crowd Counting and Localization](https://arxiv.org/abs/2503.12441)
*Yuda Zou,Zelong Liu,Yuliang Gu,Bo Du,Yongchao Xu*
<details>
  <summary>Abstract</summary>
Crowd counting and localization are important in applications such as public
security and traffic management. Existing methods have achieved impressive
results thanks to extensive laborious annotations. This paper propose a novel
point-localization-based semi-supervised crowd counting and localization method
termed Consistent-Point. We identify and address two inconsistencies of
pseudo-points, which have not been adequately explored. To enhance their
position consistency, we aggregate the positions of neighboring auxiliary
proposal-points. Additionally, an instance-wise uncertainty calibration is
proposed to improve the class consistency of pseudo-points. By generating more
consistent pseudo-points, Consistent-Point provides more stable supervision to
the training process, yielding improved results. Extensive experiments across
five widely used datasets and three different labeled ratio settings
demonstrate that our method achieves state-of-the-art performance in crowd
localization while also attaining impressive crowd counting results. The code
will be available.
</details>

### [BREEN: Bridge Data-Efficient Encoder-Free Multimodal Learning with Learnable Queries](https://arxiv.org/abs/2503.12446)
*Tianle Li,Yongming Rao,Winston Hu,Yu Cheng*
<details>
  <summary>Abstract</summary>
Encoder-free multimodal large language models(MLLMs) eliminate the need for a
well-trained vision encoder by directly processing image tokens before the
language model. While this approach reduces computational overhead and model
complexity, it often requires large amounts of training data to effectively
capture the visual knowledge typically encoded by vision models like CLIP. The
absence of a vision encoder implies that the model is likely to rely on
substantial data to learn the necessary visual-semantic alignments. In this
work, we present BREEN, a data-efficient encoder-free multimodal architecture
that mitigates this issue. BREEN leverages a learnable query and image experts
to achieve comparable performance with significantly less training data. The
learnable query, positioned between image and text tokens, is supervised by the
output of a pretrained CLIP model to distill visual knowledge, bridging the gap
between visual and textual modalities. Additionally, the image expert processes
image tokens and learnable queries independently, improving efficiency and
reducing interference with the LLM's textual capabilities. BREEN achieves
comparable performance to prior encoder-free state-of-the-art models like
Mono-InternVL, using only 13 million text-image pairs in training about one
percent of the data required by existing methods. Our work highlights a
promising direction for data-efficient encoder-free multimodal learning,
offering an alternative to traditional encoder-based approaches.
</details>

### [Causality Model for Semantic Understanding on Videos](https://arxiv.org/abs/2503.12447)
*Li Yicong*
<details>
  <summary>Abstract</summary>
After a decade of prosperity, the development of video understanding has
reached a critical juncture, where the sole reliance on massive data and
complex architectures is no longer a one-size-fits-all solution to all
situations. The presence of ubiquitous data imbalance hampers DNNs from
effectively learning the underlying causal mechanisms, leading to significant
performance drops when encountering distribution shifts, such as long-tail
imbalances and perturbed imbalances. This realization has prompted researchers
to seek alternative methodologies to capture causal patterns in video data. To
tackle these challenges and increase the robustness of DNNs, causal modeling
emerged as a principle to discover the true causal patterns behind the observed
correlations. This thesis focuses on the domain of semantic video understanding
and explores the potential of causal modeling to advance two fundamental tasks:
Video Relation Detection (VidVRD) and Video Question Answering (VideoQA).
</details>

### [LazyMAR: Accelerating Masked Autoregressive Models via Feature Caching](https://arxiv.org/abs/2503.12450)
*Feihong Yan,Qingyan Wei,Jiayi Tang,Jiajun Li,Yulin Wang,Xuming Hu,Huiqi Li,Linfeng Zhang*
<details>
  <summary>Abstract</summary>
Masked Autoregressive (MAR) models have emerged as a promising approach in
image generation, expected to surpass traditional autoregressive models in
computational efficiency by leveraging the capability of parallel decoding.
However, their dependence on bidirectional self-attention inherently conflicts
with conventional KV caching mechanisms, creating unexpected computational
bottlenecks that undermine their expected efficiency. To address this problem,
this paper studies the caching mechanism for MAR by leveraging two types of
redundancy: Token Redundancy indicates that a large portion of tokens have very
similar representations in the adjacent decoding steps, which allows us to
first cache them in previous steps and then reuse them in the later steps.
Condition Redundancy indicates that the difference between conditional and
unconditional output in classifier-free guidance exhibits very similar values
in adjacent steps. Based on these two redundancies, we propose LazyMAR, which
introduces two caching mechanisms to handle them one by one. LazyMAR is
training-free and plug-and-play for all MAR models. Experimental results
demonstrate that our method achieves 2.83 times acceleration with almost no
drop in generation quality. Our codes will be released in
https://github.com/feihongyan1/LazyMAR.
</details>

### [ISLR101: an Iranian Word-Level Sign Language Recognition Dataset](https://arxiv.org/abs/2503.12451)
*Hossein Ranjbar,Alireza Taheri*
<details>
  <summary>Abstract</summary>
Sign language recognition involves modeling complex multichannel information,
such as hand shapes and movements while relying on sufficient sign
language-specific data. However, sign languages are often under-resourced,
posing a significant challenge for research and development in this field. To
address this gap, we introduce ISLR101, the first publicly available Iranian
Sign Language dataset for isolated sign language recognition. This
comprehensive dataset includes 4,614 videos covering 101 distinct signs,
recorded by 10 different signers (3 deaf individuals, 2 sign language
interpreters, and 5 L2 learners) against varied backgrounds, with a resolution
of 800x600 pixels and a frame rate of 25 frames per second. It also includes
skeleton pose information extracted using OpenPose. We establish both a visual
appearance-based and a skeleton-based framework as baseline models, thoroughly
training and evaluating them on ISLR101. These models achieve 97.01% and 94.02%
accuracy on the test set, respectively. Additionally, we publish the train,
validation, and test splits to facilitate fair comparisons.
</details>

### [Shape Bias and Robustness Evaluation via Cue Decomposition for Image Classification and Segmentation](https://arxiv.org/abs/2503.12453)
*Edgar Heinert,Thomas Gottwald,Annika M√ºtze,Matthias Rottmann*
<details>
  <summary>Abstract</summary>
Previous works studied how deep neural networks (DNNs) perceive image content
in terms of their biases towards different image cues, such as texture and
shape. Previous methods to measure shape and texture biases are typically
style-transfer-based and limited to DNNs for image classification. In this
work, we provide a new evaluation procedure consisting of 1) a
cue-decomposition method that comprises two AI-free data pre-processing methods
extracting shape and texture cues, respectively, and 2) a novel
cue-decomposition shape bias evaluation metric that leverages the
cue-decomposition data. For application purposes we introduce a corresponding
cue-decomposition robustness metric that allows for the estimation of the
robustness of a DNN w.r.t. image corruptions. In our numerical experiments, our
findings for biases in image classification DNNs align with those of previous
evaluation metrics. However, our cue-decomposition robustness metric shows
superior results in terms of estimating the robustness of DNNs. Furthermore,
our results for DNNs on the semantic segmentation datasets Cityscapes and
ADE20k for the first time shed light into the biases of semantic segmentation
DNNs.
</details>

### [Exploring Contextual Attribute Density in Referring Expression Counting](https://arxiv.org/abs/2503.12460)
*Zhicheng Wang,Zhiyu Pan,Zhan Peng,Jian Cheng,Liwen Xiao,Wei Jiang,Zhiguo Cao*
<details>
  <summary>Abstract</summary>
Referring expression counting (REC) algorithms are for more flexible and
interactive counting ability across varied fine-grained text expressions.
However, the requirement for fine-grained attribute understanding poses
challenges for prior arts, as they struggle to accurately align attribute
information with correct visual patterns. Given the proven importance of
''visual density'', it is presumed that the limitations of current REC
approaches stem from an under-exploration of ''contextual attribute density''
(CAD). In the scope of REC, we define CAD as the measure of the information
intensity of one certain fine-grained attribute in visual regions. To model the
CAD, we propose a U-shape CAD estimator in which referring expression and
multi-scale visual features from GroundingDINO can interact with each other.
With additional density supervision, we can effectively encode CAD, which is
subsequently decoded via a novel attention procedure with CAD-refined queries.
Integrating all these contributions, our framework significantly outperforms
state-of-the-art REC methods, achieves $30\%$ error reduction in counting
metrics and a $10\%$ improvement in localization accuracy. The surprising
results shed light on the significance of contextual attribute density for REC.
Code will be at github.com/Xu3XiWang/CAD-GD.
</details>

### [MambaIC: State Space Models for High-Performance Learned Image Compression](https://arxiv.org/abs/2503.12461)
*Fanhu Zeng,Hao Tang,Yihua Shao,Siyu Chen,Ling Shao,Yan Wang*
<details>
  <summary>Abstract</summary>
A high-performance image compression algorithm is crucial for real-time
information transmission across numerous fields. Despite rapid progress in
image compression, computational inefficiency and poor redundancy modeling
still pose significant bottlenecks, limiting practical applications. Inspired
by the effectiveness of state space models (SSMs) in capturing long-range
dependencies, we leverage SSMs to address computational inefficiency in
existing methods and improve image compression from multiple perspectives. In
this paper, we integrate the advantages of SSMs for better
efficiency-performance trade-off and propose an enhanced image compression
approach through refined context modeling, which we term MambaIC. Specifically,
we explore context modeling to adaptively refine the representation of hidden
states. Additionally, we introduce window-based local attention into
channel-spatial entropy modeling to reduce potential spatial redundancy during
compression, thereby increasing efficiency. Comprehensive qualitative and
quantitative results validate the effectiveness and efficiency of our approach,
particularly for high-resolution image compression. Code is released at
https://github.com/AuroraZengfh/MambaIC.
</details>

### [Learning Privacy from Visual Entities](https://arxiv.org/abs/2503.12464)
*Alessio Xompero,Andrea Cavallaro*
<details>
  <summary>Abstract</summary>
Subjective interpretation and content diversity make predicting whether an
image is private or public a challenging task. Graph neural networks combined
with convolutional neural networks (CNNs), which consist of 14,000 to 500
millions parameters, generate features for visual entities (e.g., scene and
object types) and identify the entities that contribute to the decision. In
this paper, we show that using a simpler combination of transfer learning and a
CNN to relate privacy with scene types optimises only 732 parameters while
achieving comparable performance to that of graph-based methods. On the
contrary, end-to-end training of graph-based methods can mask the contribution
of individual components to the classification performance. Furthermore, we
show that a high-dimensional feature vector, extracted with CNNs for each
visual entity, is unnecessary and complexifies the model. The graph component
has also negligible impact on performance, which is driven by fine-tuning the
CNN to optimise image features for privacy nodes.
</details>

### [DPF-Net: Physical Imaging Model Embedded Data-Driven Underwater Image Enhancement](https://arxiv.org/abs/2503.12470)
*Han Mei,Kunqian Li,Shuaixin Liu,Chengzhi Ma,Qianli Jiang*
<details>
  <summary>Abstract</summary>
Due to the complex interplay of light absorption and scattering in the
underwater environment, underwater images experience significant degradation.
This research presents a two-stage underwater image enhancement network called
the Data-Driven and Physical Parameters Fusion Network (DPF-Net), which
harnesses the robustness of physical imaging models alongside the generality
and efficiency of data-driven methods. We first train a physical parameter
estimate module using synthetic datasets to guarantee the trustworthiness of
the physical parameters, rather than solely learning the fitting relationship
between raw and reference images by the application of the imaging equation, as
is common in prior studies. This module is subsequently trained in conjunction
with an enhancement network, where the estimated physical parameters are
integrated into a data-driven model within the embedding space. To maintain the
uniformity of the restoration process amid underwater imaging degradation, we
propose a physics-based degradation consistency loss. Additionally, we suggest
an innovative weak reference loss term utilizing the entire dataset, which
alleviates our model's reliance on the quality of individual reference images.
Our proposed DPF-Net demonstrates superior performance compared to other
benchmark methods across multiple test sets, achieving state-of-the-art
results. The source code and pre-trained models are available on the project
home page: https://github.com/OUCVisionGroup/DPF-Net.
</details>

### [Diffusion-based Synthetic Data Generation for Visible-Infrared Person Re-Identification](https://arxiv.org/abs/2503.12472)
*Wenbo Dai,Lijing Lu,Zhihang Li*
<details>
  <summary>Abstract</summary>
The performance of models is intricately linked to the abundance of training
data. In Visible-Infrared person Re-IDentification (VI-ReID) tasks, collecting
and annotating large-scale images of each individual under various cameras and
modalities is tedious, time-expensive, costly and must comply with data
protection laws, posing a severe challenge in meeting dataset requirements.
Current research investigates the generation of synthetic data as an efficient
and privacy-ensuring alternative to collecting real data in the field. However,
a specific data synthesis technique tailored for VI-ReID models has yet to be
explored. In this paper, we present a novel data generation framework, dubbed
Diffusion-based VI-ReID data Expansion (DiVE), that automatically obtain
massive RGB-IR paired images with identity preserving by decoupling identity
and modality to improve the performance of VI-ReID models. Specifically,
identity representation is acquired from a set of samples sharing the same ID,
whereas the modality of images is learned by fine-tuning the Stable Diffusion
(SD) on modality-specific data. DiVE extend the text-driven image synthesis to
identity-preserving RGB-IR multimodal image synthesis. This approach
significantly reduces data collection and annotation costs by directly
incorporating synthetic data into ReID model training. Experiments have
demonstrated that VI-ReID models trained on synthetic data produced by DiVE
consistently exhibit notable enhancements. In particular, the state-of-the-art
method, CAJ, trained with synthetic images, achieves an improvement of about
$9\%$ in mAP over the baseline on the LLCM dataset. Code:
https://github.com/BorgDiven/DiVE
</details>

### [Cross-Modal Consistency Learning for Sign Language Recognition](https://arxiv.org/abs/2503.12485)
*Kepeng Wu,Zecheng Li,Weichao Zhao,Hezhen Hu,Wengang Zhou,Houqiang Li*
<details>
  <summary>Abstract</summary>
Pre-training has been proven to be effective in boosting the performance of
Isolated Sign Language Recognition (ISLR). Existing pre-training methods solely
focus on the compact pose data, which eliminate background perturbation but
inevitably suffer from insufficient semantic cues compared to raw RGB videos.
Nevertheless, direct representation learning only from RGB videos remains
challenging due to the presence of sign-independent visual features. To address
this dilemma, we propose a Cross-modal Consistency Learning framework
(CCL-SLR), which leverages the cross-modal consistency from both RGB and pose
modalities based on self-supervised pre-training. First, CCL-SLR employs
contrastive learning for instance discrimination within and across modalities.
Through the single-modal and cross-modal contrastive learning, CCL-SLR
gradually aligns the feature spaces of RGB and pose modalities, thereby
extracting consistent sign representations. Second, we further introduce
Motion-Preserving Masking (MPM) and Semantic Positive Mining (SPM) techniques
to improve cross-modal consistency from the perspective of data augmentation
and sample similarity, respectively. Extensive experiments on four ISLR
benchmarks show that CCL-SLR achieves impressive performance, demonstrating its
effectiveness. The code will be released to the public.
</details>

### [GeoRSMLLM: A Multimodal Large Language Model for Vision-Language Tasks in Geoscience and Remote Sensing](https://arxiv.org/abs/2503.12490)
*Zilun Zhang,Haozhan Shen,Tiancheng Zhao,Bin Chen,Zian Guan,Yuhao Wang,Xu Jia,Yuxiang Cai,Yongheng Shang,Jianwei Yin*
<details>
  <summary>Abstract</summary>
The application of Vision-Language Models (VLMs) in remote sensing (RS) has
demonstrated significant potential in traditional tasks such as scene
classification, object detection, and image captioning. However, current
models, which excel in Referring Expression Comprehension (REC), struggle with
tasks involving complex instructions (e.g., exists multiple conditions) or
pixel-level operations like segmentation and change detection. In this white
paper, we provide a comprehensive hierarchical summary of vision-language tasks
in RS, categorized by the varying levels of cognitive capability required. We
introduce the Remote Sensing Vision-Language Task Set (RSVLTS), which includes
Open-Vocabulary Tasks (OVT), Referring Expression Tasks (RET), and Described
Object Tasks (DOT) with increased difficulty, and Visual Question Answering
(VQA) aloneside. Moreover, we propose a novel unified data representation using
a set-of-points approach for RSVLTS, along with a condition parser and a
self-augmentation strategy based on cyclic referring. These features are
integrated into the GeoRSMLLM model, and this enhanced model is designed to
handle a broad range of tasks of RSVLTS, paving the way for a more generalized
solution for vision-language tasks in geoscience and remote sensing.
</details>

### [Geometry-Aware Face Reconstruction Under Occluded Scenes](https://arxiv.org/abs/2503.12492)
*Dapeng Zhao*
<details>
  <summary>Abstract</summary>
Recently, deep learning-based 3D face reconstruction methods have
demonstrated promising advancements in terms of quality and efficiency.
Nevertheless, these techniques face challenges in effectively handling occluded
scenes and fail to capture intricate geometric facial details. Inspired by the
principles of GANs and bump mapping, we have successfully addressed these
issues. Our approach aims to deliver comprehensive 3D facial reconstructions,
even in the presence of occlusions.While maintaining the overall shape's
robustness, we introduce a mid-level shape refinement to the fundamental
structure. Furthermore, we illustrate how our method adeptly extends to
generate plausible details for obscured facial regions. We offer numerous
examples that showcase the effectiveness of our framework in producing
realistic results, where traditional methods often struggle. To substantiate
the superior adaptability of our approach, we have conducted extensive
experiments in the context of general 3D face reconstruction tasks, serving as
concrete evidence of its regulatory prowess compared to manual occlusion
removal methods.
</details>

### [Learning Contour-Guided 3D Face Reconstruction with Occlusions](https://arxiv.org/abs/2503.12494)
*Dapeng Zhao*
<details>
  <summary>Abstract</summary>
Recently, deep learning-based 3D face reconstruction methods have
demonstrated promising advancements in terms of quality and efficiency.
Nevertheless, these techniques face challenges in effectively handling occluded
scenes and fail to capture intricate geometric facial details. Inspired by the
principles of GANs and bump mapping, we have successfully addressed these
issues. Our approach aims to deliver comprehensive 3D facial reconstructions,
even in the presence of occlusions.While maintaining the overall shape's
robustness, we introduce a mid-level shape refinement to the fundamental
structure. Furthermore, we illustrate how our method adeptly extends to
generate plausible details for obscured facial regions. We offer numerous
examples that showcase the effectiveness of our framework in producing
realistic results, where traditional methods often struggle. To substantiate
the superior adaptability of our approach, we have conducted extensive
experiments in the context of general 3D face reconstruction tasks, serving as
concrete evidence of its regulatory prowess compared to manual occlusion
removal methods.
</details>

### [BS-Mamba for Black-Soil Area Detection On the Qinghai-Tibetan Plateau](https://arxiv.org/abs/2503.12495)
*Xuan Ma,Zewen Lv,Chengcai Ma,Tao Zhang,Yuelan Xin,Kun Zhan*
<details>
  <summary>Abstract</summary>
Extremely degraded grassland on the Qinghai-Tibetan Plateau (QTP) presents a
significant environmental challenge due to overgrazing, climate change, and
rodent activity, which degrade vegetation cover and soil quality. These
extremely degraded grassland on QTP, commonly referred to as black-soil area,
require accurate assessment to guide effective restoration efforts. In this
paper, we present a newly created QTP black-soil dataset, annotated under
expert guidance. We introduce a novel neural network model, BS-Mamba,
specifically designed for the black-soil area detection using UAV remote
sensing imagery. The BS-Mamba model demonstrates higher accuracy in identifying
black-soil area across two independent test datasets than the state-of-the-art
models. This research contributes to grassland restoration by providing an
efficient method for assessing the extent of black-soil area on the QTP.
</details>

### [Does Your Vision-Language Model Get Lost in the Long Video Sampling Dilemma?](https://arxiv.org/abs/2503.12496)
*Tianyuan Qu,Longxiang Tang,Bohao Peng,Senqiao Yang,Bei Yu,Jiaya Jia*
<details>
  <summary>Abstract</summary>
The rise of Large Vision-Language Models (LVLMs) has significantly advanced
video understanding. However, efficiently processing long videos remains a
challenge due to the ``Sampling Dilemma'': low-density sampling risks missing
critical information, while high-density sampling introduces redundancy. To
address this issue, we introduce LSDBench, the first benchmark designed to
evaluate LVLMs on long-video tasks by constructing high Necessary Sampling
Density (NSD) questions, where NSD represents the minimum sampling density
required to accurately answer a given question. LSDBench focuses on dense,
short-duration actions to rigorously assess the sampling strategies employed by
LVLMs. To tackle the challenges posed by high-NSD questions, we propose a novel
Reasoning-Driven Hierarchical Sampling (RHS) framework, which combines global
localization of question-relevant cues with local dense sampling for precise
inference. Additionally, we develop a lightweight Semantic-Guided Frame
Selector to prioritize informative frames, enabling RHS to achieve comparable
or superior performance with significantly fewer sampled frames. Together, our
LSDBench and RHS framework address the unique challenges of high-NSD long-video
tasks, setting a new standard for evaluating and improving LVLMs in this
domain.
</details>

### [Segment Any-Quality Images with Generative Latent Space Enhancement](https://arxiv.org/abs/2503.12507)
*Guangqian Guo,Yoong Guo,Xuehui Yu,Wenbo Li,Yaoxing Wang,Shan Gao*
<details>
  <summary>Abstract</summary>
Despite their success, Segment Anything Models (SAMs) experience significant
performance drops on severely degraded, low-quality images, limiting their
effectiveness in real-world scenarios. To address this, we propose GleSAM,
which utilizes Generative Latent space Enhancement to boost robustness on
low-quality images, thus enabling generalization across various image
qualities. Specifically, we adapt the concept of latent diffusion to SAM-based
segmentation frameworks and perform the generative diffusion process in the
latent space of SAM to reconstruct high-quality representation, thereby
improving segmentation. Additionally, we introduce two techniques to improve
compatibility between the pre-trained diffusion model and the segmentation
framework. Our method can be applied to pre-trained SAM and SAM2 with only
minimal additional learnable parameters, allowing for efficient optimization.
We also construct the LQSeg dataset with a greater diversity of degradation
types and levels for training and evaluating the model. Extensive experiments
demonstrate that GleSAM significantly improves segmentation robustness on
complex degradations while maintaining generalization to clear images.
Furthermore, GleSAM also performs well on unseen degradations, underscoring the
versatility of our approach and dataset.
</details>

### [AI-Powered Automated Model Construction for Patient-Specific CFD Simulations of Aortic Flows](https://arxiv.org/abs/2503.12515)
*Pan Du,Delin An,Chaoli Wang,Jian-Xun Wang*
<details>
  <summary>Abstract</summary>
Image-based modeling is essential for understanding cardiovascular
hemodynamics and advancing the diagnosis and treatment of cardiovascular
diseases. Constructing patient-specific vascular models remains
labor-intensive, error-prone, and time-consuming, limiting their clinical
applications. This study introduces a deep-learning framework that automates
the creation of simulation-ready vascular models from medical images. The
framework integrates a segmentation module for accurate voxel-based vessel
delineation with a surface deformation module that performs anatomically
consistent and unsupervised surface refinements guided by medical image data.
By unifying voxel segmentation and surface deformation into a single cohesive
pipeline, the framework addresses key limitations of existing methods,
enhancing geometric accuracy and computational efficiency. Evaluated on
publicly available datasets, the proposed approach demonstrates
state-of-the-art performance in segmentation and mesh quality while
significantly reducing manual effort and processing time. This work advances
the scalability and reliability of image-based computational modeling,
facilitating broader applications in clinical and research settings.
</details>

### [Multi Activity Sequence Alignment via Implicit Clustering](https://arxiv.org/abs/2503.12519)
*Taein Kwon,Zador Pataki,Mahdi Rad,Marc Pollefeys*
<details>
  <summary>Abstract</summary>
Self-supervised temporal sequence alignment can provide rich and effective
representations for a wide range of applications. However, existing methods for
achieving optimal performance are mostly limited to aligning sequences of the
same activity only and require separate models to be trained for each activity.
We propose a novel framework that overcomes these limitations using sequence
alignment via implicit clustering. Specifically, our key idea is to perform
implicit clip-level clustering while aligning frames in sequences. This coupled
with our proposed dual augmentation technique enhances the network's ability to
learn generalizable and discriminative representations. Our experiments show
that our proposed method outperforms state-of-the-art results and highlight the
generalization capability of our framework with multi activity and different
modalities on three diverse datasets, H2O, PennAction, and IKEA ASM. We will
release our code upon acceptance.
</details>

### [EditID: Training-Free Editable ID Customization for Text-to-Image Generation](https://arxiv.org/abs/2503.12526)
*Guandong Li,Zhaobin Chu*
<details>
  <summary>Abstract</summary>
We propose EditID, a training-free approach based on the DiT architecture,
which achieves highly editable customized IDs for text to image generation.
Existing text-to-image models for customized IDs typically focus more on ID
consistency while neglecting editability. It is challenging to alter facial
orientation, character attributes, and other features through prompts. EditID
addresses this by deconstructing the text-to-image model for customized IDs
into an image generation branch and a character feature branch. The character
feature branch is further decoupled into three modules: feature extraction,
feature fusion, and feature integration. By introducing a combination of
mapping features and shift features, along with controlling the intensity of ID
feature integration, EditID achieves semantic compression of local features
across network depths, forming an editable feature space. This enables the
successful generation of high-quality images with editable IDs while
maintaining ID consistency, achieving excellent results in the IBench
evaluation, which is an editability evaluation framework for the field of
customized ID text-to-image generation that quantitatively demonstrates the
superior performance of EditID. EditID is the first text-to-image solution to
propose customizable ID editability on the DiT architecture, meeting the
demands of long prompts and high quality image generation.
</details>

### [A Plug-and-Play Learning-based IMU Bias Factor for Robust Visual-Inertial Odometry](https://arxiv.org/abs/2503.12527)
*Yang Yi,Kunqing Wang,Jinpu Zhang,Zhen Tan,Xiangke Wang,Hui Shen,Dewen Hu*
<details>
  <summary>Abstract</summary>
The bias of low-cost Inertial Measurement Units (IMU) is a critical factor
affecting the performance of Visual-Inertial Odometry (VIO). In particular,
when visual tracking encounters errors, the optimized bias results may deviate
significantly from the true values, adversely impacting the system's stability
and localization precision. In this paper, we propose a novel plug-and-play
framework featuring the Inertial Prior Network (IPNet), which is designed to
accurately estimate IMU bias. Recognizing the substantial impact of initial
bias errors in low-cost inertial devices on system performance, our network
directly leverages raw IMU data to estimate the mean bias, eliminating the
dependency on historical estimates in traditional recursive predictions and
effectively preventing error propagation. Furthermore, we introduce an
iterative approach to calculate the mean value of the bias for network
training, addressing the lack of bias labels in many visual-inertial datasets.
The framework is evaluated on two public datasets and one self-collected
dataset. Extensive experiments demonstrate that our method significantly
enhances both localization precision and robustness, with the ATE-RMSE metric
improving on average by 46\%. The source code and video will be available at
\textcolor{red}{https://github.com/yiyscut/VIO-IPNet.git}.
</details>

### [Towards Suturing World Models: Learning Predictive Models for Robotic Surgical Tasks](https://arxiv.org/abs/2503.12531)
*Mehmet Kerem Turkcan,Mattia Ballo,Filippo Filicori,Zoran Kostic*
<details>
  <summary>Abstract</summary>
We introduce specialized diffusion-based generative models that capture the
spatiotemporal dynamics of fine-grained robotic surgical sub-stitch actions
through supervised learning on annotated laparoscopic surgery footage. The
proposed models form a foundation for data-driven world models capable of
simulating the biomechanical interactions and procedural dynamics of surgical
suturing with high temporal fidelity. Annotating a dataset of $\sim2K$ clips
extracted from simulation videos, we categorize surgical actions into
fine-grained sub-stitch classes including ideal and non-ideal executions of
needle positioning, targeting, driving, and withdrawal. We fine-tune two
state-of-the-art video diffusion models, LTX-Video and HunyuanVideo, to
generate high-fidelity surgical action sequences at $\ge$768x512 resolution and
$\ge$49 frames. For training our models, we explore both Low-Rank Adaptation
(LoRA) and full-model fine-tuning approaches. Our experimental results
demonstrate that these world models can effectively capture the dynamics of
suturing, potentially enabling improved training simulators, surgical skill
assessment tools, and autonomous surgical systems. The models also display the
capability to differentiate between ideal and non-ideal technique execution,
providing a foundation for building surgical training and evaluation systems.
We release our models for testing and as a foundation for future research.
Project Page: https://mkturkcan.github.io/suturingmodels/
</details>

### [STEVE: AStep Verification Pipeline for Computer-use Agent Training](https://arxiv.org/abs/2503.12532)
*Fanbin Lu,Zhisheng Zhong,Ziqin Wei,Shu Liu,Chi-Wing Fu,Jiaya Jia*
<details>
  <summary>Abstract</summary>
Developing AI agents to autonomously manipulate graphical user interfaces is
a long challenging task. Recent advances in data scaling law inspire us to
train computer-use agents with a scaled instruction set, yet using behavior
cloning to train agents still requires immense high-quality trajectories. To
meet the scalability need, we designed STEVE, a step verification pipeline for
computer-use agent training. First, we establish a large instruction set for
computer-use agents and collect trajectory data with some suboptimal agents.
GPT-4o is used to verify the correctness of each step in the trajectories based
on the screens before and after the action execution, assigning each step with
a binary label. Last, we adopt the Kahneman and Tversky Optimization to
optimize the agent from the binary stepwise labels. Extensive experiments
manifest that our agent outperforms supervised finetuning by leveraging both
positive and negative actions within a trajectory. Also, STEVE enables us to
train a 7B vision-language model as a computer-use agent, achieving leading
performance in the challenging live desktop environment WinAgentArena with
great efficiency at a reduced cost. Code and data:
https://github.com/FanbinLu/STEVE.
</details>

### [SPC-GS: Gaussian Splatting with Semantic-Prompt Consistency for Indoor Open-World Free-view Synthesis from Sparse Inputs](https://arxiv.org/abs/2503.12535)
*Guibiao Liao,Qing Li,Zhenyu Bao,Guoping Qiu,Kanglin Liu*
<details>
  <summary>Abstract</summary>
3D Gaussian Splatting-based indoor open-world free-view synthesis approaches
have shown significant performance with dense input images. However, they
exhibit poor performance when confronted with sparse inputs, primarily due to
the sparse distribution of Gaussian points and insufficient view supervision.
To relieve these challenges, we propose SPC-GS, leveraging Scene-layout-based
Gaussian Initialization (SGI) and Semantic-Prompt Consistency (SPC)
Regularization for open-world free view synthesis with sparse inputs.
Specifically, SGI provides a dense, scene-layout-based Gaussian distribution by
utilizing view-changed images generated from the video generation model and
view-constraint Gaussian points densification. Additionally, SPC mitigates
limited view supervision by employing semantic-prompt-based consistency
constraints developed by SAM2. This approach leverages available semantics from
training views, serving as instructive prompts, to optimize visually
overlapping regions in novel views with 2D and 3D consistency constraints.
Extensive experiments demonstrate the superior performance of SPC-GS across
Replica and ScanNet benchmarks. Notably, our SPC-GS achieves a 3.06 dB gain in
PSNR for reconstruction quality and a 7.3% improvement in mIoU for open-world
semantic segmentation.
</details>

### [BFANet: Revisiting 3D Semantic Segmentation with Boundary Feature Analysis](https://arxiv.org/abs/2503.12539)
*Weiguang Zhao,Rui Zhang,Qiufeng Wang,Guangliang Cheng,Kaizhu Huang*
<details>
  <summary>Abstract</summary>
3D semantic segmentation plays a fundamental and crucial role to understand
3D scenes. While contemporary state-of-the-art techniques predominantly
concentrate on elevating the overall performance of 3D semantic segmentation
based on general metrics (e.g. mIoU, mAcc, and oAcc), they unfortunately leave
the exploration of challenging regions for segmentation mostly neglected. In
this paper, we revisit 3D semantic segmentation through a more granular lens,
shedding light on subtle complexities that are typically overshadowed by
broader performance metrics. Concretely, we have delineated 3D semantic
segmentation errors into four comprehensive categories as well as corresponding
evaluation metrics tailored to each. Building upon this categorical framework,
we introduce an innovative 3D semantic segmentation network called BFANet that
incorporates detailed analysis of semantic boundary features. First, we design
the boundary-semantic module to decouple point cloud features into semantic and
boundary features, and fuse their query queue to enhance semantic features with
attention. Second, we introduce a more concise and accelerated boundary
pseudo-label calculation algorithm, which is 3.9 times faster than the
state-of-the-art, offering compatibility with data augmentation and enabling
efficient computation in training. Extensive experiments on benchmark data
indicate the superiority of our BFANet model, confirming the significance of
emphasizing the four uniquely designed metrics. Code is available at
https://github.com/weiguangzhao/BFANet.
</details>

### [ST-Think: How Multimodal Large Language Models Reason About 4D Worlds from Ego-Centric Videos](https://arxiv.org/abs/2503.12542)
*Peiran Wu,Yunze Liu,Chonghan Liu,Miao Liu,Junxiao Shen*
<details>
  <summary>Abstract</summary>
Humans excel at spatio-temporal reasoning, effortlessly interpreting dynamic
visual events from an egocentric viewpoint. However, whether multimodal large
language models (MLLMs) can similarly comprehend the 4D world remains
uncertain. This paper explores multimodal spatio-temporal reasoning from an
egocentric perspective, aiming to equip MLLMs with human-like reasoning
capabilities. To support this objective, we introduce Ego-ST Bench, a novel
benchmark containing over 5,000 question-answer pairs across four categories,
systematically evaluating spatial, temporal, and integrated spatio-temporal
reasoning. Additionally, we propose the ST-R1 Video model, a video-based
reasoning model that incorporates reverse thinking into its reinforcement
learning process, significantly enhancing performance. We combine
long-chain-of-thought (long-CoT) supervised fine-tuning with Group Relative
Policy Optimization (GRPO) reinforcement learning, achieving notable
improvements with limited high-quality data. Ego-ST Bench and ST-R1 provide
valuable insights and resources for advancing video-based spatio-temporal
reasoning research.
</details>

### [PEBench: A Fictitious Dataset to Benchmark Machine Unlearning for Multimodal Large Language Models](https://arxiv.org/abs/2503.12545)
*Zhaopan Xu,Pengfei Zhou,Weidong Tang,Jiaxin Ai,Wangbo Zhao,Xiaojiang Peng,Kai Wang,Yang You,Wenqi Shao,Hongxun Yao,Kaipeng Zhang*
<details>
  <summary>Abstract</summary>
In recent years, Multimodal Large Language Models (MLLMs) have demonstrated
remarkable advancements in tasks such as visual question answering, visual
understanding, and reasoning. However, this impressive progress relies on vast
amounts of data collected from the internet, raising significant concerns about
privacy and security. To address these issues, machine unlearning (MU) has
emerged as a promising solution, enabling the removal of specific knowledge
from an already trained model without requiring retraining from scratch.
Although MU for MLLMs has gained attention, current evaluations of its efficacy
remain incomplete, and the underlying problem is often poorly defined, which
hinders the development of strategies for creating more secure and trustworthy
systems. To bridge this gap, we introduce a benchmark, named PEBench, which
includes a dataset of personal entities and corresponding general event scenes,
designed to comprehensively assess the performance of MU for MLLMs. Through
PEBench, we aim to provide a standardized and robust framework to advance
research in secure and privacy-preserving multimodal models. We benchmarked 6
MU methods, revealing their strengths and limitations, and shedding light on
key challenges and opportunities for MU in MLLMs.
</details>

### [MTGS: Multi-Traversal Gaussian Splatting](https://arxiv.org/abs/2503.12552)
*Tianyu Li,Yihang Qiu,Zhenhua Wu,Carl Lindstr√∂m,Peng Su,Matthias Nie√üner,Hongyang Li*
<details>
  <summary>Abstract</summary>
Multi-traversal data, commonly collected through daily commutes or by
self-driving fleets, provides multiple viewpoints for scene reconstruction
within a road block. This data offers significant potential for high-quality
novel view synthesis, which is crucial for applications such as autonomous
vehicle simulators. However, inherent challenges in multi-traversal data often
result in suboptimal reconstruction quality, including variations in appearance
and the presence of dynamic objects. To address these issues, we propose
Multi-Traversal Gaussian Splatting (MTGS), a novel approach that reconstructs
high-quality driving scenes from arbitrarily collected multi-traversal data by
modeling a shared static geometry while separately handling dynamic elements
and appearance variations. Our method employs a multi-traversal dynamic scene
graph with a shared static node and traversal-specific dynamic nodes,
complemented by color correction nodes with learnable spherical harmonics
coefficient residuals. This approach enables high-fidelity novel view synthesis
and provides flexibility to navigate any viewpoint. We conduct extensive
experiments on a large-scale driving dataset, nuPlan, with multi-traversal
data. Our results demonstrate that MTGS improves LPIPS by 23.5% and geometry
accuracy by 46.3% compared to single-traversal baselines. The code and data
would be available to the public.
</details>

### [AdaReTaKe: Adaptive Redundancy Reduction to Perceive Longer for Video-language Understanding](https://arxiv.org/abs/2503.12559)
*Xiao Wang,Qingyi Si,Jianlong Wu,Shiyu Zhu,Li Cao,Liqiang Nie*
<details>
  <summary>Abstract</summary>
Multimodal Large Language Models (MLLMs) have revolutionized video
understanding, yet are still limited by context length when processing long
videos. Recent methods compress videos by leveraging visual redundancy
uniformly, yielding promising results. Nevertheless, our quantitative analysis
shows that redundancy varies significantly across time and model layers,
necessitating a more flexible compression strategy. We propose AdaReTaKe, a
training-free method that flexibly reduces visual redundancy by allocating
compression ratios among time and layers with theoretical guarantees.
Integrated into state-of-the-art MLLMs, AdaReTaKe improves processing capacity
from 256 to 2048 frames while preserving critical information. Experiments on
VideoMME, MLVU, LongVideoBench, and LVBench datasets demonstrate that AdaReTaKe
outperforms existing methods by 2.3% and 2.8% for 7B and 72B models,
respectively, with even greater improvements of 5.9% and 6.0% on the longest
LVBench. Our code is available at
https://github.com/SCZwangxiao/video-FlexReduc.git.
</details>

### [History-Aware Transformation of ReID Features for Multiple Object Tracking](https://arxiv.org/abs/2503.12562)
*Ruopeng Gao,Yuyao Wang,Chunxu Liu,Limin Wang*
<details>
  <summary>Abstract</summary>
The aim of multiple object tracking (MOT) is to detect all objects in a video
and bind them into multiple trajectories. Generally, this process is carried
out in two steps: detecting objects and associating them across frames based on
various cues and metrics. Many studies and applications adopt object
appearance, also known as re-identification (ReID) features, for target
matching through straightforward similarity calculation. However, we argue that
this practice is overly naive and thus overlooks the unique characteristics of
MOT tasks. Unlike regular re-identification tasks that strive to distinguish
all potential targets in a general representation, multi-object tracking
typically immerses itself in differentiating similar targets within the same
video sequence. Therefore, we believe that seeking a more suitable feature
representation space based on the different sample distributions of each
sequence will enhance tracking performance. In this paper, we propose using
history-aware transformations on ReID features to achieve more discriminative
appearance representations. Specifically, we treat historical trajectory
features as conditions and employ a tailored Fisher Linear Discriminant (FLD)
to find a spatial projection matrix that maximizes the differentiation between
different trajectories. Our extensive experiments reveal that this
training-free projection can significantly boost feature-only trackers to
achieve competitive, even superior tracking performance compared to
state-of-the-art methods while also demonstrating impressive zero-shot transfer
capabilities. This demonstrates the effectiveness of our proposal and further
encourages future investigation into the importance and customization of ReID
models in multiple object tracking. The code will be released at
https://github.com/HELLORPG/HATReID-MOT.
</details>

### [GAN-Based Single-Stage Defense for Traffic Sign Classification Under Adversarial Patch Attack](https://arxiv.org/abs/2503.12567)
*Abyad Enan,Mashrur Chowdhury*
<details>
  <summary>Abstract</summary>
Computer Vision plays a critical role in ensuring the safe navigation of
autonomous vehicles (AVs). An AV perception module is responsible for capturing
and interpreting the surrounding environment to facilitate safe navigation.
This module enables AVs to recognize traffic signs, traffic lights, and various
road users. However, the perception module is vulnerable to adversarial
attacks, which can compromise their accuracy and reliability. One such attack
is the adversarial patch attack (APA), a physical attack in which an adversary
strategically places a specially crafted sticker on an object to deceive object
classifiers. In APA, an adversarial patch is positioned on a target object,
leading the classifier to misidentify it. Such an APA can cause AVs to
misclassify traffic signs, leading to catastrophic incidents. To enhance the
security of an AV perception system against APAs, this study develops a
Generative Adversarial Network (GAN)-based single-stage defense strategy for
traffic sign classification. This approach is tailored to defend against APAs
on different classes of traffic signs without prior knowledge of a patch's
design. This study found this approach to be effective against patches of
varying sizes. Our experimental analysis demonstrates that the defense strategy
presented in this paper improves the classifier's accuracy under APA conditions
by up to 80.8% and enhances overall classification accuracy for all the traffic
signs considered in this study by 58%, compared to a classifier without any
defense mechanism. Our defense strategy is model-agnostic, making it applicable
to any traffic sign classifier, regardless of the underlying classification
model.
</details>

### [Deblur Gaussian Splatting SLAM](https://arxiv.org/abs/2503.12572)
*Francesco Girlanda,Denys Rozumnyi,Marc Pollefeys,Martin R. Oswald*
<details>
  <summary>Abstract</summary>
We present Deblur-SLAM, a robust RGB SLAM pipeline designed to recover sharp
reconstructions from motion-blurred inputs. The proposed method bridges the
strengths of both frame-to-frame and frame-to-model approaches to model
sub-frame camera trajectories that lead to high-fidelity reconstructions in
motion-blurred settings. Moreover, our pipeline incorporates techniques such as
online loop closure and global bundle adjustment to achieve a dense and precise
global trajectory. We model the physical image formation process of
motion-blurred images and minimize the error between the observed blurry images
and rendered blurry images obtained by averaging sharp virtual sub-frame
images. Additionally, by utilizing a monocular depth estimator alongside the
online deformation of Gaussians, we ensure precise mapping and enhanced image
deblurring. The proposed SLAM pipeline integrates all these components to
improve the results. We achieve state-of-the-art results for sharp map
estimation and sub-frame trajectory recovery both on synthetic and real-world
blurry input data.
</details>

### [BalancedDPO: Adaptive Multi-Metric Alignment](https://arxiv.org/abs/2503.12575)
*Dipesh Tamboli,Souradip Chakraborty,Aditya Malusare,Biplab Banerjee,Amrit Singh Bedi,Vaneet Aggarwal*
<details>
  <summary>Abstract</summary>
Text-to-image (T2I) diffusion models have made remarkable advancements, yet
aligning them with diverse preferences remains a persistent challenge. Current
methods often optimize single metrics or depend on narrowly curated datasets,
leading to overfitting and limited generalization across key visual quality
metrics. We present BalancedDPO, a novel extension of Direct Preference
Optimization (DPO) that addresses these limitations by simultaneously aligning
T2I diffusion models with multiple metrics, including human preference, CLIP
score, and aesthetic quality. Our key novelty lies in aggregating consensus
labels from diverse metrics in the preference distribution space as compared to
existing reward mixing approaches, enabling robust and scalable multi-metric
alignment while maintaining the simplicity of the standard DPO pipeline that we
refer to as BalancedDPO. Our evaluations on the Pick-a-Pic, PartiPrompt and HPD
datasets show that BalancedDPO achieves state-of-the-art results, outperforming
existing approaches across all major metrics. BalancedDPO improves the average
win rates by 15%, 7.1%, and 10.3% on Pick-a-pic, PartiPrompt and HPD,
respectively, from the DiffusionDPO.
</details>

### [Progressive Limb-Aware Virtual Try-On](https://arxiv.org/abs/2503.12588)
*Xiaoyu Han,Shengping Zhang,Qinglin Liu,Zonglin Li,Chenyang Wang*
<details>
  <summary>Abstract</summary>
Existing image-based virtual try-on methods directly transfer specific
clothing to a human image without utilizing clothing attributes to refine the
transferred clothing geometry and textures, which causes incomplete and blurred
clothing appearances. In addition, these methods usually mask the limb textures
of the input for the clothing-agnostic person representation, which results in
inaccurate predictions for human limb regions (i.e., the exposed arm skin),
especially when transforming between long-sleeved and short-sleeved garments.
To address these problems, we present a progressive virtual try-on framework,
named PL-VTON, which performs pixel-level clothing warping based on multiple
attributes of clothing and embeds explicit limb-aware features to generate
photo-realistic try-on results. Specifically, we design a Multi-attribute
Clothing Warping (MCW) module that adopts a two-stage alignment strategy based
on multiple attributes to progressively estimate pixel-level clothing
displacements. A Human Parsing Estimator (HPE) is then introduced to
semantically divide the person into various regions, which provides structural
constraints on the human body and therefore alleviates texture bleeding between
clothing and limb regions. Finally, we propose a Limb-aware Texture Fusion
(LTF) module to estimate high-quality details in limb regions by fusing
textures of the clothing and the human body with the guidance of explicit
limb-aware features. Extensive experiments demonstrate that our proposed method
outperforms the state-of-the-art virtual try-on methods both qualitatively and
quantitatively. The code is available at https://github.com/xyhanHIT/PL-VTON.
</details>

### [Personalize Anything for Free with Diffusion Transformer](https://arxiv.org/abs/2503.12590)
*Haoran Feng,Zehuan Huang,Lin Li,Hairong Lv,Lu Sheng*
<details>
  <summary>Abstract</summary>
Personalized image generation aims to produce images of user-specified
concepts while enabling flexible editing. Recent training-free approaches,
while exhibit higher computational efficiency than training-based methods,
struggle with identity preservation, applicability, and compatibility with
diffusion transformers (DiTs). In this paper, we uncover the untapped potential
of DiT, where simply replacing denoising tokens with those of a reference
subject achieves zero-shot subject reconstruction. This simple yet effective
feature injection technique unlocks diverse scenarios, from personalization to
image editing. Building upon this observation, we propose \textbf{Personalize
Anything}, a training-free framework that achieves personalized image
generation in DiT through: 1) timestep-adaptive token replacement that enforces
subject consistency via early-stage injection and enhances flexibility through
late-stage regularization, and 2) patch perturbation strategies to boost
structural diversity. Our method seamlessly supports layout-guided generation,
multi-subject personalization, and mask-controlled editing. Evaluations
demonstrate state-of-the-art performance in identity preservation and
versatility. Our work establishes new insights into DiTs while delivering a
practical paradigm for efficient personalization.
</details>

### [Point Cloud Based Scene Segmentation: A Survey](https://arxiv.org/abs/2503.12595)
*Dan Halperin,Niklas Eisl*
<details>
  <summary>Abstract</summary>
Autonomous driving is a safety-critical application, and it is therefore a
top priority that the accompanying assistance systems are able to provide
precise information about the surrounding environment of the vehicle. Tasks
such as 3D Object Detection deliver an insufficiently detailed understanding of
the surrounding scene because they only predict a bounding box for foreground
objects. In contrast, 3D Semantic Segmentation provides richer and denser
information about the environment by assigning a label to each individual
point, which is of paramount importance for autonomous driving tasks, such as
navigation or lane changes. To inspire future research, in this review paper,
we provide a comprehensive overview of the current state-of-the-art methods in
the field of Point Cloud Semantic Segmentation for autonomous driving. We
categorize the approaches into projection-based, 3D-based and hybrid methods.
Moreover, we discuss the most important and commonly used datasets for this
task and also emphasize the importance of synthetic data to support research
when real-world data is limited. We further present the results of the
different methods and compare them with respect to their segmentation accuracy
and efficiency.
</details>

### [Multimodal Chain-of-Thought Reasoning: A Comprehensive Survey](https://arxiv.org/abs/2503.12605)
*Yaoting Wang,Shengqiong Wu,Yuecheng Zhang,William Wang,Ziwei Liu,Jiebo Luo,Hao Fei*
<details>
  <summary>Abstract</summary>
By extending the advantage of chain-of-thought (CoT) reasoning in human-like
step-by-step processes to multimodal contexts, multimodal CoT (MCoT) reasoning
has recently garnered significant research attention, especially in the
integration with multimodal large language models (MLLMs). Existing MCoT
studies design various methodologies and innovative reasoning paradigms to
address the unique challenges of image, video, speech, audio, 3D, and
structured data across different modalities, achieving extensive success in
applications such as robotics, healthcare, autonomous driving, and multimodal
generation. However, MCoT still presents distinct challenges and opportunities
that require further focus to ensure consistent thriving in this field, where,
unfortunately, an up-to-date review of this domain is lacking. To bridge this
gap, we present the first systematic survey of MCoT reasoning, elucidating the
relevant foundational concepts and definitions. We offer a comprehensive
taxonomy and an in-depth analysis of current methodologies from diverse
perspectives across various application scenarios. Furthermore, we provide
insights into existing challenges and future research directions, aiming to
foster innovation toward multimodal AGI.
</details>

### [LATINO-PRO: LAtent consisTency INverse sOlver with PRompt Optimization](https://arxiv.org/abs/2503.12615)
*Alessio Spagnoletti,Jean Prost,Andr√©s Almansa,Nicolas Papadakis,Marcelo Pereyra*
<details>
  <summary>Abstract</summary>
Text-to-image latent diffusion models (LDMs) have recently emerged as
powerful generative models with great potential for solving inverse problems in
imaging. However, leveraging such models in a Plug & Play (PnP), zero-shot
manner remains challenging because it requires identifying a suitable text
prompt for the unknown image of interest. Also, existing text-to-image PnP
approaches are highly computationally expensive. We herein address these
challenges by proposing a novel PnP inference paradigm specifically designed
for embedding generative models within stochastic inverse solvers, with special
attention to Latent Consistency Models (LCMs), which distill LDMs into fast
generators. We leverage our framework to propose LAtent consisTency INverse
sOlver (LATINO), the first zero-shot PnP framework to solve inverse problems
with priors encoded by LCMs. Our conditioning mechanism avoids automatic
differentiation and reaches SOTA quality in as little as 8 neural function
evaluations. As a result, LATINO delivers remarkably accurate solutions and is
significantly more memory and computationally efficient than previous
approaches. We then embed LATINO within an empirical Bayesian framework that
automatically calibrates the text prompt from the observed measurements by
marginal maximum likelihood estimation. Extensive experiments show that prompt
self-calibration greatly improves estimation, allowing LATINO with PRompt
Optimization to define new SOTAs in image reconstruction quality and
computational efficiency.
</details>

### [Scaling Semantic Categories: Investigating the Impact on Vision Transformer Labeling Performance](https://arxiv.org/abs/2503.12617)
*Anthony Lamelas,Harrison Muchnic*
<details>
  <summary>Abstract</summary>
This study explores the impact of scaling semantic categories on the image
classification performance of vision transformers (ViTs). In this specific
case, the CLIP server provided by Jina AI is used for experimentation. The
research hypothesizes that as the number of ground truth and artificially
introduced semantically equivalent categories increases, the labeling accuracy
of ViTs improves until a theoretical maximum or limit is reached. A wide
variety of image datasets were chosen to test this hypothesis. These datasets
were processed through a custom function in Python designed to evaluate the
model's accuracy, with adjustments being made to account for format differences
between datasets. By exponentially introducing new redundant categories, the
experiment assessed accuracy trends until they plateaued, decreased, or
fluctuated inconsistently. The findings show that while semantic scaling
initially increases model performance, the benefits diminish or reverse after
surpassing a critical threshold, providing insight into the limitations and
possible optimization of category labeling strategies for ViTs.
</details>

### [Online Misinformation Detection in Live Streaming Videos](https://arxiv.org/abs/2503.12627)
*Rui Cao*
<details>
  <summary>Abstract</summary>
Online misinformation detection is an important issue and methods are
proposed to detect and curb misinformation in various forms. However, previous
studies are conducted in an offline manner. We claim a realistic misinformation
detection setting that has not been studied yet is online misinformation
detection in live streaming videos (MDLS). In the proposal, we formulate the
problem of MDLS and illustrate the importance and the challenge of the task.
Besides, we propose feasible ways of developing the problem into AI challenges
as well as potential solutions to the problem.
</details>

### [UniVG: A Generalist Diffusion Model for Unified Image Generation and Editing](https://arxiv.org/abs/2503.12652)
*Tsu-Jui Fu,Yusu Qian,Chen Chen,Wenze Hu,Zhe Gan,Yinfei Yang*
<details>
  <summary>Abstract</summary>
Text-to-Image (T2I) diffusion models have shown impressive results in
generating visually compelling images following user prompts. Building on this,
various methods further fine-tune the pre-trained T2I model for specific tasks.
However, this requires separate model architectures, training designs, and
multiple parameter sets to handle different tasks. In this paper, we introduce
UniVG, a generalist diffusion model capable of supporting a diverse range of
image generation tasks with a single set of weights. UniVG treats multi-modal
inputs as unified conditions to enable various downstream applications, ranging
from T2I generation, inpainting, instruction-based editing, identity-preserving
generation, and layout-guided generation, to depth estimation and referring
segmentation. Through comprehensive empirical studies on data mixing and
multi-task training, we provide detailed insights into the training processes
and decisions that inform our final designs. For example, we show that T2I
generation and other tasks, such as instruction-based editing, can coexist
without performance trade-offs, while auxiliary tasks like depth estimation and
referring segmentation enhance image editing. Notably, our model can even
outperform some task-specific models on their respective benchmarks, marking a
significant step towards a unified image generation model.
</details>

### [Logic-RAG: Augmenting Large Multimodal Models with Visual-Spatial Knowledge for Road Scene Understanding](https://arxiv.org/abs/2503.12663)
*Imran Kabir,Md Alimoor Reza,Syed Billah*
<details>
  <summary>Abstract</summary>
Large multimodal models (LMMs) are increasingly integrated into autonomous
driving systems for user interaction. However, their limitations in
fine-grained spatial reasoning pose challenges for system interpretability and
user trust. We introduce Logic-RAG, a novel Retrieval-Augmented Generation
(RAG) framework that improves LMMs' spatial understanding in driving scenarios.
Logic-RAG constructs a dynamic knowledge base (KB) about object-object
relationships in first-order logic (FOL) using a perception module, a
query-to-logic embedder, and a logical inference engine. We evaluated Logic-RAG
on visual-spatial queries using both synthetic and real-world driving videos.
When using popular LMMs (GPT-4V, Claude 3.5) as proxies for an autonomous
driving system, these models achieved only 55% accuracy on synthetic driving
scenes and under 75% on real-world driving scenes. Augmenting them with
Logic-RAG increased their accuracies to over 80% and 90%, respectively. An
ablation study showed that even without logical inference, the fact-based
context constructed by Logic-RAG alone improved accuracy by 15%. Logic-RAG is
extensible: it allows seamless replacement of individual components with
improved versions and enables domain experts to compose new knowledge in both
FOL and natural language. In sum, Logic-RAG addresses critical spatial
reasoning deficiencies in LMMs for autonomous driving applications. Code and
data are available at https://github.com/Imran2205/LogicRAG.
</details>

### [Domain Generalization for Improved Human Activity Recognition in Office Space Videos Using Adaptive Pre-processing](https://arxiv.org/abs/2503.12678)
*Partho Ghosh,Raisa Bentay Hossain,Mohammad Zunaed,Taufiq Hasan*
<details>
  <summary>Abstract</summary>
Automatic video activity recognition is crucial across numerous domains like
surveillance, healthcare, and robotics. However, recognizing human activities
from video data becomes challenging when training and test data stem from
diverse domains. Domain generalization, adapting to unforeseen domains, is thus
essential. This paper focuses on office activity recognition amidst
environmental variability. We propose three pre-processing techniques
applicable to any video encoder, enhancing robustness against environmental
variations. Our study showcases the efficacy of MViT, a leading
state-of-the-art video classification model, and other video encoders combined
with our techniques, outperforming state-of-the-art domain adaptation methods.
Our approach significantly boosts accuracy, precision, recall and F1 score on
unseen domains, emphasizing its adaptability in real-world scenarios with
diverse video data sources. This method lays a foundation for more reliable
video activity recognition systems across heterogeneous data domains.
</details>

### [Dynamic Angle Selection in X-Ray CT: A Reinforcement Learning Approach to Optimal Stopping](https://arxiv.org/abs/2503.12688)
*Tianyuan Wang*
<details>
  <summary>Abstract</summary>
In industrial X-ray Computed Tomography (CT), the need for rapid in-line
inspection is critical. Sparse-angle tomography plays a significant role in
this by reducing the required number of projections, thereby accelerating
processing and conserving resources. Most existing methods aim to balance
reconstruction quality and scanning time, typically relying on fixed scan
durations. Adaptive adjustment of the number of angles is essential; for
instance, more angles may be required for objects with complex geometries or
noisier projections. The concept of optimal stopping, which dynamically adjusts
this balance according to varying industrial needs, remains underutilized.
Building on our previous work, we integrate optimal stopping into sequential
Optimal Experimental Design (OED). We propose a novel method for computing the
policy gradient within the Actor-Critic framework, enabling the development of
adaptive policies for informative angle selection and scan termination.
Additionally, we investigated the gap between simulation and real-world
applications in the context of the developed learning-based method. Our trained
model, developed using synthetic data, demonstrates reliable performance when
applied to real-world data. This approach enhances the flexibility of CT
operations and expands the applicability of sparse-angle tomography in
industrial settings.
</details>

### [MagicID: Hybrid Preference Optimization for ID-Consistent and Dynamic-Preserved Video Customization](https://arxiv.org/abs/2503.12689)
*Hengjia Li,Lifan Jiang,Xi Xiao,Tianyang Wang,Hongwei Yi,Boxi Wu,Deng Cai*
<details>
  <summary>Abstract</summary>
Video identity customization seeks to produce high-fidelity videos that
maintain consistent identity and exhibit significant dynamics based on users'
reference images. However, existing approaches face two key challenges:
identity degradation over extended video length and reduced dynamics during
training, primarily due to their reliance on traditional self-reconstruction
training with static images. To address these issues, we introduce
$\textbf{MagicID}$, a novel framework designed to directly promote the
generation of identity-consistent and dynamically rich videos tailored to user
preferences. Specifically, we propose constructing pairwise preference video
data with explicit identity and dynamic rewards for preference learning,
instead of sticking to the traditional self-reconstruction. To address the
constraints of customized preference data, we introduce a hybrid sampling
strategy. This approach first prioritizes identity preservation by leveraging
static videos derived from reference images, then enhances dynamic motion
quality in the generated videos using a Frontier-based sampling method. By
utilizing these hybrid preference pairs, we optimize the model to align with
the reward differences between pairs of customized preferences. Extensive
experiments show that MagicID successfully achieves consistent identity and
natural dynamics, surpassing existing methods across various metrics.
</details>

### [AnyCalib: On-Manifold Learning for Model-Agnostic Single-View Camera Calibration](https://arxiv.org/abs/2503.12701)
*Javier Tirado-Gar√≠n,Javier Civera*
<details>
  <summary>Abstract</summary>
We present AnyCalib, a method for calibrating the intrinsic parameters of a
camera from a single in-the-wild image, that is agnostic to the camera model.
Current methods are predominantly tailored to specific camera models and/or
require extrinsic cues, such as the direction of gravity, to be visible in the
image. In contrast, we argue that the perspective and distortion cues inherent
in images are sufficient for model-agnostic camera calibration. To demonstrate
this, we frame the calibration process as the regression of the rays
corresponding to each pixel. We show, for the first time, that this
intermediate representation allows for a closed-form recovery of the intrinsics
for a wide range of camera models, including but not limited to: pinhole,
Brown-Conrady and Kannala-Brandt. Our approach also applies to edited --
cropped and stretched -- images. Experimentally, we demonstrate that AnyCalib
consistently outperforms alternative methods, including 3D foundation models,
despite being trained on orders of magnitude less data. Code is available at
https://github.com/javrtg/AnyCalib.
</details>

### [SatDepth: A Novel Dataset for Satellite Image Matching](https://arxiv.org/abs/2503.12706)
*Rahul Deshmukh,Avinash Kak*
<details>
  <summary>Abstract</summary>
Recent advances in deep-learning based methods for image matching have
demonstrated their superiority over traditional algorithms, enabling
correspondence estimation in challenging scenes with significant differences in
viewing angles, illumination and weather conditions. However, the existing
datasets, learning frameworks, and evaluation metrics for the deep-learning
based methods are limited to ground-based images recorded with pinhole cameras
and have not been explored for satellite images. In this paper, we present
``SatDepth'', a novel dataset that provides dense ground-truth correspondences
for training image matching frameworks meant specifically for satellite images.
Satellites capture images from various viewing angles and tracks through
multiple revisits over a region. To manage this variability, we propose a
dataset balancing strategy through a novel image rotation augmentation
procedure. This procedure allows for the discovery of corresponding pixels even
in the presence of large rotational differences between the images. We
benchmark four existing image matching frameworks using our dataset and carry
out an ablation study that confirms that the models trained with our dataset
with rotation augmentation outperform (up to 40% increase in precision) the
models trained with other datasets, especially when there exist large
rotational differences between the images.
</details>

### [GenStereo: Towards Open-World Generation of Stereo Images and Unsupervised Matching](https://arxiv.org/abs/2503.12720)
*Feng Qiao,Zhexiao Xiong,Eric Xing,Nathan Jacobs*
<details>
  <summary>Abstract</summary>
Stereo images are fundamental to numerous applications, including extended
reality (XR) devices, autonomous driving, and robotics. Unfortunately,
acquiring high-quality stereo images remains challenging due to the precise
calibration requirements of dual-camera setups and the complexity of obtaining
accurate, dense disparity maps. Existing stereo image generation methods
typically focus on either visual quality for viewing or geometric accuracy for
matching, but not both. We introduce GenStereo, a diffusion-based approach, to
bridge this gap. The method includes two primary innovations (1) conditioning
the diffusion process on a disparity-aware coordinate embedding and a warped
input image, allowing for more precise stereo alignment than previous methods,
and (2) an adaptive fusion mechanism that intelligently combines the
diffusion-generated image with a warped image, improving both realism and
disparity consistency. Through extensive training on 11 diverse stereo
datasets, GenStereo demonstrates strong generalization ability. GenStereo
achieves state-of-the-art performance in both stereo image generation and
unsupervised stereo matching tasks. Our framework eliminates the need for
complex hardware setups while enabling high-quality stereo image generation,
making it valuable for both real-world applications and unsupervised learning
scenarios. Project page is available at https://qjizhi.github.io/genstereo
</details>

### [Navigating Heat Exposure: Simulation of Route Planning Based on Visual Language Model Agents](https://arxiv.org/abs/2503.12731)
*Haoran Ma,Kaihan Zhang,Jiannan Cai*
<details>
  <summary>Abstract</summary>
Heat exposure significantly influences pedestrian routing behaviors. Existing
methods such as agent-based modeling (ABM) and empirical measurements fail to
account for individual physiological variations and environmental perception
mechanisms under thermal stress. This results in a lack of human-centred,
heat-adaptive routing suggestions. To address these limitations, we propose a
novel Vision Language Model (VLM)-driven Persona-Perception-Planning-Memory
(PPPM) framework that integrating street view imagery and urban network
topology to simulate heat-adaptive pedestrian routing. Through structured
prompt engineering on Gemini-2.0 model, eight distinct heat-sensitive personas
were created to model mobility behaviors during heat exposure, with empirical
validation through questionnaire survey. Results demonstrate that simulation
outputs effectively capture inter-persona variations, achieving high
significant congruence with observed route preferences and highlighting
differences in the factors driving agents decisions. Our framework is highly
cost-effective, with simulations costing 0.006USD and taking 47.81s per route.
This Artificial Intelligence-Generated Content (AIGC) methodology advances
urban climate adaptation research by enabling high-resolution simulation of
thermal-responsive mobility patterns, providing actionable insights for
climate-resilient urban planning.
</details>

### [Stereo Event-based, 6-DOF Pose Tracking for Uncooperative Spacecraft](https://arxiv.org/abs/2503.12732)
*Zibin Liu,Banglei Guan,Yang Shang,Yifei Bian,Pengju Sun,Qifeng Yu*
<details>
  <summary>Abstract</summary>
Pose tracking of uncooperative spacecraft is an essential technology for
space exploration and on-orbit servicing, which remains an open problem. Event
cameras possess numerous advantages, such as high dynamic range, high temporal
resolution, and low power consumption. These attributes hold the promise of
overcoming challenges encountered by conventional cameras, including motion
blur and extreme illumination, among others. To address the standard on-orbit
observation missions, we propose a line-based pose tracking method for
uncooperative spacecraft utilizing a stereo event camera. To begin with, we
estimate the wireframe model of uncooperative spacecraft, leveraging the
spatio-temporal consistency of stereo event streams for line-based
reconstruction. Then, we develop an effective strategy to establish
correspondences between events and projected lines of uncooperative spacecraft.
Using these correspondences, we formulate the pose tracking as a continuous
optimization process over 6-DOF motion parameters, achieved by minimizing
event-line distances. Moreover, we construct a stereo event-based uncooperative
spacecraft motion dataset, encompassing both simulated and real events. The
proposed method is quantitatively evaluated through experiments conducted on
our self-collected dataset, demonstrating an improvement in terms of
effectiveness and accuracy over competing methods. The code will be
open-sourced at https://github.com/Zibin6/SE6PT.
</details>

### [ProtoDepth: Unsupervised Continual Depth Completion with Prototypes](https://arxiv.org/abs/2503.12745)
*Patrick Rim,Hyoungseob Park,S. Gangopadhyay,Ziyao Zeng,Younjoon Chung,Alex Wong*
<details>
  <summary>Abstract</summary>
We present ProtoDepth, a novel prototype-based approach for continual
learning of unsupervised depth completion, the multimodal 3D reconstruction
task of predicting dense depth maps from RGB images and sparse point clouds.
The unsupervised learning paradigm is well-suited for continual learning, as
ground truth is not needed. However, when training on new non-stationary
distributions, depth completion models will catastrophically forget previously
learned information. We address forgetting by learning prototype sets that
adapt the latent features of a frozen pretrained model to new domains. Since
the original weights are not modified, ProtoDepth does not forget when
test-time domain identity is known. To extend ProtoDepth to the challenging
setting where the test-time domain identity is withheld, we propose to learn
domain descriptors that enable the model to select the appropriate prototype
set for inference. We evaluate ProtoDepth on benchmark dataset sequences, where
we reduce forgetting compared to baselines by 52.2% for indoor and 53.2% for
outdoor to achieve the state of the art.
</details>

### [R3-Avatar: Record and Retrieve Temporal Codebook for Reconstructing Photorealistic Human Avatars](https://arxiv.org/abs/2503.12751)
*Yifan Zhan,Wangze Xu,Qingtian Zhu,Muyao Niu,Mingze Ma,Yifei Liu,Zhihang Zhong,Xiao Sun,Yinqiang Zheng*
<details>
  <summary>Abstract</summary>
We present R3-Avatar, incorporating a temporal codebook, to overcome the
inability of human avatars to be both animatable and of high-fidelity rendering
quality. Existing video-based reconstruction of 3D human avatars either focuses
solely on rendering, lacking animation support, or learns a pose-appearance
mapping for animating, which degrades under limited training poses or complex
clothing. In this paper, we adopt a "record-retrieve-reconstruct" strategy that
ensures high-quality rendering from novel views while mitigating degradation in
novel poses. Specifically, disambiguating timestamps record temporal appearance
variations in a codebook, ensuring high-fidelity novel-view rendering, while
novel poses retrieve corresponding timestamps by matching the most similar
training poses for augmented appearance. Our R3-Avatar outperforms cutting-edge
video-based human avatar reconstruction, particularly in overcoming visual
quality degradation in extreme scenarios with limited training human poses and
complex clothing.
</details>

### [VasTSD: Learning 3D Vascular Tree-state Space Diffusion Model for Angiography Synthesis](https://arxiv.org/abs/2503.12758)
*Zhifeng Wang,Renjiao Yi,Xin Wen,Chenyang Zhu,Kai Xu*
<details>
  <summary>Abstract</summary>
Angiography imaging is a medical imaging technique that enhances the
visibility of blood vessels within the body by using contrast agents.
Angiographic images can effectively assist in the diagnosis of vascular
diseases. However, contrast agents may bring extra radiation exposure which is
harmful to patients with health risks. To mitigate these concerns, in this
paper, we aim to automatically generate angiography from non-angiographic
inputs, by leveraging and enhancing the inherent physical properties of
vascular structures. Previous methods relying on 2D slice-based angiography
synthesis struggle with maintaining continuity in 3D vascular structures and
exhibit limited effectiveness across different imaging modalities. We propose
VasTSD, a 3D vascular tree-state space diffusion model to synthesize
angiography from 3D non-angiographic volumes, with a novel state space
serialization approach that dynamically constructs vascular tree topologies,
integrating these with a diffusion-based generative model to ensure the
generation of anatomically continuous vasculature in 3D volumes. A pre-trained
vision embedder is employed to construct vascular state space representations,
enabling consistent modeling of vascular structures across multiple modalities.
Extensive experiments on various angiographic datasets demonstrate the
superiority of VasTSD over prior works, achieving enhanced continuity of blood
vessels in synthesized angiographic synthesis for multiple modalities and
anatomical regions.
</details>

### [A Survey on Human Interaction Motion Generation](https://arxiv.org/abs/2503.12763)
*Kewei Sui,Anindita Ghosh,Inwoo Hwang,Jian Wang,Chuan Guo*
<details>
  <summary>Abstract</summary>
Humans inhabit a world defined by interactions -- with other humans, objects,
and environments. These interactive movements not only convey our relationships
with our surroundings but also demonstrate how we perceive and communicate with
the real world. Therefore, replicating these interaction behaviors in digital
systems has emerged as an important topic for applications in robotics, virtual
reality, and animation. While recent advances in deep generative models and new
datasets have accelerated progress in this field, significant challenges remain
in modeling the intricate human dynamics and their interactions with entities
in the external world. In this survey, we present, for the first time, a
comprehensive overview of the literature in human interaction motion
generation. We begin by establishing foundational concepts essential for
understanding the research background. We then systematically review existing
solutions and datasets across three primary interaction tasks -- human-human,
human-object, and human-scene interactions -- followed by evaluation metrics.
Finally, we discuss open research directions and future opportunities.
</details>

### [Decouple to Reconstruct: High Quality UHD Restoration via Active Feature Disentanglement and Reversible Fusion](https://arxiv.org/abs/2503.12764)
*Yidi Liu,Dong Li,Yuxin Ma,Jie Huang,Wenlong Zhang,Xueyang Fu,Zheng-jun Zha*
<details>
  <summary>Abstract</summary>
Ultra-high-definition (UHD) image restoration often faces computational
bottlenecks and information loss due to its extremely high resolution. Existing
studies based on Variational Autoencoders (VAE) improve efficiency by
transferring the image restoration process from pixel space to latent space.
However, degraded components are inherently coupled with background elements in
degraded images, both information loss during compression and information gain
during compensation remain uncontrollable. These lead to restored images often
exhibiting image detail loss and incomplete degradation removal. To address
this issue, we propose a Controlled Differential Disentangled VAE, which
utilizes Hierarchical Contrastive Disentanglement Learning and an Orthogonal
Gated Projection Module to guide the VAE to actively discard easily recoverable
background information while encoding more difficult-to-recover degraded
information into the latent space. Additionally, we design a Complex Invertible
Multiscale Fusion Network to handle background features, ensuring their
consistency, and utilize a latent space restoration network to transform the
degraded latent features, leading to more accurate restoration results.
Extensive experimental results demonstrate that our method effectively
alleviates the information loss problem in VAE models while ensuring
computational efficiency, significantly improving the quality of UHD image
restoration, and achieves state-of-the-art results in six UHD restoration tasks
with only 1M parameters.
</details>

### [ViSpeak: Visual Instruction Feedback in Streaming Videos](https://arxiv.org/abs/2503.12769)
*Shenghao Fu,Qize Yang,Yuan-Ming Li,Yi-Xing Peng,Kun-Yu Lin,Xihan Wei,Jian-Fang Hu,Xiaohua Xie,Wei-Shi Zheng*
<details>
  <summary>Abstract</summary>
Recent advances in Large Multi-modal Models (LMMs) are primarily focused on
offline video understanding. Instead, streaming video understanding poses great
challenges to recent models due to its time-sensitive, omni-modal and
interactive characteristics. In this work, we aim to extend the streaming video
understanding from a new perspective and propose a novel task named Visual
Instruction Feedback in which models should be aware of visual contents and
learn to extract instructions from them. For example, when users wave their
hands to agents, agents should recognize the gesture and start conversations
with welcome information. Thus, following instructions in visual modality
greatly enhances user-agent interactions. To facilitate research, we define
seven key subtasks highly relevant to visual modality and collect the
ViSpeak-Instruct dataset for training and the ViSpeak-Bench for evaluation.
Further, we propose the ViSpeak model, which is a SOTA streaming video
understanding LMM with GPT-4o-level performance on various streaming video
understanding benchmarks. After finetuning on our ViSpeak-Instruct dataset,
ViSpeak is equipped with basic visual instruction feedback ability, serving as
a solid baseline for future research.
</details>

### [NuPlanQA: A Large-Scale Dataset and Benchmark for Multi-View Driving Scene Understanding in Multi-Modal Large Language Models](https://arxiv.org/abs/2503.12772)
*Sung-Yeon Park,Can Cui,Yunsheng Ma,Ahmadreza Moradipari,Rohit Gupta,Kyungtae Han,Ziran Wang*
<details>
  <summary>Abstract</summary>
Recent advances in multi-modal large language models (MLLMs) have
demonstrated strong performance across various domains; however, their ability
to comprehend driving scenes remains less proven. The complexity of driving
scenarios, which includes multi-view information, poses significant challenges
for existing MLLMs. In this paper, we introduce NuPlanQA-Eval, a multi-view,
multi-modal evaluation benchmark for driving scene understanding. To further
support generalization to multi-view driving scenarios, we also propose
NuPlanQA-1M, a large-scale dataset comprising 1M real-world visual
question-answering (VQA) pairs. For context-aware analysis of traffic scenes,
we categorize our dataset into nine subtasks across three core skills: Road
Environment Perception, Spatial Relations Recognition, and Ego-Centric
Reasoning. Furthermore, we present BEV-LLM, integrating Bird's-Eye-View (BEV)
features from multi-view images into MLLMs. Our evaluation results reveal key
challenges that existing MLLMs face in driving scene-specific perception and
spatial reasoning from ego-centric perspectives. In contrast, BEV-LLM
demonstrates remarkable adaptability to this domain, outperforming other models
in six of the nine subtasks. These findings highlight how BEV integration
enhances multi-view MLLMs while also identifying key areas that require further
refinement for effective adaptation to driving scenes. To facilitate further
research, we publicly release NuPlanQA at
https://github.com/sungyeonparkk/NuPlanQA.
</details>

### [Adaptive Deep Learning for Multiclass Breast Cancer Classification via Misprediction Risk Analysis](https://arxiv.org/abs/2503.12778)
*Gul Sheeraz,Qun Chen,Liu Feiyu,Zhou Fengjin MD*
<details>
  <summary>Abstract</summary>
Breast cancer remains one of the leading causes of cancer-related deaths
worldwide. Early detection is crucial for improving patient outcomes, yet the
diagnostic process is often complex and prone to inconsistencies among
pathologists. Computer-aided diagnostic approaches have significantly enhanced
breast cancer detection, particularly in binary classification (benign vs.
malignant). However, these methods face challenges in multiclass
classification, leading to frequent mispredictions. In this work, we propose a
novel adaptive learning approach for multiclass breast cancer classification
using H&E-stained histopathology images. First, we introduce a misprediction
risk analysis framework that quantifies and ranks the likelihood of an image
being mislabeled by a classifier. This framework leverages an interpretable
risk model that requires only a small number of labeled samples for training.
Next, we present an adaptive learning strategy that fine-tunes classifiers
based on the specific characteristics of a given dataset. This approach
minimizes misprediction risk, allowing the classifier to adapt effectively to
the target workload. We evaluate our proposed solutions on real benchmark
datasets, demonstrating that our risk analysis framework more accurately
identifies mispredictions compared to existing methods. Furthermore, our
adaptive learning approach significantly improves the performance of
state-of-the-art deep neural network classifiers.
</details>

### [TransDiff: Diffusion-Based Method for Manipulating Transparent Objects Using a Single RGB-D Image](https://arxiv.org/abs/2503.12779)
*Haoxiao Wang,Kaichen Zhou,Binrui Gu,Zhiyuan Feng,Weijie Wang,Peilin Sun,Yicheng Xiao,Jianhua Zhang,Hao Dong*
<details>
  <summary>Abstract</summary>
Manipulating transparent objects presents significant challenges due to the
complexities introduced by their reflection and refraction properties, which
considerably hinder the accurate estimation of their 3D shapes. To address
these challenges, we propose a single-view RGB-D-based depth completion
framework, TransDiff, that leverages the Denoising Diffusion Probabilistic
Models(DDPM) to achieve material-agnostic object grasping in desktop.
Specifically, we leverage features extracted from RGB images, including
semantic segmentation, edge maps, and normal maps, to condition the depth map
generation process. Our method learns an iterative denoising process that
transforms a random depth distribution into a depth map, guided by initially
refined depth information, ensuring more accurate depth estimation in scenarios
involving transparent objects. Additionally, we propose a novel training method
to better align the noisy depth and RGB image features, which are used as
conditions to refine depth estimation step by step. Finally, we utilized an
improved inference process to accelerate the denoising procedure. Through
comprehensive experimental validation, we demonstrate that our method
significantly outperforms the baselines in both synthetic and real-world
benchmarks with acceptable inference time. The demo of our method can be found
on https://wang-haoxiao.github.io/TransDiff/
</details>

### [LangDA: Building Context-Awareness via Language for Domain Adaptive Semantic Segmentation](https://arxiv.org/abs/2503.12780)
*Chang Liu,Bavesh Balaji,Saad Hossain,C Thomas,Kwei-Herng Lai,Raviteja Vemulapalli,Alexander Wong,Sirisha Rambhatla*
<details>
  <summary>Abstract</summary>
Unsupervised domain adaptation for semantic segmentation (DASS) aims to
transfer knowledge from a label-rich source domain to a target domain with no
labels. Two key approaches in DASS are (1) vision-only approaches using masking
or multi-resolution crops, and (2) language-based approaches that use generic
class-wise prompts informed by target domain (e.g. "a {snowy} photo of a
{class}"). However, the former is susceptible to noisy pseudo-labels that are
biased to the source domain. The latter does not fully capture the intricate
spatial relationships of objects -- key for dense prediction tasks. To this
end, we propose LangDA. LangDA addresses these challenges by, first, learning
contextual relationships between objects via VLM-generated scene descriptions
(e.g. "a pedestrian is on the sidewalk, and the street is lined with
buildings."). Second, LangDA aligns the entire image features with text
representation of this context-aware scene caption and learns generalized
representations via text. With this, LangDA sets the new state-of-the-art
across three DASS benchmarks, outperforming existing methods by 2.6%, 1.4% and
3.9%.
</details>

### [SAM2 for Image and Video Segmentation: A Comprehensive Survey](https://arxiv.org/abs/2503.12781)
*Zhang Jiaxing,Tang Hao*
<details>
  <summary>Abstract</summary>
Despite significant advances in deep learning for image and video
segmentation, existing models continue to face challenges in cross-domain
adaptability and generalization. Image and video segmentation are fundamental
tasks in computer vision with wide-ranging applications in healthcare,
agriculture, industrial inspection, and autonomous driving. With the advent of
large-scale foundation models, SAM2 - an improved version of SAM (Segment
Anything Model)has been optimized for segmentation tasks, demonstrating
enhanced performance in complex scenarios. However, SAM2's adaptability and
limitations in specific domains require further investigation. This paper
systematically analyzes the application of SAM2 in image and video segmentation
and evaluates its performance in various fields. We begin by introducing the
foundational concepts of image segmentation, categorizing foundation models,
and exploring the technical characteristics of SAM and SAM2. Subsequently, we
delve into SAM2's applications in static image and video segmentation,
emphasizing its performance in specialized areas such as medical imaging and
the challenges of cross-domain adaptability. As part of our research, we
reviewed over 200 related papers to provide a comprehensive analysis of the
topic. Finally, the paper highlights the strengths and weaknesses of SAM2 in
segmentation tasks, identifies the technical challenges it faces, and proposes
future development directions. This review provides valuable insights and
practical recommendations for optimizing and applying SAM2 in real-world
scenarios.
</details>

### [Mixed-granularity Implicit Representation for Continuous Hyperspectral Compressive Reconstruction](https://arxiv.org/abs/2503.12783)
*Jianan Li,Huan Chen,Wangcai Zhao,Rui Chen,Tingfa Xu*
<details>
  <summary>Abstract</summary>
Hyperspectral Images (HSIs) are crucial across numerous fields but are
hindered by the long acquisition times associated with traditional
spectrometers. The Coded Aperture Snapshot Spectral Imaging (CASSI) system
mitigates this issue through a compression technique that accelerates the
acquisition process. However, reconstructing HSIs from compressed data presents
challenges due to fixed spatial and spectral resolution constraints. This study
introduces a novel method using implicit neural representation for continuous
hyperspectral image reconstruction. We propose the Mixed Granularity Implicit
Representation (MGIR) framework, which includes a Hierarchical Spectral-Spatial
Implicit Encoder for efficient multi-scale implicit feature extraction. This is
complemented by a Mixed-Granularity Local Feature Aggregator that adaptively
integrates local features across scales, combined with a decoder that merges
coordinate information for precise reconstruction. By leveraging implicit
neural representations, the MGIR framework enables reconstruction at any
desired spatial-spectral resolution, significantly enhancing the flexibility
and adaptability of the CASSI system. Extensive experimental evaluations
confirm that our model produces reconstructed images at arbitrary resolutions
and matches state-of-the-art methods across varying spectral-spatial
compression ratios. The code will be released at https://github.com/chh11/MGIR.
</details>

### [Privacy-Preserving Biometric Verification with Handwritten Random Digit String](https://arxiv.org/abs/2503.12786)
*Peirong Zhang,Yuliang Liu,Songxuan Lai,Hongliang Li,Lianwen Jin*
<details>
  <summary>Abstract</summary>
Handwriting verification has stood as a steadfast identity authentication
method for decades. However, this technique risks potential privacy breaches
due to the inclusion of personal information in handwritten biometrics such as
signatures. To address this concern, we propose using the Random Digit String
(RDS) for privacy-preserving handwriting verification. This approach allows
users to authenticate themselves by writing an arbitrary digit sequence,
effectively ensuring privacy protection. To evaluate the effectiveness of RDS,
we construct a new HRDS4BV dataset composed of online naturally handwritten
RDS. Unlike conventional handwriting, RDS encompasses unconstrained and
variable content, posing significant challenges for modeling consistent
personal writing style. To surmount this, we propose the Pattern Attentive
VErification Network (PAVENet), along with a Discriminative Pattern Mining
(DPM) module. DPM adaptively enhances the recognition of consistent and
discriminative writing patterns, thus refining handwriting style
representation. Through comprehensive evaluations, we scrutinize the
applicability of online RDS verification and showcase a pronounced
outperformance of our model over existing methods. Furthermore, we discover a
noteworthy forgery phenomenon that deviates from prior findings and discuss its
positive impact in countering malicious impostor attacks. Substantially, our
work underscores the feasibility of privacy-preserving biometric verification
and propels the prospects of its broader acceptance and application.
</details>

### [DeepPerception: Advancing R1-like Cognitive Visual Perception in MLLMs for Knowledge-Intensive Visual Grounding](https://arxiv.org/abs/2503.12797)
*Xinyu Ma,Ziyang Ding,Zhicong Luo,Chi Chen,Zonghao Guo,Derek F. Wong,Xiaoyi Feng,Maosong Sun*
<details>
  <summary>Abstract</summary>
Human experts excel at fine-grained visual discrimination by leveraging
domain knowledge to refine perceptual features, a capability that remains
underdeveloped in current Multimodal Large Language Models (MLLMs). Despite
possessing vast expert-level knowledge, MLLMs struggle to integrate reasoning
into visual perception, often generating direct responses without deeper
analysis. To bridge this gap, we introduce knowledge-intensive visual grounding
(KVG), a novel visual grounding task that requires both fine-grained perception
and domain-specific knowledge integration. To address the challenges of KVG, we
propose DeepPerception, an MLLM enhanced with cognitive visual perception
capabilities. Our approach consists of (1) an automated data synthesis pipeline
that generates high-quality, knowledge-aligned training samples, and (2) a
two-stage training framework combining supervised fine-tuning for cognitive
reasoning scaffolding and reinforcement learning to optimize
perception-cognition synergy. To benchmark performance, we introduce KVG-Bench
a comprehensive dataset spanning 10 domains with 1.3K manually curated test
cases. Experimental results demonstrate that DeepPerception significantly
outperforms direct fine-tuning, achieving +8.08\% accuracy improvements on
KVG-Bench and exhibiting +4.60\% superior cross-domain generalization over
baseline approaches. Our findings highlight the importance of integrating
cognitive processes into MLLMs for human-like visual perception and open new
directions for multimodal reasoning research. The data, codes, and models are
released at https://github.com/thunlp/DeepPerception.
</details>

### [Grounded Chain-of-Thought for Multimodal Large Language Models](https://arxiv.org/abs/2503.12799)
*Qiong Wu,Xiangcong Yang,Yiyi Zhou,Chenxin Fang,Baiyang Song,Xiaoshuai Sun,Rongrong Ji*
<details>
  <summary>Abstract</summary>
Despite great progress, existing multimodal large language models (MLLMs) are
prone to visual hallucination, greatly impeding their trustworthy applications.
In this paper, we study this problem from the perspective of visual-spatial
reasoning, and propose a new learning task for MLLMs, termed Grounded
Chain-of-Thought (GCoT). Different from recent visual CoT studies, which focus
more on visual knowledge reasoning, GCoT is keen to helping MLLMs to recognize
and ground the relevant visual cues step by step, thereby predicting the
correct answer with grounding coordinates as the intuitive basis. To facilitate
this task, we also carefully design and construct a dataset called multimodal
grounded chain-of-thought (MM-GCoT) consisting of 24,022 GCoT examples for
5,033 images. Besides, a comprehensive consistency evaluation system is also
introduced, including the metrics of answer accuracy, grounding accuracy and
answer-grounding consistency. We further design and conduct a bunch of
experiments on 12 advanced MLLMs, and reveal some notable findings: i. most
MLLMs performs poorly on the consistency evaluation, indicating obvious visual
hallucination; ii. visual hallucination is not directly related to the
parameter size and general multimodal performance, i.e., a larger and stronger
MLLM is not less affected by this issue. Lastly, we also demonstrate that the
proposed dataset can help existing MLLMs to well cultivate their GCoT
capability and reduce the inconsistent answering significantly. Moreover, their
GCoT can be also generalized to exiting multimodal tasks, such as open-world QA
and REC.
</details>

### [Pairwise Similarity Regularization for Semi-supervised Graph Medical Image Segmentation](https://arxiv.org/abs/2503.12800)
*Jialu Zhou,Dianxi Shi,Shaowu Yang,Chunping Qiu,Luoxi Jing,Mengzhu Wang*
<details>
  <summary>Abstract</summary>
With fully leveraging the value of unlabeled data, semi-supervised medical
image segmentation algorithms significantly reduces the limitation of limited
labeled data, achieving a significant improvement in accuracy. However, the
distributional shift between labeled and unlabeled data weakens the utilization
of information from the labeled data. To alleviate the problem, we propose a
graph network feature alignment method based on pairwise similarity
regularization (PaSR) for semi-supervised medical image segmentation. PaSR
aligns the graph structure of images in different domains by maintaining
consistency in the pairwise structural similarity of feature graphs between the
target domain and the source domain, reducing distribution shift issues in
medical images. Meanwhile, further improving the accuracy of pseudo-labels in
the teacher network by aligning graph clustering information to enhance the
semi-supervised efficiency of the model. The experimental part was verified on
three medical image segmentation benchmark datasets, with results showing
improvements over advanced methods in various metrics. On the ACDC dataset, it
achieved an average improvement of more than 10.66%.
</details>

### [Hydra-MDP++: Advancing End-to-End Driving via Expert-Guided Hydra-Distillation](https://arxiv.org/abs/2503.12820)
*Kailin Li,Zhenxin Li,Shiyi Lan,Yuan Xie,Zhizhong Zhang,Jiayi Liu,Zuxuan Wu,Zhiding Yu,Jose M. Alvarez*
<details>
  <summary>Abstract</summary>
Hydra-MDP++ introduces a novel teacher-student knowledge distillation
framework with a multi-head decoder that learns from human demonstrations and
rule-based experts. Using a lightweight ResNet-34 network without complex
components, the framework incorporates expanded evaluation metrics, including
traffic light compliance (TL), lane-keeping ability (LK), and extended comfort
(EC) to address unsafe behaviors not captured by traditional NAVSIM-derived
teachers. Like other end-to-end autonomous driving approaches, \hydra processes
raw images directly without relying on privileged perception signals.
Hydra-MDP++ achieves state-of-the-art performance by integrating these
components with a 91.0% drive score on NAVSIM through scaling to a V2-99 image
encoder, demonstrating its effectiveness in handling diverse driving scenarios
while maintaining computational efficiency.
</details>

### [From Head to Tail: Towards Balanced Representation in Large Vision-Language Models through Adaptive Data Calibration](https://arxiv.org/abs/2503.12821)
*Mingyang Song,Xiaoye Qu,Jiawei Zhou,Yu Cheng*
<details>
  <summary>Abstract</summary>
Large Vision-Language Models (LVLMs) have achieved significant progress in
combining visual comprehension with language generation. Despite this success,
the training data of LVLMs still suffers from Long-Tail (LT) problems, where
the data distribution is highly imbalanced. Previous works have mainly focused
on traditional VLM architectures, i.e., CLIP or ViT, and specific tasks such as
recognition and classification. Nevertheless, the exploration of LVLM (e.g.
LLaVA) and more general tasks (e.g. Visual Question Answering and Visual
Reasoning) remains under-explored. In this paper, we first conduct an in-depth
analysis of the LT issues in LVLMs and identify two core causes: the
overrepresentation of head concepts and the underrepresentation of tail
concepts. Based on the above observation, we propose an $\textbf{A}$daptive
$\textbf{D}$ata $\textbf{R}$efinement Framework ($\textbf{ADR}$), which
consists of two stages: $\textbf{D}$ata $\textbf{R}$ebalancing ($\textbf{DR}$)
and $\textbf{D}$ata $\textbf{S}$ynthesis ($\textbf{DS}$). In the DR stage, we
adaptively rebalance the redundant data based on entity distributions, while in
the DS stage, we leverage Denoising Diffusion Probabilistic Models (DDPMs) and
scarce images to supplement underrepresented portions. Through comprehensive
evaluations across eleven benchmarks, our proposed ADR effectively mitigates
the long-tail problem in the training data, improving the average performance
of LLaVA 1.5 relatively by 4.36%, without increasing the training data volume.
</details>

### [GSBAK$^K$: $top$-$K$ Geometric Score-based Black-box Attack](https://arxiv.org/abs/2503.12827)
*Md Farhamdur Reza,Richeng Jin,Tianfu Wu,Huaiyu Dai*
<details>
  <summary>Abstract</summary>
Existing score-based adversarial attacks mainly focus on crafting $top$-1
adversarial examples against classifiers with single-label classification.
Their attack success rate and query efficiency are often less than
satisfactory, particularly under small perturbation requirements; moreover, the
vulnerability of classifiers with multi-label learning is yet to be studied. In
this paper, we propose a comprehensive surrogate free score-based attack, named
\b geometric \b score-based \b black-box \b attack (GSBAK$^K$), to craft
adversarial examples in an aggressive $top$-$K$ setting for both untargeted and
targeted attacks, where the goal is to change the $top$-$K$ predictions of the
target classifier. We introduce novel gradient-based methods to find a good
initial boundary point to attack. Our iterative method employs novel gradient
estimation techniques, particularly effective in $top$-$K$ setting, on the
decision boundary to effectively exploit the geometry of the decision boundary.
Additionally, GSBAK$^K$ can be used to attack against classifiers with
$top$-$K$ multi-label learning. Extensive experimental results on ImageNet and
PASCAL VOC datasets validate the effectiveness of GSBAK$^K$ in crafting
$top$-$K$ adversarial examples.
</details>

### [PASTA: Part-Aware Sketch-to-3D Shape Generation with Text-Aligned Prior](https://arxiv.org/abs/2503.12834)
*Seunggwan Lee,Hwanhee Jung,Byoungsoo Koh,Qixing Huang,Sangho Yoon,Sangpil Kim*
<details>
  <summary>Abstract</summary>
A fundamental challenge in conditional 3D shape generation is to minimize the
information loss and maximize the intention of user input. Existing approaches
have predominantly focused on two types of isolated conditional signals, i.e.,
user sketches and text descriptions, each of which does not offer flexible
control of the generated shape. In this paper, we introduce PASTA, the flexible
approach that seamlessly integrates a user sketch and a text description for 3D
shape generation. The key idea is to use text embeddings from a vision-language
model to enrich the semantic representation of sketches. Specifically, these
text-derived priors specify the part components of the object, compensating for
missing visual cues from ambiguous sketches. In addition, we introduce ISG-Net
which employs two types of graph convolutional networks: IndivGCN, which
processes fine-grained details, and PartGCN, which aggregates these details
into parts and refines the structure of objects. Extensive experiments
demonstrate that PASTA outperforms existing methods in part-level editing and
achieves state-of-the-art results in sketch-to-3D shape generation.
</details>

### [CompMarkGS: Robust Watermarking for Compression 3D Gaussian Splatting](https://arxiv.org/abs/2503.12836)
*Sumin In,Youngdong Jang,Utae Jeong,MinHyuk Jang,Hyeongcheol Park,Eunbyung Park,Sangpil Kim*
<details>
  <summary>Abstract</summary>
3D Gaussian Splatting (3DGS) enables rapid differentiable rendering for 3D
reconstruction and novel view synthesis, leading to its widespread commercial
use. Consequently, copyright protection via watermarking has become critical.
However, because 3DGS relies on millions of Gaussians, which require gigabytes
of storage, efficient transfer and storage require compression. Existing 3DGS
watermarking methods are vulnerable to quantization-based compression, often
resulting in the loss of the embedded watermark. To address this challenge, we
propose a novel watermarking method that ensures watermark robustness after
model compression while maintaining high rendering quality. In detail, we
incorporate a quantization distortion layer that simulates compression during
training, preserving the watermark under quantization-based compression. Also,
we propose a learnable watermark embedding feature that embeds the watermark
into the anchor feature, ensuring structural consistency and seamless
integration into the 3D scene. Furthermore, we present a frequency-aware anchor
growing mechanism to enhance image quality in high-frequency regions by
effectively identifying Guassians within these regions. Experimental results
confirm that our method preserves the watermark and maintains superior image
quality under high compression, validating it as a promising approach for a
secure 3DGS model.
</details>

### [DreamLayer: Simultaneous Multi-Layer Generation via Diffusion Mode](https://arxiv.org/abs/2503.12838)
*Junjia Huang,Pengxiang Yan,Jinhang Cai,Jiyang Liu,Zhao Wang,Yitong Wang,Xinglong Wu,Guanbin Li*
<details>
  <summary>Abstract</summary>
Text-driven image generation using diffusion models has recently gained
significant attention. To enable more flexible image manipulation and editing,
recent research has expanded from single image generation to transparent layer
generation and multi-layer compositions. However, existing approaches often
fail to provide a thorough exploration of multi-layer structures, leading to
inconsistent inter-layer interactions, such as occlusion relationships, spatial
layout, and shadowing. In this paper, we introduce DreamLayer, a novel
framework that enables coherent text-driven generation of multiple image
layers, by explicitly modeling the relationship between transparent foreground
and background layers. DreamLayer incorporates three key components, i.e.,
Context-Aware Cross-Attention (CACA) for global-local information exchange,
Layer-Shared Self-Attention (LSSA) for establishing robust inter-layer
connections, and Information Retained Harmonization (IRH) for refining fusion
details at the latent level. By leveraging a coherent full-image context,
DreamLayer builds inter-layer connections through attention mechanisms and
applies a harmonization step to achieve seamless layer fusion. To facilitate
research in multi-layer generation, we construct a high-quality, diverse
multi-layer dataset including 400k samples. Extensive experiments and user
studies demonstrate that DreamLayer generates more coherent and well-aligned
layers, with broad applicability, including latent-space image editing and
image-to-layer decomposition.
</details>

### [Towards Scalable Foundation Model for Multi-modal and Hyperspectral Geospatial Data](https://arxiv.org/abs/2503.12843)
*Haozhe Si,Yuxuan Wan,Minh Do,Deepak Vasisht,Han Zhao,Hendrik F. Hamann*
<details>
  <summary>Abstract</summary>
Geospatial raster (imagery) data, such as that collected by satellite-based
imaging systems at different times and spectral bands, hold immense potential
for enabling a wide range of high-impact applications. This potential stems
from the rich information that is spatially and temporally contextualized
across multiple channels and sensing modalities. Recent work has adapted
existing self-supervised learning approaches for such geospatial data. However,
they fall short of scalable model architectures, leading to inflexibility and
computational inefficiencies when faced with an increasing number of channels
and modalities. To address these limitations, we introduce Low-rank Efficient
Spatial-Spectral Vision Transformer (LESS ViT) with three key innovations: i)
the LESS Attention Block that approximates high-dimensional spatial-spectral
attention through Kronecker's product of the low-dimensional spatial and
spectral attention components; ii) the Continuous Positional-Channel Embedding
Layer that preserves both spatial and spectral continuity and physical
characteristics of each patch; and iii) the Perception Field Mask that exploits
local spatial dependencies by constraining attention to neighboring patches. To
evaluate the proposed innovations, we construct a benchmark, GFM-Bench, which
serves as a comprehensive benchmark for such geospatial raster data. We
pretrain LESS ViT using a Hyperspectral Masked Autoencoder framework with
integrated positional and channel masking strategies. Experimental results
demonstrate that our proposed method surpasses current state-of-the-art
multi-modal geospatial foundation models, achieving superior performance with
less computation and fewer parameters. The flexibility and extensibility of our
framework make it a promising direction for future geospatial data analysis
tasks that involve a wide range of modalities and channels.
</details>

### [GuideDog: A Real-World Egocentric Multimodal Dataset for Blind and Low-Vision Accessibility-Aware Guidance](https://arxiv.org/abs/2503.12844)
*Junhyeok Kim,Jaewoo Park,Junhee Park,Sangeyl Lee,Jiwan Chung,Jisung Kim,Ji Hoon Joung,Youngjae Yu*
<details>
  <summary>Abstract</summary>
Mobility remains a significant challenge for the 2.2 billion people worldwide
affected by blindness and low vision (BLV), with 7% of visually impaired
individuals experiencing falls at least once a month. While recent advances in
Multimodal Large Language Models (MLLMs) offer promising opportunities for BLV
assistance, their development has been hindered by limited datasets. This
limitation stems from the fact that BLV-aware annotation requires specialized
domain knowledge and intensive labor. To address this gap, we introduce
GuideDog, a novel accessibility-aware guide dataset containing 22K
image-description pairs (including 2K human-annotated pairs) that capture
diverse real-world scenes from a pedestrian's viewpoint. Our approach shifts
the annotation burden from generation to verification through a collaborative
human-AI framework grounded in established accessibility standards,
significantly improving efficiency while maintaining high-quality annotations.
We also develop GuideDogQA, a subset of 818 samples featuring multiple-choice
questions designed to evaluate fine-grained visual perception capabilities,
specifically object recognition and relative depth perception. Our experimental
results highlight the importance of accurate spatial understanding for
effective BLV guidance. GuideDog and GuideDogQA will advance research in
MLLM-based assistive technologies for BLV individuals while contributing to
broader applications in understanding egocentric scenes for robotics and
augmented reality. The code and dataset will be publicly available.
</details>

### [ACT360: An Efficient 360-Degree Action Detection and Summarization Framework for Mission-Critical Training and Debriefing](https://arxiv.org/abs/2503.12852)
*Aditi Tiwari,Klara Nahrstedt*
<details>
  <summary>Abstract</summary>
Effective training and debriefing are critical in high-stakes,
mission-critical environments such as disaster response, military simulations,
and industrial safety, where precision and minimizing errors are paramount. The
traditional post-training analysis relies on manually reviewing 2D videos, a
time-consuming process that lacks comprehensive situational awareness. To
address these limitations, we introduce ACT360, a system that leverages
360-degree videos and machine learning for automated action detection and
structured debriefing. ACT360 integrates 360YOWO, an enhanced You Only Watch
Once (YOWO) model with spatial attention and equirectangular-aware convolution
(EAC) to mitigate panoramic video distortions. To enable deployment in
resource-constrained environments, we apply quantization and model pruning,
reducing the model size by 74% while maintaining robust accuracy (mAP drop of
only 1.5%, from 0.865 to 0.850) and improving inference speed. We validate our
approach on a publicly available dataset of 55 labeled 360-degree videos
covering seven key operational actions, recorded across various real-world
training sessions and environmental conditions. Additionally, ACT360 integrates
360AIE (Action Insight Explorer), a web-based interface for automatic action
detection, retrieval, and textual summarization using large language models
(LLMs), significantly enhancing post-incident analysis efficiency. ACT360
serves as a generalized framework for mission-critical debriefing,
incorporating EAC, spatial attention, summarization, and model optimization.
These innovations apply to any training environment requiring lightweight
action detection and structured post-exercise analysis.
</details>

### [Adaptive Transformer Attention and Multi-Scale Fusion for Spine 3D Segmentation](https://arxiv.org/abs/2503.12853)
*Yanlin Xiang,Qingyuan He,Ting Xu,Ran Hao,Jiacheng Hu,Hanchao Zhang*
<details>
  <summary>Abstract</summary>
This study proposes a 3D semantic segmentation method for the spine based on
the improved SwinUNETR to improve segmentation accuracy and robustness. Aiming
at the complex anatomical structure of spinal images, this paper introduces a
multi-scale fusion mechanism to enhance the feature extraction capability by
using information of different scales, thereby improving the recognition
accuracy of the model for the target area. In addition, the introduction of the
adaptive attention mechanism enables the model to dynamically adjust the
attention to the key area, thereby optimizing the boundary segmentation effect.
The experimental results show that compared with 3D CNN, 3D U-Net, and 3D U-Net
+ Transformer, the model of this study has achieved significant improvements in
mIoU, mDice, and mAcc indicators, and has better segmentation performance. The
ablation experiment further verifies the effectiveness of the proposed improved
method, proving that multi-scale fusion and adaptive attention mechanism have a
positive effect on the segmentation task. Through the visualization analysis of
the inference results, the model can better restore the real anatomical
structure of the spinal image. Future research can further optimize the
Transformer structure and expand the data scale to improve the generalization
ability of the model. This study provides an efficient solution for the task of
medical image segmentation, which is of great significance to intelligent
medical image analysis.
</details>

### [VITED: Video Temporal Evidence Distillation](https://arxiv.org/abs/2503.12855)
*Yujie Lu,Yale Song,William Wang,Lorenzo Torresani,Tushar Nagarajan*
<details>
  <summary>Abstract</summary>
We investigate complex video question answering via chain-of-evidence
reasoning -- identifying sequences of temporal spans from multiple relevant
parts of the video, together with visual evidence within them. Existing models
struggle with multi-step reasoning as they uniformly sample a fixed number of
frames, which can miss critical evidence distributed nonuniformly throughout
the video. Moreover, they lack the ability to temporally localize such evidence
in the broader context of the full video, which is required for answering
complex questions. We propose a framework to enhance existing VideoQA datasets
with evidence reasoning chains, automatically constructed by searching for
optimal intervals of interest in the video with supporting evidence, that
maximizes the likelihood of answering a given question. We train our model
(VITED) to generate these evidence chains directly, enabling it to both
localize evidence windows as well as perform multi-step reasoning across them
in long-form video content. We show the value of our evidence-distilled models
on a suite of long video QA benchmarks where we outperform state-of-the-art
approaches that lack evidence reasoning capabilities.
</details>

### [CAT-3DGS Pro: A New Benchmark for Efficient 3DGS Compression](https://arxiv.org/abs/2503.12862)
*Yu-Ting Zhan,He-bi Yang,Cheng-Yuan Ho,Jui-Chiu Chiang,Wen-Hsiao Peng*
<details>
  <summary>Abstract</summary>
3D Gaussian Splatting (3DGS) has shown immense potential for novel view
synthesis. However, achieving rate-distortion-optimized compression of 3DGS
representations for transmission and/or storage applications remains a
challenge. CAT-3DGS introduces a context-adaptive triplane hyperprior for
end-to-end optimized compression, delivering state-of-the-art coding
performance. Despite this, it requires prolonged training and decoding time. To
address these limitations, we propose CAT-3DGS Pro, an enhanced version of
CAT-3DGS that improves both compression performance and computational
efficiency. First, we introduce a PCA-guided vector-matrix hyperprior, which
replaces the triplane-based hyperprior to reduce redundant parameters. To
achieve a more balanced rate-distortion trade-off and faster encoding, we
propose an alternate optimization strategy (A-RDO). Additionally, we refine the
sampling rate optimization method in CAT-3DGS, leading to significant
improvements in rate-distortion performance. These enhancements result in a
46.6% BD-rate reduction and 3x speedup in training time on BungeeNeRF, while
achieving 5x acceleration in decoding speed for the Amsterdam scene compared to
CAT-3DGS.
</details>

### [SCAP: Transductive Test-Time Adaptation via Supportive Clique-based Attribute Prompting](https://arxiv.org/abs/2503.12866)
*Chenyu Zhang,Kunlun Xu,Zichen Liu,Yuxin Peng,Jiahuan Zhou*
<details>
  <summary>Abstract</summary>
Vision-language models (VLMs) encounter considerable challenges when adapting
to domain shifts stemming from changes in data distribution. Test-time
adaptation (TTA) has emerged as a promising approach to enhance VLM performance
under such conditions. In practice, test data often arrives in batches, leading
to increasing interest in the transductive TTA setting. However, existing TTA
methods primarily focus on individual test samples, overlooking crucial
cross-sample correlations within a batch. While recent ViT-based TTA methods
have introduced batch-level adaptation, they remain suboptimal for VLMs due to
inadequate integration of the text modality. To address these limitations, we
propose a novel transductive TTA framework, Supportive Clique-based Attribute
Prompting (SCAP), which effectively combines visual and textual information to
enhance adaptation by generating fine-grained attribute prompts across test
batches. SCAP first forms supportive cliques of test samples in an unsupervised
manner based on visual similarity and learns an attribute prompt for each
clique, capturing shared attributes critical for adaptation. For each test
sample, SCAP aggregates attribute prompts from its associated cliques,
providing enriched contextual information. To ensure adaptability over time, we
incorporate a retention module that dynamically updates attribute prompts and
their associated attributes as new data arrives. Comprehensive experiments
across multiple benchmarks demonstrate that SCAP outperforms existing
state-of-the-art methods, significantly advancing VLM generalization under
domain shifts. Our code is available at
https://github.com/zhoujiahuan1991/CVPR2025-SCAP.
</details>

### [UniReg: Foundation Model for Controllable Medical Image Registration](https://arxiv.org/abs/2503.12868)
*Zi Li,Jianpeng Zhang,Tai Ma,Tony C. W. Mok,Yan-Jie Zhou,Zeli Chen,Xianghua Ye,Le Lu,Dakai Jin*
<details>
  <summary>Abstract</summary>
Learning-based medical image registration has achieved performance parity
with conventional methods while demonstrating a substantial advantage in
computational efficiency. However, learning-based registration approaches lack
generalizability across diverse clinical scenarios, requiring the laborious
development of multiple isolated networks for specific registration tasks,
e.g., inter-/intra-subject registration or organ-specific alignment. % To
overcome this limitation, we propose \textbf{UniReg}, the first interactive
foundation model for medical image registration, which combines the precision
advantages of task-specific learning methods with the generalization of
traditional optimization methods. Our key innovation is a unified framework for
diverse registration scenarios, achieved through a conditional deformation
field estimation within a unified registration model. This is realized through
a dynamic learning paradigm that explicitly encodes: (1) anatomical structure
priors, (2) registration type constraints (inter/intra-subject), and (3)
instance-specific features, enabling the generation of scenario-optimal
deformation fields. % Through comprehensive experiments encompassing $90$
anatomical structures at different body regions, our UniReg model demonstrates
comparable performance with contemporary state-of-the-art methodologies while
achieving ~50\% reduction in required training iterations relative to the
conventional learning-based paradigm. This optimization contributes to a
significant reduction in computational resources, such as training time. Code
and model will be available.
</details>

### [Evolution-based Region Adversarial Prompt Learning for Robustness Enhancement in Vision-Language Models](https://arxiv.org/abs/2503.12874)
*Xiaojun Jia,Sensen Gao,Simeng Qin,Ke Ma,Xinfeng Li,Yihao Huang,Wei Dong,Yang Liu,Xiaochun Cao*
<details>
  <summary>Abstract</summary>
Large pre-trained vision-language models (VLMs), such as CLIP, demonstrate
impressive generalization but remain highly vulnerable to adversarial examples
(AEs). Previous work has explored robust text prompts through adversarial
training, achieving some improvement in both robustness and generalization.
However, they primarily rely on singlegradient direction perturbations (e.g.,
PGD) to generate AEs, which lack diversity, resulting in limited improvement in
adversarial robustness. To address these limitations, we propose an
evolution-based region adversarial prompt tuning method called ER-APT, which
combines gradient methods with genetic evolution to generate more diverse and
challenging AEs. In each training iteration, we first generate AEs using
traditional gradient-based methods. Subsequently, a genetic evolution mechanism
incorporating selection, mutation, and crossover is applied to optimize the
AEs, ensuring a broader and more aggressive perturbation distribution.The final
evolved AEs are used for prompt tuning, achieving region-based adversarial
optimization instead of conventional single-point adversarial prompt tuning. We
also propose a dynamic loss weighting method to adjust prompt learning
efficiency for accuracy and robustness. Experimental evaluations on various
benchmark datasets demonstrate the superiority of our proposed method,
outperforming stateof-the-art APT methods. The code is released at
https://github.com/jiaxiaojunQAQ/ER-APT.
</details>

### [An interpretable approach to automating the assessment of biofouling in video footage](https://arxiv.org/abs/2503.12875)
*Evelyn J. Mannix,Bartholomew A. Woodham*
<details>
  <summary>Abstract</summary>
Biofouling$\unicode{x2013}$communities of organisms that grow on hard
surfaces immersed in water$\unicode{x2013}$provides a pathway for the spread of
invasive marine species and diseases. To address this risk, international
vessels are increasingly being obligated to provide evidence of their
biofouling management practices. Verification that these activities are
effective requires underwater inspections, using divers or underwater remotely
operated vehicles (ROVs), and the collection and analysis of large amounts of
imagery and footage. Automated assessment using computer vision techniques can
significantly streamline this process, and this work shows how this challenge
can be addressed efficiently and effectively using the interpretable Component
Features (ComFe) approach with a DINOv2 Vision Transformer (ViT) foundation
model. ComFe is able to obtain improved performance in comparison to previous
non-interpretable Convolutional Neural Network (CNN) methods, with
significantly fewer weights and greater transparency$\unicode{x2013}$through
identifying which regions of the image contribute to the classification, and
which images in the training data lead to that conclusion. All code, data and
model weights are publicly released.
</details>

### [DreamRenderer: Taming Multi-Instance Attribute Control in Large-Scale Text-to-Image Models](https://arxiv.org/abs/2503.12885)
*Dewei Zhou,Mingwei Li,Zongxin Yang,Yi Yang*
<details>
  <summary>Abstract</summary>
Image-conditioned generation methods, such as depth- and canny-conditioned
approaches, have demonstrated remarkable abilities for precise image synthesis.
However, existing models still struggle to accurately control the content of
multiple instances (or regions). Even state-of-the-art models like FLUX and
3DIS face challenges, such as attribute leakage between instances, which limits
user control. To address these issues, we introduce DreamRenderer, a
training-free approach built upon the FLUX model. DreamRenderer enables users
to control the content of each instance via bounding boxes or masks, while
ensuring overall visual harmony. We propose two key innovations: 1) Bridge
Image Tokens for Hard Text Attribute Binding, which uses replicated image
tokens as bridge tokens to ensure that T5 text embeddings, pre-trained solely
on text data, bind the correct visual attributes for each instance during Joint
Attention; 2) Hard Image Attribute Binding applied only to vital layers.
Through our analysis of FLUX, we identify the critical layers responsible for
instance attribute rendering and apply Hard Image Attribute Binding only in
these layers, using soft binding in the others. This approach ensures precise
control while preserving image quality. Evaluations on the COCO-POS and
COCO-MIG benchmarks demonstrate that DreamRenderer improves the Image Success
Ratio by 17.7% over FLUX and enhances the performance of layout-to-image models
like GLIGEN and 3DIS by up to 26.8%. Project Page:
https://limuloo.github.io/DreamRenderer/.
</details>

### [RGBAvatar: Reduced Gaussian Blendshapes for Online Modeling of Head Avatars](https://arxiv.org/abs/2503.12886)
*Linzhou Li,Yumeng Li,Yanlin Weng,Youyi Zheng,Kun Zhou*
<details>
  <summary>Abstract</summary>
We present Reduced Gaussian Blendshapes Avatar (RGBAvatar), a method for
reconstructing photorealistic, animatable head avatars at speeds sufficient for
on-the-fly reconstruction. Unlike prior approaches that utilize linear bases
from 3D morphable models (3DMM) to model Gaussian blendshapes, our method maps
tracked 3DMM parameters into reduced blendshape weights with an MLP, leading to
a compact set of blendshape bases. The learned compact base composition
effectively captures essential facial details for specific individuals, and
does not rely on the fixed base composition weights of 3DMM, leading to
enhanced reconstruction quality and higher efficiency. To further expedite the
reconstruction process, we develop a novel color initialization estimation
method and a batch-parallel Gaussian rasterization process, achieving
state-of-the-art quality with training throughput of about 630 images per
second. Moreover, we propose a local-global sampling strategy that enables
direct on-the-fly reconstruction, immediately reconstructing the model as video
streams in real time while achieving quality comparable to offline settings.
Our source code is available at https://github.com/gapszju/RGBAvatar.
</details>

### [UncTrack: Reliable Visual Object Tracking with Uncertainty-Aware Prototype Memory Network](https://arxiv.org/abs/2503.12888)
*Siyuan Yao,Yang Guo,Yanyang Yan,Wenqi Ren,Xiaochun Cao*
<details>
  <summary>Abstract</summary>
Transformer-based trackers have achieved promising success and become the
dominant tracking paradigm due to their accuracy and efficiency. Despite the
substantial progress, most of the existing approaches tackle object tracking as
a deterministic coordinate regression problem, while the target localization
uncertainty has been greatly overlooked, which hampers trackers' ability to
maintain reliable target state prediction in challenging scenarios. To address
this issue, we propose UncTrack, a novel uncertainty-aware transformer tracker
that predicts the target localization uncertainty and incorporates this
uncertainty information for accurate target state inference. Specifically,
UncTrack utilizes a transformer encoder to perform feature interaction between
template and search images. The output features are passed into an
uncertainty-aware localization decoder (ULD) to coarsely predict the
corner-based localization and the corresponding localization uncertainty. Then
the localization uncertainty is sent into a prototype memory network (PMN) to
excavate valuable historical information to identify whether the target state
prediction is reliable or not. To enhance the template representation, the
samples with high confidence are fed back into the prototype memory bank for
memory updating, making the tracker more robust to challenging appearance
variations. Extensive experiments demonstrate that our method outperforms other
state-of-the-art methods. Our code is available at
https://github.com/ManOfStory/UncTrack.
</details>

### [UCF-Crime-DVS: A Novel Event-Based Dataset for Video Anomaly Detection with Spiking Neural Networks](https://arxiv.org/abs/2503.12905)
*Yuanbin Qian,Shuhan Ye,Chong Wang,Xiaojie Cai,Jiangbo Qian,Jiafei Wu*
<details>
  <summary>Abstract</summary>
Video anomaly detection plays a significant role in intelligent surveillance
systems. To enhance model's anomaly recognition ability, previous works have
typically involved RGB, optical flow, and text features. Recently, dynamic
vision sensors (DVS) have emerged as a promising technology, which capture
visual information as discrete events with a very high dynamic range and
temporal resolution. It reduces data redundancy and enhances the capture
capacity of moving objects compared to conventional camera. To introduce this
rich dynamic information into the surveillance field, we created the first DVS
video anomaly detection benchmark, namely UCF-Crime-DVS. To fully utilize this
new data modality, a multi-scale spiking fusion network (MSF) is designed based
on spiking neural networks (SNNs). This work explores the potential application
of dynamic information from event data in video anomaly detection. Our
experiments demonstrate the effectiveness of our framework on UCF-Crime-DVS and
its superior performance compared to other models, establishing a new baseline
for SNN-based weakly supervised video anomaly detection.
</details>

### [MFP-CLIP: Exploring the Efficacy of Multi-Form Prompts for Zero-Shot Industrial Anomaly Detection](https://arxiv.org/abs/2503.12910)
*Jingyi Yuan,Pengyu Jie,Junyin Zhang,Ziao Li,Chenqiang Gao*
<details>
  <summary>Abstract</summary>
Recently, zero-shot anomaly detection (ZSAD) has emerged as a pivotal
paradigm for identifying defects in unseen categories without requiring target
samples in training phase. However, existing ZSAD methods struggle with the
boundary of small and complex defects due to insufficient representations. Most
of them use the single manually designed prompts, failing to work for diverse
objects and anomalies. In this paper, we propose MFP-CLIP, a novel prompt-based
CLIP framework which explores the efficacy of multi-form prompts for zero-shot
industrial anomaly detection. We employ an image to text prompting(I2TP)
mechanism to better represent the object in the image. MFP-CLIP enhances
perception to multi-scale and complex anomalies by self prompting(SP) and a
multi-patch feature aggregation(MPFA) module. To precisely localize defects, we
introduce the mask prompting(MP) module to guide model to focus on potential
anomaly regions. Extensive experiments are conducted on two wildly used
industrial anomaly detection benchmarks, MVTecAD and VisA, demonstrating
MFP-CLIP's superiority in ZSAD.
</details>

### [Pose as a Modality: A Psychology-Inspired Network for Personality Recognition with a New Multimodal Dataset](https://arxiv.org/abs/2503.12912)
*Bin Tang,Keqi Pan,Miao Zheng,Ning Zhou,Jialu Sui,Dandan Zhu,Cheng-Long Deng,Shu-Guang Kuai*
<details>
  <summary>Abstract</summary>
In recent years, predicting Big Five personality traits from multimodal data
has received significant attention in artificial intelligence (AI). However,
existing computational models often fail to achieve satisfactory performance.
Psychological research has shown a strong correlation between pose and
personality traits, yet previous research has largely ignored pose data in
computational models. To address this gap, we develop a novel multimodal
dataset that incorporates full-body pose data. The dataset includes video
recordings of 287 participants completing a virtual interview with 36
questions, along with self-reported Big Five personality scores as labels. To
effectively utilize this multimodal data, we introduce the Psychology-Inspired
Network (PINet), which consists of three key modules: Multimodal Feature
Awareness (MFA), Multimodal Feature Interaction (MFI), and Psychology-Informed
Modality Correlation Loss (PIMC Loss). The MFA module leverages the Vision
Mamba Block to capture comprehensive visual features related to personality,
while the MFI module efficiently fuses the multimodal features. The PIMC Loss,
grounded in psychological theory, guides the model to emphasize different
modalities for different personality dimensions. Experimental results show that
the PINet outperforms several state-of-the-art baseline models. Furthermore,
the three modules of PINet contribute almost equally to the model's overall
performance. Incorporating pose data significantly enhances the model's
performance, with the pose modality ranking mid-level in importance among the
five modalities. These findings address the existing gap in personality-related
datasets that lack full-body pose data and provide a new approach for improving
the accuracy of personality prediction models, highlighting the importance of
integrating psychological insights into AI frameworks.
</details>

### [Efficient Multimodal 3D Object Detector via Instance-Level Contrastive Distillation](https://arxiv.org/abs/2503.12914)
*Zhuoqun Su,Huimin Lu,Shuaifeng Jiao,Junhao Xiao,Yaonan Wang,Xieyuanli Chen*
<details>
  <summary>Abstract</summary>
Multimodal 3D object detectors leverage the strengths of both geometry-aware
LiDAR point clouds and semantically rich RGB images to enhance detection
performance. However, the inherent heterogeneity between these modalities,
including unbalanced convergence and modal misalignment, poses significant
challenges. Meanwhile, the large size of the detection-oriented feature also
constrains existing fusion strategies to capture long-range dependencies for
the 3D detection tasks. In this work, we introduce a fast yet effective
multimodal 3D object detector, incorporating our proposed Instance-level
Contrastive Distillation (ICD) framework and Cross Linear Attention Fusion
Module (CLFM). ICD aligns instance-level image features with LiDAR
representations through object-aware contrastive distillation, ensuring
fine-grained cross-modal consistency. Meanwhile, CLFM presents an efficient and
scalable fusion strategy that enhances cross-modal global interactions within
sizable multimodal BEV features. Extensive experiments on the KITTI and
nuScenes 3D object detection benchmarks demonstrate the effectiveness of our
methods. Notably, our 3D object detector outperforms state-of-the-art (SOTA)
methods while achieving superior efficiency. The implementation of our method
has been released as open-source at: https://github.com/nubot-nudt/ICD-Fusion.
</details>

### [MMLNB: Multi-Modal Learning for Neuroblastoma Subtyping Classification Assisted with Textual Description Generation](https://arxiv.org/abs/2503.12927)
*Huangwei Chen,Zhu Zhu,Zhenyu Yan,Yifei Chen,Mingyang Ding,Chenlei Li,Feiwei Qin*
<details>
  <summary>Abstract</summary>
Neuroblastoma (NB), a leading cause of childhood cancer mortality, exhibits
significant histopathological variability, necessitating precise subtyping for
accurate prognosis and treatment. Traditional diagnostic methods rely on
subjective evaluations that are time-consuming and inconsistent. To address
these challenges, we introduce MMLNB, a multi-modal learning (MML) model that
integrates pathological images with generated textual descriptions to improve
classification accuracy and interpretability. The approach follows a two-stage
process. First, we fine-tune a Vision-Language Model (VLM) to enhance
pathology-aware text generation. Second, the fine-tuned VLM generates textual
descriptions, using a dual-branch architecture to independently extract visual
and textual features. These features are fused via Progressive Robust
Multi-Modal Fusion (PRMF) Block for stable training. Experimental results show
that the MMLNB model is more accurate than the single modal model. Ablation
studies demonstrate the importance of multi-modal fusion, fine-tuning, and the
PRMF mechanism. This research creates a scalable AI-driven framework for
digital pathology, enhancing reliability and interpretability in NB subtyping
classification. Our source code is available at
https://github.com/HovChen/MMLNB.
</details>

### [AR-1-to-3: Single Image to Consistent 3D Object Generation via Next-View Prediction](https://arxiv.org/abs/2503.12929)
*Xuying Zhang,Yupeng Zhou,Kai Wang,Yikai Wang,Zhen Li,Xiuli Shao,Daquan Zhou,Qibin Hou,Ming-Ming Cheng*
<details>
  <summary>Abstract</summary>
Novel view synthesis (NVS) is a cornerstone for image-to-3d creation.
However, existing works still struggle to maintain consistency between the
generated views and the input views, especially when there is a significant
camera pose difference, leading to poor-quality 3D geometries and textures. We
attribute this issue to their treatment of all target views with equal priority
according to our empirical observation that the target views closer to the
input views exhibit higher fidelity. With this inspiration, we propose
AR-1-to-3, a novel next-view prediction paradigm based on diffusion models that
first generates views close to the input views, which are then utilized as
contextual information to progressively synthesize farther views. To encode the
generated view subsequences as local and global conditions for the next-view
prediction, we accordingly develop a stacked local feature encoding strategy
(Stacked-LE) and an LSTM-based global feature encoding strategy (LSTM-GE).
Extensive experiments demonstrate that our method significantly improves the
consistency between the generated views and the input views, producing
high-fidelity 3D assets.
</details>

### [L2HCount:Generalizing Crowd Counting from Low to High Crowd Density via Density Simulation](https://arxiv.org/abs/2503.12935)
*Guoliang Xu,Jianqin Yin,Ren Zhang,Yonghao Dang,Feng Zhou,Bo Yu*
<details>
  <summary>Abstract</summary>
Since COVID-19, crowd-counting tasks have gained wide applications. While
supervised methods are reliable, annotation is more challenging in high-density
scenes due to small head sizes and severe occlusion, whereas it's simpler in
low-density scenes. Interestingly, can we train the model in low-density scenes
and generalize it to high-density scenes? Therefore, we propose a low- to
high-density generalization framework (L2HCount) that learns the pattern
related to high-density scenes from low-density ones, enabling it to generalize
well to high-density scenes. Specifically, we first introduce a High-Density
Simulation Module and a Ground-Truth Generation Module to construct fake
high-density images along with their corresponding ground-truth crowd
annotations respectively by image-shifting technique, effectively simulating
high-density crowd patterns. However, the simulated images have two issues:
image blurring and loss of low-density image characteristics. Therefore, we
second propose a Head Feature Enhancement Module to extract clear features in
the simulated high-density scene. Third, we propose a Dual-Density Memory
Encoding Module that uses two crowd memories to learn scene-specific patterns
from low- and simulated high-density scenes, respectively. Extensive
experiments on four challenging datasets have shown the promising performance
of L2HCount.
</details>

### [GIFT: Generated Indoor video frames for Texture-less point tracking](https://arxiv.org/abs/2503.12944)
*Jianzheng Huang,Xianyu Mo,Ziling Liu,Jinyu Yang,Feng Zheng*
<details>
  <summary>Abstract</summary>
Point tracking is becoming a powerful solver for motion estimation and video
editing. Compared to classical feature matching, point tracking methods have
the key advantage of robustly tracking points under complex camera motion
trajectories and over extended periods. However, despite certain improvements
in methodologies, current point tracking methods still struggle to track any
position in video frames, especially in areas that are texture-less or weakly
textured. In this work, we first introduce metrics for evaluating the texture
intensity of a 3D object. Using these metrics, we classify the 3D models in
ShapeNet into three levels of texture intensity and create GIFT, a challenging
synthetic benchmark comprising 1800 indoor video sequences with rich
annotations. Unlike existing datasets that assign ground truth points
arbitrarily, GIFT precisely anchors ground truth on classified target objects,
ensuring that each video corresponds to a specific texture intensity level.
Furthermore, we comprehensively evaluate current methods on GIFT to assess
their performance across different texture intensity levels and analyze the
impact of texture on point tracking.
</details>

### [DivCon-NeRF: Generating Augmented Rays with Diversity and Consistency for Few-shot View Synthesis](https://arxiv.org/abs/2503.12947)
*Ingyun Lee,Jae Won Jang,Seunghyeon Seo,Nojun Kwak*
<details>
  <summary>Abstract</summary>
Neural Radiance Field (NeRF) has shown remarkable performance in novel view
synthesis but requires many multiview images, making it impractical for
few-shot scenarios. Ray augmentation was proposed to prevent overfitting for
sparse training data by generating additional rays. However, existing methods,
which generate augmented rays only near the original rays, produce severe
floaters and appearance distortion due to limited viewpoints and inconsistent
rays obstructed by nearby obstacles and complex surfaces. To address these
problems, we propose DivCon-NeRF, which significantly enhances both diversity
and consistency. It employs surface-sphere augmentation, which preserves the
distance between the original camera and the predicted surface point. This
allows the model to compare the order of high-probability surface points and
filter out inconsistent rays easily without requiring the exact depth. By
introducing inner-sphere augmentation, DivCon-NeRF randomizes angles and
distances for diverse viewpoints, further increasing diversity. Consequently,
our method significantly reduces floaters and visual distortions, achieving
state-of-the-art performance on the Blender, LLFF, and DTU datasets. Our code
will be publicly available.
</details>

### [Frame-wise Conditioning Adaptation for Fine-Tuning Diffusion Models in Text-to-Video Prediction](https://arxiv.org/abs/2503.12953)
*Zheyuan Liu,Junyan Wang,Zicheng Duan,Cristian Rodriguez-Opazo,Anton van den Hengel*
<details>
  <summary>Abstract</summary>
Text-video prediction (TVP) is a downstream video generation task that
requires a model to produce subsequent video frames given a series of initial
video frames and text describing the required motion. In practice TVP methods
focus on a particular category of videos depicting manipulations of objects
carried out by human beings or robot arms. Previous methods adapt models
pre-trained on text-to-image tasks, and thus tend to generate video that lacks
the required continuity. A natural progression would be to leverage more recent
pre-trained text-to-video (T2V) models. This approach is rendered more
challenging by the fact that the most common fine-tuning technique, low-rank
adaptation (LoRA), yields undesirable results. In this work, we propose an
adaptation-based strategy we label Frame-wise Conditioning Adaptation (FCA).
Within the module, we devise a sub-module that produces frame-wise text
embeddings from the input text, which acts as an additional text condition to
aid generation. We use FCA to fine-tune the T2V model, which incorporates the
initial frame(s) as an extra condition. We compare and discuss the more
effective strategy for injecting such embeddings into the T2V model. We conduct
extensive ablation studies on our design choices with quantitative and
qualitative performance analysis. Our approach establishes a new
state-of-the-art for the task of TVP. The project page is at
https://github.com/Cuberick-Orion/FCA .
</details>

### [HIS-GPT: Towards 3D Human-In-Scene Multimodal Understanding](https://arxiv.org/abs/2503.12955)
*Jiahe Zhao,Ruibing Hou,Zejie Tian,Hong Chang,Shiguang Shan*
<details>
  <summary>Abstract</summary>
We propose a new task to benchmark human-in-scene understanding for embodied
agents: Human-In-Scene Question Answering (HIS-QA). Given a human motion within
a 3D scene, HIS-QA requires the agent to comprehend human states and behaviors,
reason about its surrounding environment, and answer human-related questions
within the scene. To support this new task, we present HIS-Bench, a multimodal
benchmark that systematically evaluates HIS understanding across a broad
spectrum, from basic perception to commonsense reasoning and planning. Our
evaluation of various vision-language models on HIS-Bench reveals significant
limitations in their ability to handle HIS-QA tasks. To this end, we propose
HIS-GPT, the first foundation model for HIS understanding. HIS-GPT integrates
3D scene context and human motion dynamics into large language models while
incorporating specialized mechanisms to capture human-scene interactions.
Extensive experiments demonstrate that HIS-GPT sets a new state-of-the-art on
HIS-QA tasks. We hope this work inspires future research on human behavior
analysis in 3D scenes, advancing embodied AI and world models.
</details>

### [Unlock Pose Diversity: Accurate and Efficient Implicit Keypoint-based Spatiotemporal Diffusion for Audio-driven Talking Portrait](https://arxiv.org/abs/2503.12963)
*Chaolong Yang,Kai Yao,Yuyao Yan,Chenru Jiang,Weiguang Zhao,Jie Sun,Guangliang Cheng,Yifei Zhang,Bin Dong,Kaizhu Huang*
<details>
  <summary>Abstract</summary>
Audio-driven single-image talking portrait generation plays a crucial role in
virtual reality, digital human creation, and filmmaking. Existing approaches
are generally categorized into keypoint-based and image-based methods.
Keypoint-based methods effectively preserve character identity but struggle to
capture fine facial details due to the fixed points limitation of the 3D
Morphable Model. Moreover, traditional generative networks face challenges in
establishing causality between audio and keypoints on limited datasets,
resulting in low pose diversity. In contrast, image-based approaches produce
high-quality portraits with diverse details using the diffusion network but
incur identity distortion and expensive computational costs. In this work, we
propose KDTalker, the first framework to combine unsupervised implicit 3D
keypoint with a spatiotemporal diffusion model. Leveraging unsupervised
implicit 3D keypoints, KDTalker adapts facial information densities, allowing
the diffusion process to model diverse head poses and capture fine facial
details flexibly. The custom-designed spatiotemporal attention mechanism
ensures accurate lip synchronization, producing temporally consistent,
high-quality animations while enhancing computational efficiency. Experimental
results demonstrate that KDTalker achieves state-of-the-art performance
regarding lip synchronization accuracy, head pose diversity, and execution
efficiency.Our codes are available at https://github.com/chaolongy/KDTalker.
</details>

### [Training Video Foundation Models with NVIDIA NeMo](https://arxiv.org/abs/2503.12964)
*Zeeshan Patel,Ethan He,Parth Mannan,Xiaowei Ren,Ryan Wolf,Niket Agarwal,Jacob Huffman,Zhuoyao Wang,Carl Wang,Jack Chang,Yan Bai,Tommy Huang,Linnan Wang,Sahil Jain,Shanmugam Ramasamy,Joseph Jennings,Ekaterina Sirazitdinova,Oleg Sudakov,Mingyuan Ma,Bobby Chen,Forrest Lin,Hao Wang,Vasanth Rao Naik Sabavat,Sriharsha Niverty,Rong Ou,Pallab Bhattacharya,David Page,Nima Tajbakhsh,Ashwath Aithal*
<details>
  <summary>Abstract</summary>
Video Foundation Models (VFMs) have recently been used to simulate the real
world to train physical AI systems and develop creative visual experiences.
However, there are significant challenges in training large-scale, high quality
VFMs that can generate high-quality videos. We present a scalable, open-source
VFM training pipeline with NVIDIA NeMo, providing accelerated video dataset
curation, multimodal data loading, and parallelized video diffusion model
training and inference. We also provide a comprehensive performance analysis
highlighting best practices for efficient VFM training and inference.
</details>

### [OptiPMB: Enhancing 3D Multi-Object Tracking with Optimized Poisson Multi-Bernoulli Filtering](https://arxiv.org/abs/2503.12968)
*Guanhua Ding,Yuxuan Xia,Runwei Guan,Qinchen Wu,Tao Huang,Weiping Ding,Jinping Sun,Guoqiang Mao*
<details>
  <summary>Abstract</summary>
Accurate 3D multi-object tracking (MOT) is crucial for autonomous driving, as
it enables robust perception, navigation, and planning in complex environments.
While deep learning-based solutions have demonstrated impressive 3D MOT
performance, model-based approaches remain appealing for their simplicity,
interpretability, and data efficiency. Conventional model-based trackers
typically rely on random vector-based Bayesian filters within the
tracking-by-detection (TBD) framework but face limitations due to heuristic
data association and track management schemes. In contrast, random finite set
(RFS)-based Bayesian filtering handles object birth, survival, and death in a
theoretically sound manner, facilitating interpretability and parameter tuning.
In this paper, we present OptiPMB, a novel RFS-based 3D MOT method that employs
an optimized Poisson multi-Bernoulli (PMB) filter while incorporating several
key innovative designs within the TBD framework. Specifically, we propose a
measurement-driven hybrid adaptive birth model for improved track
initialization, employ adaptive detection probability parameters to effectively
maintain tracks for occluded objects, and optimize density pruning and track
extraction modules to further enhance overall tracking performance. Extensive
evaluations on nuScenes and KITTI datasets show that OptiPMB achieves superior
tracking accuracy compared with state-of-the-art methods, thereby establishing
a new benchmark for model-based 3D MOT and offering valuable insights for
future research on RFS-based trackers in autonomous driving.
</details>

### [Action tube generation by person query matching for spatio-temporal action detection](https://arxiv.org/abs/2503.12969)
*Kazuki Omi,Jion Oshima,Toru Tamaki*
<details>
  <summary>Abstract</summary>
This paper proposes a method for spatio-temporal action detection (STAD) that
directly generates action tubes from the original video without relying on
post-processing steps such as IoU-based linking and clip splitting. Our
approach applies query-based detection (DETR) to each frame and matches DETR
queries to link the same person across frames. We introduce the Query Matching
Module (QMM), which uses metric learning to bring queries for the same person
closer together across frames compared to queries for different people. Action
classes are predicted using the sequence of queries obtained from QMM matching,
allowing for variable-length inputs from videos longer than a single clip.
Experimental results on JHMDB, UCF101-24, and AVA datasets demonstrate that our
method performs well for large position changes of people while offering
superior computational efficiency and lower resource requirements.
</details>

### [Aligning Vision to Language: Text-Free Multimodal Knowledge Graph Construction for Enhanced LLMs Reasoning](https://arxiv.org/abs/2503.12972)
*Junming Liu,Siyuan Meng,Yanting Gao,Song Mao,Pinlong Cai,Guohang Yan,Yirong Chen,Zilin Bian,Botian Shi,Ding Wang*
<details>
  <summary>Abstract</summary>
Multimodal reasoning in Large Language Models (LLMs) struggles with
incomplete knowledge and hallucination artifacts, challenges that textual
Knowledge Graphs (KGs) only partially mitigate due to their modality isolation.
While Multimodal Knowledge Graphs (MMKGs) promise enhanced cross-modal
understanding, their practical construction is impeded by semantic narrowness
of manual text annotations and inherent noise in visual-semantic entity
linkages. In this paper, we propose Vision-align-to-Language integrated
Knowledge Graph (VaLiK), a novel approach for constructing MMKGs that enhances
LLMs reasoning through cross-modal information supplementation. Specifically,
we cascade pre-trained Vision-Language Models (VLMs) to align image features
with text, transforming them into descriptions that encapsulate image-specific
information. Furthermore, we developed a cross-modal similarity verification
mechanism to quantify semantic consistency, effectively filtering out noise
introduced during feature alignment. Even without manually annotated image
captions, the refined descriptions alone suffice to construct the MMKG.
Compared to conventional MMKGs construction paradigms, our approach achieves
substantial storage efficiency gains while maintaining direct entity-to-image
linkage capability. Experimental results on multimodal reasoning tasks
demonstrate that LLMs augmented with VaLiK outperform previous state-of-the-art
models. Our code is published at https://github.com/Wings-Of-Disaster/VaLiK.
</details>

### [Prospects for Mitigating Spectral Variability in Tropical Species Classification Using Self-Supervised Learning](https://arxiv.org/abs/2503.12973)
*Colin Prieur,Nassim Ait Ali Braham,Paul Tresson,Gr√©goire Vincent,Jocelyn Chanussot*
<details>
  <summary>Abstract</summary>
Airborne hyperspectral imaging is a promising method for identifying tropical
species, but spectral variability between acquisitions hinders consistent
results. This paper proposes using Self-Supervised Learning (SSL) to encode
spectral features that are robust to abiotic variability and relevant for
species identification. By employing the state-of-the-art Barlow-Twins approach
on repeated spectral acquisitions, we demonstrate the ability to develop stable
features. For the classification of 40 tropical species, experiments show that
these features can outperform typical reflectance products in terms of
robustness to spectral variability by 10 points of accuracy across dates.
</details>

### [Exploring 3D Activity Reasoning and Planning: From Implicit Human Intentions to Route-Aware Planning](https://arxiv.org/abs/2503.12974)
*Xueying Jiang,Wenhao Li,Xiaoqin Zhang,Ling Shao,Shijian Lu*
<details>
  <summary>Abstract</summary>
3D activity reasoning and planning has attracted increasing attention in
human-robot interaction and embodied AI thanks to the recent advance in
multimodal learning. However, most existing works share two constraints: 1)
heavy reliance on explicit instructions with little reasoning on implicit user
intention; 2) negligence of inter-step route planning on robot moves. To bridge
the gaps, we propose 3D activity reasoning and planning, a novel 3D task that
reasons the intended activities from implicit instructions and decomposes them
into steps with inter-step routes and planning under the guidance of
fine-grained 3D object shapes and locations from scene segmentation. We tackle
the new 3D task from two perspectives. First, we construct ReasonPlan3D, a
large-scale benchmark that covers diverse 3D scenes with rich implicit
instructions and detailed annotations for multi-step task planning, inter-step
route planning, and fine-grained segmentation. Second, we design a novel
framework that introduces progressive plan generation with contextual
consistency across multiple steps, as well as a scene graph that is updated
dynamically for capturing critical objects and their spatial relations.
Extensive experiments demonstrate the effectiveness of our benchmark and
framework in reasoning activities from implicit human instructions, producing
accurate stepwise task plans, and seamlessly integrating route planning for
multi-step moves. The dataset and code will be released.
</details>

### [Analyzing Swimming Performance Using Drone Captured Aerial Videos](https://arxiv.org/abs/2503.12981)
*Thu Tran,Kenny Tsu Wei Choo,Shaohui Foong,Hitesh Bhardwaj,Shane Kyi Hla Win,Wei Jun Ang,Kenneth Goh,Rajesh Krishna Balan*
<details>
  <summary>Abstract</summary>
Monitoring swimmer performance is crucial for improving training and
enhancing athletic techniques. Traditional methods for tracking swimmers, such
as above-water and underwater cameras, face limitations due to the need for
multiple cameras and obstructions from water splashes. This paper presents a
novel approach for tracking swimmers using a moving UAV. The proposed system
employs a UAV equipped with a high-resolution camera to capture aerial footage
of the swimmers. The footage is then processed using computer vision algorithms
to extract the swimmers' positions and movements. This approach offers several
advantages, including single camera use and comprehensive coverage. The
system's accuracy is evaluated with both training and in competition videos.
The results demonstrate the system's ability to accurately track swimmers'
movements, limb angles, stroke duration and velocity with the maximum error of
0.3 seconds and 0.35~m/s for stroke duration and velocity, respectively.
</details>

### [SparseAlign: A Fully Sparse Framework for Cooperative Object Detection](https://arxiv.org/abs/2503.12982)
*Yunshuang Yuan,Yan Xia,Daniel Cremers,Monika Sester*
<details>
  <summary>Abstract</summary>
Cooperative perception can increase the view field and decrease the occlusion
of an ego vehicle, hence improving the perception performance and safety of
autonomous driving. Despite the success of previous works on cooperative object
detection, they mostly operate on dense Bird's Eye View (BEV) feature maps,
which are computationally demanding and can hardly be extended to long-range
detection problems. More efficient fully sparse frameworks are rarely explored.
In this work, we design a fully sparse framework, SparseAlign, with three key
features: an enhanced sparse 3D backbone, a query-based temporal context
learning module, and a robust detection head specially tailored for sparse
features. Extensive experimental results on both OPV2V and DairV2X datasets
show that our framework, despite its sparsity, outperforms the state of the art
with less communication bandwidth requirements. In addition, experiments on the
OPV2Vt and DairV2Xt datasets for time-aligned cooperative object detection also
show a significant performance gain compared to the baseline works.
</details>

### [Concept-as-Tree: Synthetic Data is All You Need for VLM Personalization](https://arxiv.org/abs/2503.12999)
*Ruichuan An,Kai Zeng,Ming Lu,Sihan Yang,Renrui Zhang,Huitong Ji,Qizhe Zhang,Yulin Luo,Hao Liang,Wentao Zhang*
<details>
  <summary>Abstract</summary>
Vision-Language Models (VLMs) have demonstrated exceptional performance in
various multi-modal tasks. Recently, there has been an increasing interest in
improving the personalization capabilities of VLMs. To better integrate
user-provided concepts into VLMs, many methods use positive and negative
samples to fine-tune these models. However, the scarcity of user-provided
positive samples and the low quality of retrieved negative samples pose
challenges for fine-tuning. To reveal the relationship between sample and model
performance, we systematically investigate the impact of positive and negative
samples (easy and hard) and their diversity on VLM personalization tasks. Based
on the detailed analysis, we introduce Concept-as-Tree (CaT), which represents
a concept as a tree structure, thereby enabling the data generation of positive
and negative samples with varying difficulty and diversity for VLM
personalization. With a well-designed data filtering strategy, our CaT
framework can ensure the quality of generated data, constituting a powerful
pipeline. We perform thorough experiments with various VLM personalization
baselines to assess the effectiveness of the pipeline, alleviating the lack of
positive samples and the low quality of negative samples. Our results
demonstrate that CaT equipped with the proposed data filter significantly
enhances the personalization capabilities of VLMs across the MyVLM, Yo'LLaVA,
and MC-LLaVA datasets. To our knowledge, this work is the first controllable
synthetic data pipeline for VLM personalization. The code is released at
\href{https://github.com/zengkaiya/CaT}{https://github.com/zengkaiya/CaT}.
</details>

### [TFDM: Time-Variant Frequency-Based Point Cloud Diffusion with Mamba](https://arxiv.org/abs/2503.13004)
*Jiaxu Liu,Li Li,Hubert P. H. Shum,Toby P. Breckon*
<details>
  <summary>Abstract</summary>
Diffusion models currently demonstrate impressive performance over various
generative tasks. Recent work on image diffusion highlights the strong
capabilities of Mamba (state space models) due to its efficient handling of
long-range dependencies and sequential data modeling. Unfortunately, joint
consideration of state space models with 3D point cloud generation remains
limited. To harness the powerful capabilities of the Mamba model for 3D point
cloud generation, we propose a novel diffusion framework containing dual latent
Mamba block (DM-Block) and a time-variant frequency encoder (TF-Encoder). The
DM-Block apply a space-filling curve to reorder points into sequences suitable
for Mamba state-space modeling, while operating in a latent space to mitigate
the computational overhead that arises from direct 3D data processing.
Meanwhile, the TF-Encoder takes advantage of the ability of the diffusion model
to refine fine details in later recovery stages by prioritizing key points
within the U-Net architecture. This frequency-based mechanism ensures enhanced
detail quality in the final stages of generation. Experimental results on the
ShapeNet-v2 dataset demonstrate that our method achieves state-of-the-art
performance (ShapeNet-v2: 0.14\% on 1-NNA-Abs50 EMD and 57.90\% on COV EMD) on
certain metrics for specific categories while reducing computational parameters
and inference time by up to 10$\times$ and 9$\times$, respectively. Source code
is available in Supplementary Materials and will be released upon accpetance.
</details>

### [Test-Time Domain Generalization via Universe Learning: A Multi-Graph Matching Approach for Medical Image Segmentation](https://arxiv.org/abs/2503.13012)
*Xingguo Lv,Xingbo Dong,Liwen Wang,Jiewen Yang,Lei Zhao,Bin Pu,Zhe Jin,Xuejun Li*
<details>
  <summary>Abstract</summary>
Despite domain generalization (DG) has significantly addressed the
performance degradation of pre-trained models caused by domain shifts, it often
falls short in real-world deployment. Test-time adaptation (TTA), which adjusts
a learned model using unlabeled test data, presents a promising solution.
However, most existing TTA methods struggle to deliver strong performance in
medical image segmentation, primarily because they overlook the crucial prior
knowledge inherent to medical images. To address this challenge, we incorporate
morphological information and propose a framework based on multi-graph
matching. Specifically, we introduce learnable universe embeddings that
integrate morphological priors during multi-source training, along with novel
unsupervised test-time paradigms for domain adaptation. This approach
guarantees cycle-consistency in multi-matching while enabling the model to more
effectively capture the invariant priors of unseen data, significantly
mitigating the effects of domain shifts. Extensive experiments demonstrate that
our method outperforms other state-of-the-art approaches on two medical image
segmentation benchmarks for both multi-source and single-source domain
generalization tasks. The source code is available at
https://github.com/Yore0/TTDG-MGM.
</details>

### [Efficient Motion-Aware Video MLLM](https://arxiv.org/abs/2503.13016)
*Zijia Zhao,Yuqi Huo,Tongtian Yue,Longteng Guo,Haoyu Lu,Bingning Wang,Weipeng Chen,Jing Liu*
<details>
  <summary>Abstract</summary>
Most current video MLLMs rely on uniform frame sampling and image-level
encoders, resulting in inefficient data processing and limited motion
awareness. To address these challenges, we introduce EMA, an Efficient
Motion-Aware video MLLM that utilizes compressed video structures as inputs. We
propose a motion-aware GOP (Group of Pictures) encoder that fuses spatial and
motion information within a GOP unit in the compressed video stream, generating
compact, informative visual tokens. By integrating fewer but denser RGB frames
with more but sparser motion vectors in this native slow-fast input
architecture, our approach reduces redundancy and enhances motion
representation. Additionally, we introduce MotionBench, a benchmark for
evaluating motion understanding across four motion types: linear, curved,
rotational, and contact-based. Experimental results show that EMA achieves
state-of-the-art performance on both MotionBench and popular video question
answering benchmarks, while reducing inference costs. Moreover, EMA
demonstrates strong scalability, as evidenced by its competitive performance on
long video understanding benchmarks.
</details>

### [Real-Time Multi-Object Tracking using YOLOv8 and SORT on a SoC FPGA](https://arxiv.org/abs/2503.13023)
*Michal Danilowicz,Tomasz Kryjak*
<details>
  <summary>Abstract</summary>
Multi-object tracking (MOT) is one of the most important problems in computer
vision and a key component of any vision-based perception system used in
advanced autonomous mobile robotics. Therefore, its implementation on low-power
and real-time embedded platforms is highly desirable. Modern MOT algorithms
should be able to track objects of a given class (e.g. people or vehicles). In
addition, the number of objects to be tracked is not known in advance, and they
may appear and disappear at any time, as well as be obscured. For these
reasons, the most popular and successful approaches have recently been based on
the tracking paradigm. Therefore, the presence of a high quality object
detector is essential, which in practice accounts for the vast majority of the
computational and memory complexity of the whole MOT system. In this paper, we
propose an FPGA (Field-Programmable Gate Array) implementation of an embedded
MOT system based on a quantized YOLOv8 detector and the SORT (Simple Online
Realtime Tracker) tracker. We use a modified version of the FINN framework to
utilize external memory for model parameters and to support operations
necessary required by YOLOv8. We discuss the evaluation of detection and
tracking performance using the COCO and MOT15 datasets, where we achieve 0.21
mAP and 38.9 MOTA respectively. As the computational platform, we use an MPSoC
system (Zynq UltraScale+ device from AMD/Xilinx) where the detector is deployed
in reprogrammable logic and the tracking algorithm is implemented in the
processor system.
</details>

### [PoseSyn: Synthesizing Diverse 3D Pose Data from In-the-Wild 2D Data](https://arxiv.org/abs/2503.13025)
*ChangHee Yang,Hyeonseop Song,Seokhun Choi,Seungwoo Lee,Jaechul Kim,Hoseok Do*
<details>
  <summary>Abstract</summary>
Despite considerable efforts to enhance the generalization of 3D pose
estimators without costly 3D annotations, existing data augmentation methods
struggle in real world scenarios with diverse human appearances and complex
poses. We propose PoseSyn, a novel data synthesis framework that transforms
abundant in the wild 2D pose dataset into diverse 3D pose image pairs. PoseSyn
comprises two key components: Error Extraction Module (EEM), which identifies
challenging poses from the 2D pose datasets, and Motion Synthesis Module (MSM),
which synthesizes motion sequences around the challenging poses. Then, by
generating realistic 3D training data via a human animation model aligned with
challenging poses and appearances PoseSyn boosts the accuracy of various 3D
pose estimators by up to 14% across real world benchmarks including various
backgrounds and occlusions, challenging poses, and multi view scenarios.
Extensive experiments further confirm that PoseSyn is a scalable and effective
approach for improving generalization without relying on expensive 3D
annotations, regardless of the pose estimator's model size or design.
</details>

### [HiMTok: Learning Hierarchical Mask Tokens for Image Segmentation with Large Multimodal Model](https://arxiv.org/abs/2503.13026)
*Tao Wang,Changxu Cheng,Lingfeng Wang,Senda Chen,Wuyue Zhao*
<details>
  <summary>Abstract</summary>
The remarkable performance of large multimodal models (LMMs) has attracted
significant interest from the image segmentation community. To align with the
next-token-prediction paradigm, current LMM-driven segmentation methods either
use object boundary points to represent masks or introduce special segmentation
tokens, whose hidden states are decoded by a segmentation model requiring the
original image as input. However, these approaches often suffer from inadequate
mask representation and complex architectures, limiting the potential of LMMs.
In this work, we propose the Hierarchical Mask Tokenizer (HiMTok), which
represents segmentation masks with up to 32 tokens and eliminates the need for
the original image during mask de-tokenization. HiMTok allows for compact and
coarse-to-fine mask representations, aligning well with the LLM
next-token-prediction paradigm and facilitating the direct acquisition of
segmentation capabilities. We develop a 3-stage training recipe for progressive
learning of segmentation and visual capabilities, featuring a hierarchical mask
loss for effective coarse-to-fine learning. Additionally, we enable
bidirectional information flow, allowing conversion between bounding boxes and
mask tokens to fully leverage multi-task training potential. Extensive
experiments demonstrate that our method achieves state-of-the-art performance
across various segmentation tasks,while also enhancing visual grounding and
maintaining overall visual understanding.
</details>

### [Beyond Role-Based Surgical Domain Modeling: Generalizable Re-Identification in the Operating Room](https://arxiv.org/abs/2503.13028)
*Tony Danjun Wang,Lennart Bastian,Tobias Czempiel,Christian Heiliger,Nassir Navab*
<details>
  <summary>Abstract</summary>
Surgical domain models improve workflow optimization through automated
predictions of each staff member's surgical role. However, mounting evidence
indicates that team familiarity and individuality impact surgical outcomes. We
present a novel staff-centric modeling approach that characterizes individual
team members through their distinctive movement patterns and physical
characteristics, enabling long-term tracking and analysis of surgical personnel
across multiple procedures. To address the challenge of inter-clinic
variability, we develop a generalizable re-identification framework that
encodes sequences of 3D point clouds to capture shape and articulated motion
patterns unique to each individual. Our method achieves 86.19% accuracy on
realistic clinical data while maintaining 75.27% accuracy when transferring
between different environments - a 12% improvement over existing methods. When
used to augment markerless personnel tracking, our approach improves accuracy
by over 50%. Through extensive validation across three datasets and the
introduction of a novel workflow visualization technique, we demonstrate how
our framework can reveal novel insights into surgical team dynamics and space
utilization patterns, advancing methods to analyze surgical workflows and team
coordination.
</details>

### [All You Need to Know About Training Image Retrieval Models](https://arxiv.org/abs/2503.13045)
*Gabriele Berton,Kevin Musgrave,Carlo Masone*
<details>
  <summary>Abstract</summary>
Image retrieval is the task of finding images in a database that are most
similar to a given query image. The performance of an image retrieval pipeline
depends on many training-time factors, including the embedding model
architecture, loss function, data sampler, mining function, learning rate(s),
and batch size. In this work, we run tens of thousands of training runs to
understand the effect each of these factors has on retrieval accuracy. We also
discover best practices that hold across multiple datasets. The code is
available at https://github.com/gmberton/image-retrieval
</details>

### [InsightDrive: Insight Scene Representation for End-to-End Autonomous Driving](https://arxiv.org/abs/2503.13047)
*Ruiqi Song,Xianda Guo,Hangbin Wu,Qinggong Wei,Long Chen*
<details>
  <summary>Abstract</summary>
Directly generating planning results from raw sensors has become increasingly
prevalent due to its adaptability and robustness in complex scenarios. Scene
representation, as a key module in the pipeline, has traditionally relied on
conventional perception, which focus on the global scene. However, in driving
scenarios, human drivers typically focus only on regions that directly impact
driving, which often coincide with those required for end-to-end autonomous
driving. In this paper, a novel end-to-end autonomous driving method called
InsightDrive is proposed, which organizes perception by language-guided scene
representation. We introduce an instance-centric scene tokenizer that
transforms the surrounding environment into map- and object-aware instance
tokens. Scene attention language descriptions, which highlight key regions and
obstacles affecting the ego vehicle's movement, are generated by a
vision-language model that leverages the cognitive reasoning capabilities of
foundation models. We then align scene descriptions with visual features using
the vision-language model, guiding visual attention through these descriptions
to give effectively scene representation. Furthermore, we employ self-attention
and cross-attention mechanisms to model the ego-agents and ego-map
relationships to comprehensively build the topological relationships of the
scene. Finally, based on scene understanding, we jointly perform motion
prediction and planning. Extensive experiments on the widely used nuScenes
benchmark demonstrate that the proposed InsightDrive achieves state-of-the-art
performance in end-to-end autonomous driving. The code is available at
https://github.com/songruiqi/InsightDrive
</details>

### [Uncertainty-Aware Knowledge Distillation for Compact and Efficient 6DoF Pose Estimation](https://arxiv.org/abs/2503.13053)
*Nassim Ali Ousalah,Anis Kacem,Enjie Ghorbel,Emmanuel Koumandakis,Djamila Aouada*
<details>
  <summary>Abstract</summary>
Compact and efficient 6DoF object pose estimation is crucial in applications
such as robotics, augmented reality, and space autonomous navigation systems,
where lightweight models are critical for real-time accurate performance. This
paper introduces a novel uncertainty-aware end-to-end Knowledge Distillation
(KD) framework focused on keypoint-based 6DoF pose estimation. Keypoints
predicted by a large teacher model exhibit varying levels of uncertainty that
can be exploited within the distillation process to enhance the accuracy of the
student model while ensuring its compactness. To this end, we propose a
distillation strategy that aligns the student and teacher predictions by
adjusting the knowledge transfer based on the uncertainty associated with each
teacher keypoint prediction. Additionally, the proposed KD leverages this
uncertainty-aware alignment of keypoints to transfer the knowledge at key
locations of their respective feature maps. Experiments on the widely-used
LINEMOD benchmark demonstrate the effectiveness of our method, achieving
superior 6DoF object pose estimation with lightweight models compared to
state-of-the-art approaches. Further validation on the SPEED+ dataset for
spacecraft pose estimation highlights the robustness of our approach under
diverse 6DoF pose estimation scenarios.
</details>

### [Do Vision Models Develop Human-Like Progressive Difficulty Understanding?](https://arxiv.org/abs/2503.13058)
*Zeyi Huang,Utkarsh Ojha,Yuyang Ji,Donghyun Lee,Yong Jae Lee*
<details>
  <summary>Abstract</summary>
When a human undertakes a test, their responses likely follow a pattern: if
they answered an easy question $(2 \times 3)$ incorrectly, they would likely
answer a more difficult one $(2 \times 3 \times 4)$ incorrectly; and if they
answered a difficult question correctly, they would likely answer the easy one
correctly. Anything else hints at memorization. Do current visual recognition
models exhibit a similarly structured learning capacity? In this work, we
consider the task of image classification and study if those models' responses
follow that pattern. Since real images aren't labeled with difficulty, we first
create a dataset of 100 categories, 10 attributes, and 3 difficulty levels
using recent generative models: for each category (e.g., dog) and attribute
(e.g., occlusion), we generate images of increasing difficulty (e.g., a dog
without occlusion, a dog only partly visible). We find that most of the models
do in fact behave similarly to the aforementioned pattern around 80-90% of the
time. Using this property, we then explore a new way to evaluate those models.
Instead of testing the model on every possible test image, we create an
adaptive test akin to GRE, in which the model's performance on the current
round of images determines the test images in the next round. This allows the
model to skip over questions too easy/hard for itself, and helps us get its
overall performance in fewer steps.
</details>

### [Historic Scripts to Modern Vision: A Novel Dataset and A VLM Framework for Transliteration of Modi Script to Devanagari](https://arxiv.org/abs/2503.13060)
*Harshal Kausadikar,Tanvi Kale,Onkar Susladkar,Sparsh Mittal*
<details>
  <summary>Abstract</summary>
In medieval India, the Marathi language was written using the Modi script.
The texts written in Modi script include extensive knowledge about medieval
sciences, medicines, land records and authentic evidence about Indian history.
Around 40 million documents are in poor condition and have not yet been
transliterated. Furthermore, only a few experts in this domain can
transliterate this script into English or Devanagari. Most of the past research
predominantly focuses on individual character recognition. A system that can
transliterate Modi script documents to Devanagari script is needed. We propose
the MoDeTrans dataset, comprising 2,043 images of Modi script documents
accompanied by their corresponding textual transliterations in Devanagari. We
further introduce MoScNet (\textbf{Mo}di \textbf{Sc}ript \textbf{Net}work), a
novel Vision-Language Model (VLM) framework for transliterating Modi script
images into Devanagari text. MoScNet leverages Knowledge Distillation, where a
student model learns from a teacher model to enhance transliteration
performance. The final student model of MoScNet has better performance than the
teacher model while having 163$\times$ lower parameters. Our work is the first
to perform direct transliteration from the handwritten Modi script to the
Devanagari script. MoScNet also shows competitive results on the optical
character recognition (OCR) task.
</details>

### [Federated Learning with Domain Shift Eraser](https://arxiv.org/abs/2503.13063)
*Zheng Wang,Zihui Wang,Zheng Wang,Xiaoliang Fan,Cheng Wang*
<details>
  <summary>Abstract</summary>
Federated learning (FL) is emerging as a promising technique for
collaborative learning without local data leaving their devices. However,
clients' data originating from diverse domains may degrade model performance
due to domain shifts, preventing the model from learning consistent
representation space. In this paper, we propose a novel FL framework, Federated
Domain Shift Eraser (FDSE), to improve model performance by differently erasing
each client's domain skew and enhancing their consensus. First, we formulate
the model forward passing as an iterative deskewing process that extracts and
then deskews features alternatively. This is efficiently achieved by
decomposing each original layer in the neural network into a Domain-agnostic
Feature Extractor (DFE) and a Domain-specific Skew Eraser (DSE). Then, a
regularization term is applied to promise the effectiveness of feature
deskewing by pulling local statistics of DSE's outputs close to the globally
consistent ones. Finally, DFE modules are fairly aggregated and broadcast to
all the clients to maximize their consensus, and DSE modules are personalized
for each client via similarity-aware aggregation to erase their domain skew
differently. Comprehensive experiments were conducted on three datasets to
confirm the advantages of our method in terms of accuracy, efficiency, and
generalizability.
</details>

### [Crab: A Unified Audio-Visual Scene Understanding Model with Explicit Cooperation](https://arxiv.org/abs/2503.13068)
*Henghui Du,Guangyao Li,Chang Zhou,Chunjie Zhang,Alan Zhao,Di Hu*
<details>
  <summary>Abstract</summary>
In recent years, numerous tasks have been proposed to encourage model to
develop specified capability in understanding audio-visual scene, primarily
categorized into temporal localization, spatial localization, spatio-temporal
reasoning, and pixel-level understanding. Instead, human possesses a unified
understanding ability for diversified tasks. Therefore, designing an
audio-visual model with general capability to unify these tasks is of great
value. However, simply joint training for all tasks can lead to interference
due to the heterogeneity of audiovisual data and complex relationship among
tasks. We argue that this problem can be solved through explicit cooperation
among tasks. To achieve this goal, we propose a unified learning method which
achieves explicit inter-task cooperation from both the perspectives of data and
model thoroughly. Specifically, considering the labels of existing datasets are
simple words, we carefully refine these datasets and construct an Audio-Visual
Unified Instruction-tuning dataset with Explicit reasoning process (AV-UIE),
which clarifies the cooperative relationship among tasks. Subsequently, to
facilitate concrete cooperation in learning stage, an interaction-aware LoRA
structure with multiple LoRA heads is designed to learn different aspects of
audiovisual data interaction. By unifying the explicit cooperation across the
data and model aspect, our method not only surpasses existing unified
audio-visual model on multiple tasks, but also outperforms most specialized
models for certain tasks. Furthermore, we also visualize the process of
explicit cooperation and surprisingly find that each LoRA head has certain
audio-visual understanding ability. Code and dataset:
https://github.com/GeWu-Lab/Crab
</details>

### [Rewards Are Enough for Fast Photo-Realistic Text-to-image Generation](https://arxiv.org/abs/2503.13070)
*Yihong Luo,Tianyang Hu,Weijian Luo,Kenji Kawaguchi,Jing Tang*
<details>
  <summary>Abstract</summary>
Aligning generated images to complicated text prompts and human preferences
is a central challenge in Artificial Intelligence-Generated Content (AIGC).
With reward-enhanced diffusion distillation emerging as a promising approach
that boosts controllability and fidelity of text-to-image models, we identify a
fundamental paradigm shift: as conditions become more specific and reward
signals stronger, the rewards themselves become the dominant force in
generation. In contrast, the diffusion losses serve as an overly expensive form
of regularization. To thoroughly validate our hypothesis, we introduce R0, a
novel conditional generation approach via regularized reward maximization.
Instead of relying on tricky diffusion distillation losses, R0 proposes a new
perspective that treats image generations as an optimization problem in data
space which aims to search for valid images that have high compositional
rewards. By innovative designs of the generator parameterization and proper
regularization techniques, we train state-of-the-art few-step text-to-image
generative models with R0 at scales. Our results challenge the conventional
wisdom of diffusion post-training and conditional generation by demonstrating
that rewards play a dominant role in scenarios with complex conditions. We hope
our findings can contribute to further research into human-centric and
reward-centric generation paradigms across the broader field of AIGC. Code is
available at https://github.com/Luo-Yihong/R0.
</details>

### [DehazeMamba: SAR-guided Optical Remote Sensing Image Dehazing with Adaptive State Space Model](https://arxiv.org/abs/2503.13073)
*Zhicheng Zhao,Jinquan Yan,Chenglong Li,Xiao Wang,Jin Tang*
<details>
  <summary>Abstract</summary>
Optical remote sensing image dehazing presents significant challenges due to
its extensive spatial scale and highly non-uniform haze distribution, which
traditional single-image dehazing methods struggle to address effectively.
While Synthetic Aperture Radar (SAR) imagery offers inherently haze-free
reference information for large-scale scenes, existing SAR-guided dehazing
approaches face two critical limitations: the integration of SAR information
often diminishes the quality of haze-free regions, and the instability of
feature quality further exacerbates cross-modal domain shift. To overcome these
challenges, we introduce DehazeMamba, a novel SAR-guided dehazing network built
on a progressive haze decoupling fusion strategy. Our approach incorporates two
key innovations: a Haze Perception and Decoupling Module (HPDM) that
dynamically identifies haze-affected regions through optical-SAR difference
analysis, and a Progressive Fusion Module (PFM) that mitigates domain shift
through a two-stage fusion process based on feature quality assessment. To
facilitate research in this domain, we present MRSHaze, a large-scale benchmark
dataset comprising 8,000 pairs of temporally synchronized, precisely
geo-registered SAR-optical images with high resolution and diverse haze
conditions. Extensive experiments demonstrate that DehazeMamba significantly
outperforms state-of-the-art methods, achieving a 0.73 dB improvement in PSNR
and substantial enhancements in downstream tasks such as semantic segmentation.
The dataset is available at
https://github.com/mmic-lcl/Datasets-and-benchmark-code.
</details>

### [Rethinking Image Evaluation in Super-Resolution](https://arxiv.org/abs/2503.13074)
*Shaolin Su,Josep M. Rocafort,Danna Xue,David Serrano-Lozano,Lei Sun,Javier Vazquez-Corral*
<details>
  <summary>Abstract</summary>
While recent advancing image super-resolution (SR) techniques are continually
improving the perceptual quality of their outputs, they can usually fail in
quantitative evaluations. This inconsistency leads to a growing distrust in
existing image metrics for SR evaluations. Though image evaluation depends on
both the metric and the reference ground truth (GT), researchers typically do
not inspect the role of GTs, as they are generally accepted as `perfect'
references. However, due to the data being collected in the early years and the
ignorance of controlling other types of distortions, we point out that GTs in
existing SR datasets can exhibit relatively poor quality, which leads to biased
evaluations. Following this observation, in this paper, we are interested in
the following questions: Are GT images in existing SR datasets 100\%
trustworthy for model evaluations? How does GT quality affect this evaluation?
And how to make fair evaluations if there exist imperfect GTs? To answer these
questions, this paper presents two main contributions. First, by systematically
analyzing seven state-of-the-art SR models across three real-world SR datasets,
we show that SR performances can be consistently affected across models by
low-quality GTs, and models can perform quite differently when GT quality is
controlled. Second, we propose a novel perceptual quality metric, Relative
Quality Index (RQI), that measures the relative quality discrepancy of image
pairs, thus issuing the biased evaluations caused by unreliable GTs. Our
proposed model achieves significantly better consistency with human opinions.
We expect our work to provide insights for the SR community on how future
datasets, models, and metrics should be developed.
</details>

### [Gaussian On-the-Fly Splatting: A Progressive Framework for Robust Near Real-Time 3DGS Optimization](https://arxiv.org/abs/2503.13086)
*Yiwei Xu,Yifei Yu,Wentian Gan,Tengfei Wang,Zongqian Zhan,Hao Cheng,Xin Wang*
<details>
  <summary>Abstract</summary>
3D Gaussian Splatting (3DGS) achieves high-fidelity rendering with fast
real-time performance, but existing methods rely on offline training after full
Structure-from-Motion (SfM) processing. In contrast, this work introduces
On-the-Fly GS, a progressive framework enabling near real-time 3DGS
optimization during image capture. As each image arrives, its pose and sparse
points are updated via on-the-fly SfM, and newly optimized Gaussians are
immediately integrated into the 3DGS field. We propose a progressive local
optimization strategy to prioritize new images and their neighbors by their
corresponding overlapping relationship, allowing the new image and its
overlapping images to get more training. To further stabilize training across
old and new images, an adaptive learning rate schedule balances the iterations
and the learning rate. Moreover, to maintain overall quality of the 3DGS field,
an efficient global optimization scheme prevents overfitting to the newly added
images. Experiments on multiple benchmark datasets show that our On-the-Fly GS
reduces training time significantly, optimizing each new image in seconds with
minimal rendering loss, offering the first practical step toward rapid,
progressive 3DGS reconstruction.
</details>

### [ClearSight: Visual Signal Enhancement for Object Hallucination Mitigation in Multimodal Large language Models](https://arxiv.org/abs/2503.13107)
*Hao Yin,Guangzong Si,Zilei Wang*
<details>
  <summary>Abstract</summary>
Contrastive decoding strategies are widely used to mitigate object
hallucinations in multimodal large language models (MLLMs). By reducing
over-reliance on language priors, these strategies ensure that generated
content remains closely grounded in visual inputs, producing contextually
accurate outputs. Since contrastive decoding requires no additional training or
external tools, it offers both computational efficiency and versatility, making
it highly attractive. However, these methods present two main limitations: (1)
bluntly suppressing language priors can compromise coherence and accuracy of
generated content, and (2) processing contrastive inputs adds computational
load, significantly slowing inference speed. To address these challenges, we
propose Visual Amplification Fusion (VAF), a plug-and-play technique that
enhances attention to visual signals within the model's middle layers, where
modality fusion predominantly occurs. This approach enables more effective
capture of visual features, reducing the model's bias toward language modality.
Experimental results demonstrate that VAF significantly reduces hallucinations
across various MLLMs without affecting inference speed, while maintaining
coherence and accuracy in generated outputs.
</details>

### [Lifting the Veil on Visual Information Flow in MLLMs: Unlocking Pathways to Faster Inference](https://arxiv.org/abs/2503.13108)
*Hao Yin,Guangzong Si,Zilei Wang*
<details>
  <summary>Abstract</summary>
Multimodal large language models (MLLMs) improve performance on
vision-language tasks by integrating visual features from pre-trained vision
encoders into large language models (LLMs). However, how MLLMs process and
utilize visual information remains unclear. In this paper, a shift in the
dominant flow of visual information is uncovered: (1) in shallow layers, strong
interactions are observed between image tokens and instruction tokens, where
most visual information is injected into instruction tokens to form cross-modal
semantic representations; (2) in deeper layers, image tokens primarily interact
with each other, aggregating the remaining visual information to optimize
semantic representations within visual modality. Based on these insights, we
propose Hierarchical Modality-Aware Pruning (HiMAP), a plug-and-play inference
acceleration method that dynamically prunes image tokens at specific layers,
reducing computational costs by approximately 65% without sacrificing
performance. Our findings offer a new understanding of visual information
processing in MLLMs and provide a state-of-the-art solution for efficient
inference.
</details>

### [DTGBrepGen: A Novel B-rep Generative Model through Decoupling Topology and Geometry](https://arxiv.org/abs/2503.13110)
*Jing Li,Yihang Fu,Falai Chen*
<details>
  <summary>Abstract</summary>
Boundary representation (B-rep) of geometric models is a fundamental format
in Computer-Aided Design (CAD). However, automatically generating valid and
high-quality B-rep models remains challenging due to the complex
interdependence between the topology and geometry of the models. Existing
methods tend to prioritize geometric representation while giving insufficient
attention to topological constraints, making it difficult to maintain
structural validity and geometric accuracy. In this paper, we propose
DTGBrepGen, a novel topology-geometry decoupled framework for B-rep generation
that explicitly addresses both aspects. Our approach first generates valid
topological structures through a two-stage process that independently models
edge-face and edge-vertex adjacency relationships. Subsequently, we employ
Transformer-based diffusion models for sequential geometry generation,
progressively generating vertex coordinates, followed by edge geometries and
face geometries which are represented as B-splines. Extensive experiments on
diverse CAD datasets show that DTGBrepGen significantly outperforms existing
methods in both topological validity and geometric accuracy, achieving higher
validity rates and producing more diverse and realistic B-reps. Our code is
publicly available at https://github.com/jinli99/DTGBrepGen.
</details>

### [MM-Spatial: Exploring 3D Spatial Understanding in Multimodal LLMs](https://arxiv.org/abs/2503.13111)
*Erik Daxberger,Nina Wenzel,David Griffiths,Haiming Gang,Justin Lazarow,Gefen Kohavi,Kai Kang,Marcin Eichner,Yinfei Yang,Afshin Dehghan,Peter Grasch*
<details>
  <summary>Abstract</summary>
Multimodal large language models (MLLMs) excel at 2D visual understanding but
remain limited in their ability to reason about 3D space. In this work, we
leverage large-scale high-quality 3D scene data with open-set annotations to
introduce 1) a novel supervised fine-tuning dataset and 2) a new evaluation
benchmark, focused on indoor scenes. Our Cubify Anything VQA (CA-VQA) data
covers diverse spatial tasks including spatial relationship prediction, metric
size and distance estimation, and 3D grounding. We show that CA-VQA enables us
to train MM-Spatial, a strong generalist MLLM that also achieves
state-of-the-art performance on 3D spatial understanding benchmarks, including
our own. We show how incorporating metric depth and multi-view inputs (provided
in CA-VQA) can further improve 3D understanding, and demonstrate that data
alone allows our model to achieve depth perception capabilities comparable to
dedicated monocular depth estimation models. We will publish our SFT dataset
and benchmark.
</details>

### [3D Human Interaction Generation: A Survey](https://arxiv.org/abs/2503.13120)
*Siyuan Fan,Wenke Huang,Xiantao Cai,Bo Du*
<details>
  <summary>Abstract</summary>
3D human interaction generation has emerged as a key research area, focusing
on producing dynamic and contextually relevant interactions between humans and
various interactive entities. Recent rapid advancements in 3D model
representation methods, motion capture technologies, and generative models have
laid a solid foundation for the growing interest in this domain. Existing
research in this field can be broadly categorized into three areas: human-scene
interaction, human-object interaction, and human-human interaction. Despite the
rapid advancements in this area, challenges remain due to the need for
naturalness in human motion generation and the accurate interaction between
humans and interactive entities. In this survey, we present a comprehensive
literature review of human interaction generation, which, to the best of our
knowledge, is the first of its kind. We begin by introducing the foundational
technologies, including model representations, motion capture methods, and
generative models. Subsequently, we introduce the approaches proposed for the
three sub-tasks, along with their corresponding datasets and evaluation
metrics. Finally, we discuss potential future research directions in this area
and conclude the survey. Through this survey, we aim to offer a comprehensive
overview of the current advancements in the field, highlight key challenges,
and inspire future research works.
</details>

### [Non-Destructive Detection of Sub-Micron Imperceptible Scratches On Laser Chips Based On Consistent Texture Entropy Recursive Optimization Semi-Supervised Network](https://arxiv.org/abs/2503.13125)
*Pan Liu*
<details>
  <summary>Abstract</summary>
Laser chips, the core components of semiconductor lasers, are extensively
utilized in various industries, showing great potential for future application.
Smoothness emitting surfaces are crucial in chip production, as even
imperceptible scratches can significantly degrade performance and lifespan,
thus impeding production efficiency and yield. Therefore, non-destructively
detecting these imperceptible scratches on the emitting surfaces is essential
for enhancing yield and reducing costs. These sub-micron level scratches,
barely visible against the background, are extremely difficult to detect with
conventional methods, compounded by a lack of labeled datasets. To address this
challenge, this paper introduces TexRecNet, a consistent texture entropy
recursive optimization semi-supervised network. The network, based on a
recursive optimization architecture, iteratively improves the detection
accuracy of imperceptible scratch edges, using outputs from previous cycles to
inform subsequent inputs and guide the network's positional encoding. It also
introduces image texture entropy, utilizing a substantial amount of unlabeled
data to expand the training set while maintaining training signal reliability.
Ultimately, by analyzing the inconsistency of the network output sequences
obtained during the recursive process, a semi-supervised training strategy with
recursive consistency constraints is proposed, using outputs from the recursive
process for non-destructive signal augmentation and consistently optimizes the
loss function for efficient end-to-end training. Experimental results show that
this method, utilizing a substantial amount of unsupervised data, achieves
75.6% accuracy and 74.8% recall in detecting imperceptible scratches, an 8.5%
and 33.6% improvement over conventional Unet, enhancing quality control in
laser chips.
</details>

### [ChainHOI: Joint-based Kinematic Chain Modeling for Human-Object Interaction Generation](https://arxiv.org/abs/2503.13130)
*Ling-An Zeng,Guohong Huang,Yi-Lin Wei,Shengbo Gu,Yu-Ming Tang,Jingke Meng,Wei-Shi Zheng*
<details>
  <summary>Abstract</summary>
We propose ChainHOI, a novel approach for text-driven human-object
interaction (HOI) generation that explicitly models interactions at both the
joint and kinetic chain levels. Unlike existing methods that implicitly model
interactions using full-body poses as tokens, we argue that explicitly modeling
joint-level interactions is more natural and effective for generating realistic
HOIs, as it directly captures the geometric and semantic relationships between
joints, rather than modeling interactions in the latent pose space. To this
end, ChainHOI introduces a novel joint graph to capture potential interactions
with objects, and a Generative Spatiotemporal Graph Convolution Network to
explicitly model interactions at the joint level. Furthermore, we propose a
Kinematics-based Interaction Module that explicitly models interactions at the
kinetic chain level, ensuring more realistic and biomechanically coherent
motions. Evaluations on two public datasets demonstrate that ChainHOI
significantly outperforms previous methods, generating more realistic, and
semantically consistent HOIs. Code is available
\href{https://github.com/qinghuannn/ChainHOI}{here}.
</details>

### [Patient-specific radiomic feature selection with reconstructed healthy persona of knee MR images](https://arxiv.org/abs/2503.13131)
*Yaxi Chen,Simin Ni,Aleksandra Ivanova,Shaheer U. Saeed,Rikin Hargunani,Jie Huang,Chaozong Liu,Yipeng Hu*
<details>
  <summary>Abstract</summary>
Classical radiomic features have been designed to describe image appearance
and intensity patterns. These features are directly interpretable and readily
understood by radiologists. Compared with end-to-end deep learning (DL) models,
lower dimensional parametric models that use such radiomic features offer
enhanced interpretability but lower comparative performance in clinical tasks.
In this study, we propose an approach where a standard logistic regression
model performance is substantially improved by learning to select radiomic
features for individual patients, from a pool of candidate features. This
approach has potentials to maintain the interpretability of such approaches
while offering comparable performance to DL. We also propose to expand the
feature pool by generating a patient-specific healthy persona via
mask-inpainting using a denoising diffusion model trained on healthy subjects.
Such a pathology-free baseline feature set allows further opportunity in novel
feature discovery and improved condition classification. We demonstrate our
method on multiple clinical tasks of classifying general abnormalities,
anterior cruciate ligament tears, and meniscus tears. Experimental results
demonstrate that our approach achieved comparable or even superior performance
than state-of-the-art DL approaches while offering added interpretability by
using radiomic features extracted from images and supplemented by generating
healthy personas. Example clinical cases are discussed in-depth to demonstrate
the intepretability-enabled utilities such as human-explainable feature
discovery and patient-specific location/view selection. These findings
highlight the potentials of the combination of subject-specific feature
selection with generative models in augmenting radiomic analysis for more
interpretable decision-making. The codes are available at:
https://github.com/YaxiiC/RadiomicsPersona.git
</details>

### [Enhancing zero-shot learning in medical imaging: integrating clip with advanced techniques for improved chest x-ray analysis](https://arxiv.org/abs/2503.13134)
*Prakhar Bhardwaj,Sheethal Bhat,Andreas Maier*
<details>
  <summary>Abstract</summary>
Due to the large volume of medical imaging data, advanced AI methodologies
are needed to assist radiologists in diagnosing thoracic diseases from chest
X-rays (CXRs). Existing deep learning models often require large, labeled
datasets, which are scarce in medical imaging due to the time-consuming and
expert-driven annotation process. In this paper, we extend the existing
approach to enhance zero-shot learning in medical imaging by integrating
Contrastive Language-Image Pre-training (CLIP) with Momentum Contrast (MoCo),
resulting in our proposed model, MoCoCLIP. Our method addresses challenges
posed by class-imbalanced and unlabeled datasets, enabling improved detection
of pulmonary pathologies. Experimental results on the NIH ChestXray14 dataset
demonstrate that MoCoCLIP outperforms the state-of-the-art CheXZero model,
achieving relative improvement of approximately 6.5%. Furthermore, on the
CheXpert dataset, MoCoCLIP demonstrates superior zero-shot performance,
achieving an average AUC of 0.750 compared to CheXZero with 0.746 AUC,
highlighting its enhanced generalization capabilities on unseen data.
</details>

### [Logic-in-Frames: Dynamic Keyframe Search via Visual Semantic-Logical Verification for Long Video Understanding](https://arxiv.org/abs/2503.13139)
*Weiyu Guo,Ziyang Chen,Shaoguang Wang,Jianxiang He,Yijie Xu,Jinhui Ye,Ying Sun,Hui Xiong*
<details>
  <summary>Abstract</summary>
Understanding long video content is a complex endeavor that often relies on
densely sampled frame captions or end-to-end feature selectors, yet these
techniques commonly overlook the logical relationships between textual queries
and visual elements. In practice, computational constraints necessitate coarse
frame subsampling, a challenge analogous to ``finding a needle in a haystack.''
To address this issue, we introduce a semantics-driven search framework that
reformulates keyframe selection under the paradigm of Visual Semantic-Logical
Search. Specifically, we systematically define four fundamental logical
dependencies: 1) spatial co-occurrence, 2) temporal proximity, 3) attribute
dependency, and 4) causal order. These relations dynamically update frame
sampling distributions through an iterative refinement process, enabling
context-aware identification of semantically critical frames tailored to
specific query requirements. Our method establishes new SOTA performance on the
manually annotated benchmark in key-frame selection metrics. Furthermore, when
applied to downstream video question-answering tasks, the proposed approach
demonstrates the best performance gains over existing methods on LongVideoBench
and Video-MME, validating its effectiveness in bridging the logical gap between
textual queries and visual-temporal reasoning. The code will be publicly
available.
</details>

### [Iterative Predictor-Critic Code Decoding for Real-World Image Dehazing](https://arxiv.org/abs/2503.13147)
*Jiayi Fu,Siyu Liu,Zikun Liu,Chun-Le Guo,Hyunhee Park,Ruiqi Wu,Guoqing Wang,Chongyi Li*
<details>
  <summary>Abstract</summary>
We propose a novel Iterative Predictor-Critic Code Decoding framework for
real-world image dehazing, abbreviated as IPC-Dehaze, which leverages the
high-quality codebook prior encapsulated in a pre-trained VQGAN. Apart from
previous codebook-based methods that rely on one-shot decoding, our method
utilizes high-quality codes obtained in the previous iteration to guide the
prediction of the Code-Predictor in the subsequent iteration, improving code
prediction accuracy and ensuring stable dehazing performance. Our idea stems
from the observations that 1) the degradation of hazy images varies with haze
density and scene depth, and 2) clear regions play crucial cues in restoring
dense haze regions. However, it is non-trivial to progressively refine the
obtained codes in subsequent iterations, owing to the difficulty in determining
which codes should be retained or replaced at each iteration. Another key
insight of our study is to propose Code-Critic to capture interrelations among
codes. The Code-Critic is used to evaluate code correlations and then resample
a set of codes with the highest mask scores, i.e., a higher score indicates
that the code is more likely to be rejected, which helps retain more accurate
codes and predict difficult ones. Extensive experiments demonstrate the
superiority of our method over state-of-the-art methods in real-world dehazing.
</details>

### [DynSTG-Mamba: Dynamic Spatio-Temporal Graph Mamba with Cross-Graph Knowledge Distillation for Gait Disorders Recognition](https://arxiv.org/abs/2503.13156)
*Zakariae Zrimek,Youssef Mourchid,Mohammed El Hassouni*
<details>
  <summary>Abstract</summary>
Gait disorder recognition plays a crucial role in the early diagnosis and
monitoring of movement disorders. Existing approaches, including
spatio-temporal graph convolutional networks (ST-GCNs), often face high memory
demands and struggle to capture complex spatio-temporal dependencies, limiting
their efficiency in clinical applications. To address these challenges, we
introduce DynSTG-Mamba (Dynamic Spatio-Temporal Graph Mamba), a novel framework
that combines DF-STGNN and STG-Mamba to enhance motion sequence modeling. The
DF-STGNN incorporates a dynamic spatio-temporal filter that adaptively adjusts
spatial connections between skeletal joints and temporal interactions across
different movement phases. This approach ensures better feature propagation
through dynamic graph structures by considering the hierarchical nature and
dynamics of skeletal gait data. Meanwhile, STG-Mamba, an extension of Mamba
adapted for skeletal motion data, ensures a continuous propagation of states,
facilitating the capture of long-term dependencies while reducing computational
complexity. To reduce the number of model parameters and computational costs
while maintaining consistency, we propose Cross-Graph Relational Knowledge
Distillation, a novel knowledge transfer mechanism that aligns relational
information between teacher (large architecture) and student models (small
architecture) while using shared memory. This ensures that the interactions and
movement patterns of the joints are accurately preserved in the motion
sequences. We validate our DynSTG-Mamba on KOA-NM, PD-WALK, and ATAXIA
datasets, where it outperforms state-of-the-art approaches by achieving in
terms of Accuracy, F1-score, and Recall. Our results highlight the efficiency
and robustness of our approach, offering a lightweight yet highly accurate
solution for automated gait analysis and movement disorder assessment.
</details>

### [Language-guided Open-world Video Anomaly Detection](https://arxiv.org/abs/2503.13160)
*Zihao Liu,Xiaoyu Wu,Jianqin Wu,Xuxu Wang,Linlin Yang*
<details>
  <summary>Abstract</summary>
Video anomaly detection models aim to detect anomalies that deviate from what
is expected. In open-world scenarios, the expected events may change as
requirements change. For example, not wearing a mask is considered abnormal
during a flu outbreak but normal otherwise. However, existing methods assume
that the definition of anomalies is invariable, and thus are not applicable to
the open world. To address this, we propose a novel open-world VAD paradigm
with variable definitions, allowing guided detection through user-provided
natural language at inference time. This paradigm necessitates establishing a
robust mapping from video and textual definition to anomaly score. Therefore,
we propose LaGoVAD (Language-guided Open-world VAD), a model that dynamically
adapts anomaly definitions through two regularization strategies: diversifying
the relative durations of anomalies via dynamic video synthesis, and enhancing
feature robustness through contrastive learning with negative mining. Training
such adaptable models requires diverse anomaly definitions, but existing
datasets typically provide given labels without semantic descriptions. To
bridge this gap, we collect PreVAD (Pre-training Video Anomaly Dataset), the
largest and most diverse video anomaly dataset to date, featuring 35,279
annotated videos with multi-level category labels and descriptions that
explicitly define anomalies. Zero-shot experiments on seven datasets
demonstrate SOTA performance. Data and code will be released.
</details>

### [Beyond RGB: Adaptive Parallel Processing for RAW Object Detection](https://arxiv.org/abs/2503.13163)
*Shani Gamrian,Hila Barel,Feiran Li,Masakazu Yoshimura,Daisuke Iso*
<details>
  <summary>Abstract</summary>
Object detection models are typically applied to standard RGB images
processed through Image Signal Processing (ISP) pipelines, which are designed
to enhance sensor-captured RAW images for human vision. However, these ISP
functions can lead to a loss of critical information that may be essential in
optimizing for computer vision tasks, such as object detection. In this work,
we introduce Raw Adaptation Module (RAM), a module designed to replace the
traditional ISP, with parameters optimized specifically for RAW object
detection. Inspired by the parallel processing mechanisms of the human visual
system, RAM departs from existing learned ISP methods by applying multiple ISP
functions in parallel rather than sequentially, allowing for a more
comprehensive capture of image features. These processed representations are
then fused in a specialized module, which dynamically integrates and optimizes
the information for the target task. This novel approach not only leverages the
full potential of RAW sensor data but also enables task-specific
pre-processing, resulting in superior object detection performance. Our
approach outperforms RGB-based methods and achieves state-of-the-art results
across diverse RAW image datasets under varying lighting conditions and dynamic
ranges.
</details>

### [From Zero to Detail: Deconstructing Ultra-High-Definition Image Restoration from Progressive Spectral Perspective](https://arxiv.org/abs/2503.13165)
*Chen Zhao,Zhizhou Chen,Yunzhe Xu,Enxuan Gu,Jian Li,Zili Yi,Qian Wang,Jian Yang,Ying Tai*
<details>
  <summary>Abstract</summary>
Ultra-high-definition (UHD) image restoration faces significant challenges
due to its high resolution, complex content, and intricate details. To cope
with these challenges, we analyze the restoration process in depth through a
progressive spectral perspective, and deconstruct the complex UHD restoration
problem into three progressive stages: zero-frequency enhancement,
low-frequency restoration, and high-frequency refinement. Building on this
insight, we propose a novel framework, ERR, which comprises three collaborative
sub-networks: the zero-frequency enhancer (ZFE), the low-frequency restorer
(LFR), and the high-frequency refiner (HFR). Specifically, the ZFE integrates
global priors to learn global mapping, while the LFR restores low-frequency
information, emphasizing reconstruction of coarse-grained content. Finally, the
HFR employs our designed frequency-windowed kolmogorov-arnold networks (FW-KAN)
to refine textures and details, producing high-quality image restoration. Our
approach significantly outperforms previous UHD methods across various tasks,
with extensive ablation studies validating the effectiveness of each component.
The code is available at \href{https://github.com/NJU-PCALab/ERR}{here}.
</details>

### [DeGauss: Dynamic-Static Decomposition with Gaussian Splatting for Distractor-free 3D Reconstruction](https://arxiv.org/abs/2503.13176)
*Rui Wang,Quentin Lohmeyer,Mirko Meboldt,Siyu Tang*
<details>
  <summary>Abstract</summary>
Reconstructing clean, distractor-free 3D scenes from real-world captures
remains a significant challenge, particularly in highly dynamic and cluttered
settings such as egocentric videos. To tackle this problem, we introduce
DeGauss, a simple and robust self-supervised framework for dynamic scene
reconstruction based on a decoupled dynamic-static Gaussian Splatting design.
DeGauss models dynamic elements with foreground Gaussians and static content
with background Gaussians, using a probabilistic mask to coordinate their
composition and enable independent yet complementary optimization. DeGauss
generalizes robustly across a wide range of real-world scenarios, from casual
image collections to long, dynamic egocentric videos, without relying on
complex heuristics or extensive supervision. Experiments on benchmarks
including NeRF-on-the-go, ADT, AEA, Hot3D, and EPIC-Fields demonstrate that
DeGauss consistently outperforms existing methods, establishing a strong
baseline for generalizable, distractor-free 3D reconstructionin highly dynamic,
interaction-rich environments.
</details>

### [A super-resolution reconstruction method for lightweight building images based on an expanding feature modulation network](https://arxiv.org/abs/2503.13179)
*Yi Zhang,Wenye Zhou,Ruonan Lin*
<details>
  <summary>Abstract</summary>
This study proposes a lightweight method for building image super-resolution
using a Dilated Contextual Feature Modulation Network (DCFMN). The process
includes obtaining high-resolution images, down-sampling them to
low-resolution, enhancing the low-resolution images, constructing and training
a lightweight network model, and generating super-resolution outputs. To
address challenges such as regular textures and long-range dependencies in
building images, the DCFMN integrates an expansion separable modulation unit
and a local feature enhancement module. The former employs multiple expansion
convolutions equivalent to a large kernel to efficiently aggregate multi-scale
features while leveraging a simple attention mechanism for adaptivity. The
latter encodes local features, mixes channel information, and ensures no
additional computational burden during inference through reparameterization.
This approach effectively resolves the limitations of existing lightweight
super-resolution networks in modeling long-range dependencies, achieving
accurate and efficient global feature modeling without increasing computational
costs, and significantly improving both reconstruction quality and lightweight
efficiency for building image super-resolution models.
</details>

### [Triad: Empowering LMM-based Anomaly Detection with Vision Expert-guided Visual Tokenizer and Manufacturing Process](https://arxiv.org/abs/2503.13184)
*Yuanze Li,Shihao Yuan,Haolin Wang,Qizhang Li,Ming Liu,Chen Xu,Guangming Shi,Wangmeng Zuo*
<details>
  <summary>Abstract</summary>
Although recent methods have tried to introduce large multimodal models
(LMMs) into industrial anomaly detection (IAD), their generalization in the IAD
field is far inferior to that for general purposes. We summarize the main
reasons for this gap into two aspects. On one hand, general-purpose LMMs lack
cognition of defects in the visual modality, thereby failing to sufficiently
focus on defect areas. Therefore, we propose to modify the AnyRes structure of
the LLaVA model, providing the potential anomalous areas identified by existing
IAD models to the LMMs. On the other hand, existing methods mainly focus on
identifying defects by learning defect patterns or comparing with normal
samples, yet they fall short of understanding the causes of these defects.
Considering that the generation of defects is closely related to the
manufacturing process, we propose a manufacturing-driven IAD paradigm. An
instruction-tuning dataset for IAD (InstructIAD) and a data organization
approach for Chain-of-Thought with manufacturing (CoT-M) are designed to
leverage the manufacturing process for IAD. Based on the above two
modifications, we present Triad, a novel LMM-based method incorporating an
expert-guided region-of-interest tokenizer and manufacturing process for
industrial anomaly detection. Extensive experiments show that our Triad not
only demonstrates competitive performance against current LMMs but also
achieves further improved accuracy when equipped with manufacturing processes.
Source code, training data, and pre-trained models will be publicly available
at https://github.com/tzjtatata/Triad.
</details>

### [3DAxisPrompt: Promoting the 3D Grounding and Reasoning in GPT-4o](https://arxiv.org/abs/2503.13185)
*Dingning Liu,Cheng Wang,Peng Gao,Renrui Zhang,Xinzhu Ma,Yuan Meng,Zhihui Wang*
<details>
  <summary>Abstract</summary>
Multimodal Large Language Models (MLLMs) exhibit impressive capabilities
across a variety of tasks, especially when equipped with carefully designed
visual prompts. However, existing studies primarily focus on logical reasoning
and visual understanding, while the capability of MLLMs to operate effectively
in 3D vision remains an ongoing area of exploration. In this paper, we
introduce a novel visual prompting method, called 3DAxisPrompt, to elicit the
3D understanding capabilities of MLLMs in real-world scenes. More specifically,
our method leverages the 3D coordinate axis and masks generated from the
Segment Anything Model (SAM) to provide explicit geometric priors to MLLMs and
then extend their impressive 2D grounding and reasoning ability to real-world
3D scenarios. Besides, we first provide a thorough investigation of the
potential visual prompting formats and conclude our findings to reveal the
potential and limits of 3D understanding capabilities in GPT-4o, as a
representative of MLLMs. Finally, we build evaluation environments with four
datasets, i.e., ScanRefer, ScanNet, FMB, and nuScene datasets, covering various
3D tasks. Based on this, we conduct extensive quantitative and qualitative
experiments, which demonstrate the effectiveness of the proposed method.
Overall, our study reveals that MLLMs, with the help of 3DAxisPrompt, can
effectively perceive an object's 3D position in real-world scenarios.
Nevertheless, a single prompt engineering approach does not consistently
achieve the best outcomes for all 3D tasks. This study highlights the
feasibility of leveraging MLLMs for 3D vision grounding/reasoning with prompt
engineering techniques.
</details>

### [3D Hierarchical Panoptic Segmentation in Real Orchard Environments Across Different Sensors](https://arxiv.org/abs/2503.13188)
*Matteo Sodano,Federico Magistri,Elias Marks,Fares Hosn,Aibek Zurbayev,Rodrigo Marcuzzi,Meher V. R. Malladi,Jens Behley,Cyrill Stachniss*
<details>
  <summary>Abstract</summary>
Crop yield estimation is a relevant problem in agriculture, because an
accurate crop yield estimate can support farmers' decisions on harvesting or
precision intervention. Robots can help to automate this process. To do so,
they need to be able to perceive the surrounding environment to identify target
objects. In this paper, we introduce a novel approach to address the problem of
hierarchical panoptic segmentation of apple orchards on 3D data from different
sensors. Our approach is able to simultaneously provide semantic segmentation,
instance segmentation of trunks and fruits, and instance segmentation of plants
(a single trunk with its fruits). This allows us to identify relevant
information such as individual plants, fruits, and trunks, and capture the
relationship among them, such as precisely estimate the number of fruits
associated to each tree in an orchard. Additionally, to efficiently evaluate
our approach for hierarchical panoptic segmentation, we provide a dataset
designed specifically for this task. Our dataset is recorded in Bonn in a real
apple orchard with a variety of sensors, spanning from a terrestrial laser
scanner to a RGB-D camera mounted on different robotic platforms. The
experiments show that our approach surpasses state-of-the-art approaches in 3D
panoptic segmentation in the agricultural domain, while also providing full
hierarchical panoptic segmentation. Our dataset has been made publicly
available at https://www.ipb.uni-bonn.de/data/hops/. We will provide the
open-source implementation of our approach and public competiton for
hierarchical panoptic segmentation on the hidden test sets upon paper
acceptance.
</details>

### [Clustering is back: Reaching state-of-the-art LiDAR instance segmentation without training](https://arxiv.org/abs/2503.13203)
*Corentin Sautier,Gilles Puy,Alexandre Boulch,Renaud Marlet,Vincent Lepetit*
<details>
  <summary>Abstract</summary>
Panoptic segmentation of LiDAR point clouds is fundamental to outdoor scene
understanding, with autonomous driving being a primary application. While
state-of-the-art approaches typically rely on end-to-end deep learning
architectures and extensive manual annotations of instances, the significant
cost and time investment required for labeling large-scale point cloud datasets
remains a major bottleneck in this field. In this work, we demonstrate that
competitive panoptic segmentation can be achieved using only semantic labels,
with instances predicted without any training or annotations. Our method
achieves performance comparable to current state-of-the-art supervised methods
on standard benchmarks including SemanticKITTI and nuScenes, and outperforms
every publicly available method on SemanticKITTI as a drop-in instance head
replacement, while running in real-time on a single-threaded CPU and requiring
no instance labels. Our method is fully explainable, and requires no learning
or parameter tuning. Code is available at https://github.com/valeoai/Alpine/
</details>

### [MedLoRD: A Medical Low-Resource Diffusion Model for High-Resolution 3D CT Image Synthesis](https://arxiv.org/abs/2503.13211)
*Marvin Seyfarth,Salman Ul Hassan Dar,Isabelle Ayx,Matthias Alexander Fink,Stefan O. Schoenberg,Hans-Ulrich Kauczor,Sandy Engelhardt*
<details>
  <summary>Abstract</summary>
Advancements in AI for medical imaging offer significant potential. However,
their applications are constrained by the limited availability of data and the
reluctance of medical centers to share it due to patient privacy concerns.
Generative models present a promising solution by creating synthetic data as a
substitute for real patient data. However, medical images are typically
high-dimensional, and current state-of-the-art methods are often impractical
for computational resource-constrained healthcare environments. These models
rely on data sub-sampling, raising doubts about their feasibility and
real-world applicability. Furthermore, many of these models are evaluated on
quantitative metrics that alone can be misleading in assessing the image
quality and clinical meaningfulness of the generated images. To address this,
we introduce MedLoRD, a generative diffusion model designed for computational
resource-constrained environments. MedLoRD is capable of generating
high-dimensional medical volumes with resolutions up to
512$\times$512$\times$256, utilizing GPUs with only 24GB VRAM, which are
commonly found in standard desktop workstations. MedLoRD is evaluated across
multiple modalities, including Coronary Computed Tomography Angiography and
Lung Computed Tomography datasets. Extensive evaluations through radiological
evaluation, relative regional volume analysis, adherence to conditional masks,
and downstream tasks show that MedLoRD generates high-fidelity images closely
adhering to segmentation mask conditions, surpassing the capabilities of
current state-of-the-art generative models for medical image synthesis in
computational resource-constrained environments.
</details>

### [A General Adaptive Dual-level Weighting Mechanism for Remote Sensing Pansharpening](https://arxiv.org/abs/2503.13214)
*Jie Huang,Haorui Chen,Jiaxuan Ren,Siran Peng,Liangjian Deng*
<details>
  <summary>Abstract</summary>
Currently, deep learning-based methods for remote sensing pansharpening have
advanced rapidly. However, many existing methods struggle to fully leverage
feature heterogeneity and redundancy, thereby limiting their effectiveness. We
use the covariance matrix to model the feature heterogeneity and redundancy and
propose Correlation-Aware Covariance Weighting (CACW) to adjust them. CACW
captures these correlations through the covariance matrix, which is then
processed by a nonlinear function to generate weights for adjustment. Building
upon CACW, we introduce a general adaptive dual-level weighting mechanism
(ADWM) to address these challenges from two key perspectives, enhancing a wide
range of existing deep-learning methods. First, Intra-Feature Weighting (IFW)
evaluates correlations among channels within each feature to reduce redundancy
and enhance unique information. Second, Cross-Feature Weighting (CFW) adjusts
contributions across layers based on inter-layer correlations, refining the
final output. Extensive experiments demonstrate the superior performance of
ADWM compared to recent state-of-the-art (SOTA) methods. Furthermore, we
validate the effectiveness of our approach through generality experiments,
redundancy visualization, comparison experiments, key variables and complexity
analysis, and ablation studies. Our code is available at
https://github.com/Jie-1203/ADWM.
</details>

### [HoloGest: Decoupled Diffusion and Motion Priors for Generating Holisticly Expressive Co-speech Gestures](https://arxiv.org/abs/2503.13229)
*Yongkang Cheng,Shaoli Huang*
<details>
  <summary>Abstract</summary>
Animating virtual characters with holistic co-speech gestures is a
challenging but critical task. Previous systems have primarily focused on the
weak correlation between audio and gestures, leading to physically unnatural
outcomes that degrade the user experience. To address this problem, we
introduce HoleGest, a novel neural network framework based on decoupled
diffusion and motion priors for the automatic generation of high-quality,
expressive co-speech gestures. Our system leverages large-scale human motion
datasets to learn a robust prior with low audio dependency and high motion
reliance, enabling stable global motion and detailed finger movements. To
improve the generation efficiency of diffusion-based models, we integrate
implicit joint constraints with explicit geometric and conditional constraints,
capturing complex motion distributions between large strides. This integration
significantly enhances generation speed while maintaining high-quality motion.
Furthermore, we design a shared embedding space for gesture-transcription text
alignment, enabling the generation of semantically correct gesture actions.
Extensive experiments and user feedback demonstrate the effectiveness and
potential applications of our model, with our method achieving a level of
realism close to the ground truth, providing an immersive user experience. Our
code, model, and demo are are available at
https://cyk990422.github.io/HoloGest.github.io/.
</details>

### [Sampling Innovation-Based Adaptive Compressive Sensing](https://arxiv.org/abs/2503.13241)
*Zhifu Tian,Tao Hu,Chaoyang Niu,Di Wu,Shu Wang*
<details>
  <summary>Abstract</summary>
Scene-aware Adaptive Compressive Sensing (ACS) has attracted significant
interest due to its promising capability for efficient and high-fidelity
acquisition of scene images. ACS typically prescribes adaptive sampling
allocation (ASA) based on previous samples in the absence of ground truth.
However, when confronting unknown scenes, existing ACS methods often lack
accurate judgment and robust feedback mechanisms for ASA, thus limiting the
high-fidelity sensing of the scene. In this paper, we introduce a Sampling
Innovation-Based ACS (SIB-ACS) method that can effectively identify and
allocate sampling to challenging image reconstruction areas, culminating in
high-fidelity image reconstruction. An innovation criterion is proposed to
judge ASA by predicting the decrease in image reconstruction error attributable
to sampling increments, thereby directing more samples towards regions where
the reconstruction error diminishes significantly. A sampling innovation-guided
multi-stage adaptive sampling (AS) framework is proposed, which iteratively
refines the ASA through a multi-stage feedback process. For image
reconstruction, we propose a Principal Component Compressed Domain Network
(PCCD-Net), which efficiently and faithfully reconstructs images under AS
scenarios. Extensive experiments demonstrate that the proposed SIB-ACS method
significantly outperforms the state-of-the-art methods in terms of image
reconstruction fidelity and visual effects. Codes are available at
https://github.com/giant-pandada/SIB-ACS_CVPR2025.
</details>

### [Don't Judge Before You CLIP: A Unified Approach for Perceptual Tasks](https://arxiv.org/abs/2503.13260)
*Amit Zalcher,Navve Wasserman,Roman Beliy,Oliver Heinimann,Michal Irani*
<details>
  <summary>Abstract</summary>
Visual perceptual tasks aim to predict human judgment of images (e.g.,
emotions invoked by images, image quality assessment). Unlike objective tasks
such as object/scene recognition, perceptual tasks rely on subjective human
assessments, making its data-labeling difficult. The scarcity of such
human-annotated data results in small datasets leading to poor generalization.
Typically, specialized models were designed for each perceptual task, tailored
to its unique characteristics and its own training dataset. We propose a
unified architectural framework for solving multiple different perceptual tasks
leveraging CLIP as a prior. Our approach is based on recent cognitive findings
which indicate that CLIP correlates well with human judgment. While CLIP was
explicitly trained to align images and text, it implicitly also learned human
inclinations. We attribute this to the inclusion of human-written image
captions in CLIP's training data, which contain not only factual image
descriptions, but inevitably also human sentiments and emotions. This makes
CLIP a particularly strong prior for perceptual tasks. Accordingly, we suggest
that minimal adaptation of CLIP suffices for solving a variety of perceptual
tasks. Our simple unified framework employs a lightweight adaptation to
fine-tune CLIP to each task, without requiring any task-specific architectural
changes. We evaluate our approach on three tasks: (i) Image Memorability
Prediction, (ii) No-reference Image Quality Assessment, and (iii) Visual
Emotion Analysis. Our model achieves state-of-the-art results on all three
tasks, while demonstrating improved generalization across different datasets.
</details>

### [FlexWorld: Progressively Expanding 3D Scenes for Flexiable-View Synthesis](https://arxiv.org/abs/2503.13265)
*Luxi Chen,Zihan Zhou,Min Zhao,Yikai Wang,Ge Zhang,Wenhao Huang,Hao Sun,Ji-Rong Wen,Chongxuan Li*
<details>
  <summary>Abstract</summary>
Generating flexible-view 3D scenes, including 360{\deg} rotation and zooming,
from single images is challenging due to a lack of 3D data. To this end, we
introduce FlexWorld, a novel framework consisting of two key components: (1) a
strong video-to-video (V2V) diffusion model to generate high-quality novel view
images from incomplete input rendered from a coarse scene, and (2) a
progressive expansion process to construct a complete 3D scene. In particular,
leveraging an advanced pre-trained video model and accurate depth-estimated
training pairs, our V2V model can generate novel views under large camera pose
variations. Building upon it, FlexWorld progressively generates new 3D content
and integrates it into the global scene through geometry-aware scene fusion.
Extensive experiments demonstrate the effectiveness of FlexWorld in generating
high-quality novel view videos and flexible-view 3D scenes from single images,
achieving superior visual quality under multiple popular metrics and datasets
compared to existing state-of-the-art methods. Qualitatively, we highlight that
FlexWorld can generate high-fidelity scenes with flexible views like 360{\deg}
rotations and zooming. Project page: https://ml-gsai.github.io/FlexWorld.
</details>

### [Generative Gaussian Splatting: Generating 3D Scenes with Video Diffusion Priors](https://arxiv.org/abs/2503.13272)
*Katja Schwarz,Norman Mueller,Peter Kontschieder*
<details>
  <summary>Abstract</summary>
Synthesizing consistent and photorealistic 3D scenes is an open problem in
computer vision. Video diffusion models generate impressive videos but cannot
directly synthesize 3D representations, i.e., lack 3D consistency in the
generated sequences. In addition, directly training generative 3D models is
challenging due to a lack of 3D training data at scale. In this work, we
present Generative Gaussian Splatting (GGS) -- a novel approach that integrates
a 3D representation with a pre-trained latent video diffusion model.
Specifically, our model synthesizes a feature field parameterized via 3D
Gaussian primitives. The feature field is then either rendered to feature maps
and decoded into multi-view images, or directly upsampled into a 3D radiance
field. We evaluate our approach on two common benchmark datasets for scene
synthesis, RealEstate10K and ScanNet+, and find that our proposed GGS model
significantly improves both the 3D consistency of the generated multi-view
images, and the quality of the generated 3D scenes over all relevant baselines.
Compared to a similar model without 3D representation, GGS improves FID on the
generated 3D scenes by ~20% on both RealEstate10K and ScanNet+. Project page:
https://katjaschwarz.github.io/ggs/
</details>

### [Progressive Human Motion Generation Based on Text and Few Motion Frames](https://arxiv.org/abs/2503.13300)
*Ling-An Zeng,Gaojie Wu,Ancong Wu,Jian-Fang Hu,Wei-Shi Zheng*
<details>
  <summary>Abstract</summary>
Although existing text-to-motion (T2M) methods can produce realistic human
motion from text description, it is still difficult to align the generated
motion with the desired postures since using text alone is insufficient for
precisely describing diverse postures. To achieve more controllable generation,
an intuitive way is to allow the user to input a few motion frames describing
precise desired postures. Thus, we explore a new Text-Frame-to-Motion (TF2M)
generation task that aims to generate motions from text and very few given
frames. Intuitively, the closer a frame is to a given frame, the lower the
uncertainty of this frame is when conditioned on this given frame. Hence, we
propose a novel Progressive Motion Generation (PMG) method to progressively
generate a motion from the frames with low uncertainty to those with high
uncertainty in multiple stages. During each stage, new frames are generated by
a Text-Frame Guided Generator conditioned on frame-aware semantics of the text,
given frames, and frames generated in previous stages. Additionally, to
alleviate the train-test gap caused by multi-stage accumulation of incorrectly
generated frames during testing, we propose a Pseudo-frame Replacement Strategy
for training. Experimental results show that our PMG outperforms existing T2M
generation methods by a large margin with even one given frame, validating the
effectiveness of our PMG. Code will be released.
</details>

### [UniHOPE: A Unified Approach for Hand-Only and Hand-Object Pose Estimation](https://arxiv.org/abs/2503.13303)
*Yinqiao Wang,Hao Xu,Pheng-Ann Heng,Chi-Wing Fu*
<details>
  <summary>Abstract</summary>
Estimating the 3D pose of hand and potential hand-held object from monocular
images is a longstanding challenge. Yet, existing methods are specialized,
focusing on either bare-hand or hand interacting with object. No method can
flexibly handle both scenarios and their performance degrades when applied to
the other scenario. In this paper, we propose UniHOPE, a unified approach for
general 3D hand-object pose estimation, flexibly adapting both scenarios.
Technically, we design a grasp-aware feature fusion module to integrate
hand-object features with an object switcher to dynamically control the
hand-object pose estimation according to grasping status. Further, to uplift
the robustness of hand pose estimation regardless of object presence, we
generate realistic de-occluded image pairs to train the model to learn
object-induced hand occlusions, and formulate multi-level feature enhancement
techniques for learning occlusion-invariant features. Extensive experiments on
three commonly-used benchmarks demonstrate UniHOPE's SOTA performance in
addressing hand-only and hand-object scenarios. Code will be released on
https://github.com/JoyboyWang/UniHOPE_Pytorch.
</details>

### [MagicDistillation: Weak-to-Strong Video Distillation for Large-Scale Portrait Few-Step Synthesis](https://arxiv.org/abs/2503.13319)
*Shitong Shao,Hongwei Yi,Hanzhong Guo,Tian Ye,Daquan Zhou,Michael Lingelbach,Zhiqiang Xu,Zeke Xie*
<details>
  <summary>Abstract</summary>
Fine-tuning open-source large-scale VDMs for the portrait video synthesis
task can result in significant improvements across multiple dimensions, such as
visual quality and natural facial motion dynamics. Despite their advancements,
how to achieve step distillation and reduce the substantial computational
overhead of large-scale VDMs remains unexplored. To fill this gap, this paper
proposes Weak-to-Strong Video Distillation (W2SVD) to mitigate both the issue
of insufficient training memory and the problem of training collapse observed
in vanilla DMD during the training process. Specifically, we first leverage
LoRA to fine-tune the fake diffusion transformer (DiT) to address the
out-of-memory issue. Then, we employ the W2S distribution matching to adjust
the real DiT's parameter, subtly shifting it toward the fake DiT's parameter.
This adjustment is achieved by utilizing the weak weight of the low-rank
branch, effectively alleviate the conundrum where the video synthesized by the
few-step generator deviates from the real data distribution, leading to
inaccuracies in the KL divergence approximation. Additionally, we minimize the
distance between the fake data distribution and the ground truth distribution
to further enhance the visual quality of the synthesized videos. As
experimentally demonstrated on HunyuanVideo, W2SVD surpasses the standard
Euler, LCM, DMD and even the 28-step standard sampling in FID/FVD and VBench in
1/4-step video synthesis. The project page is in
https://w2svd.github.io/W2SVD/.
</details>

### [Edit Transfer: Learning Image Editing via Vision In-Context Relations](https://arxiv.org/abs/2503.13327)
*Lan Chen,Qi Mao,Yuchao Gu,Mike Zheng Shou*
<details>
  <summary>Abstract</summary>
We introduce a new setting, Edit Transfer, where a model learns a
transformation from just a single source-target example and applies it to a new
query image. While text-based methods excel at semantic manipulations through
textual prompts, they often struggle with precise geometric details (e.g.,
poses and viewpoint changes). Reference-based editing, on the other hand,
typically focuses on style or appearance and fails at non-rigid
transformations. By explicitly learning the editing transformation from a
source-target pair, Edit Transfer mitigates the limitations of both text-only
and appearance-centric references. Drawing inspiration from in-context learning
in large language models, we propose a visual relation in-context learning
paradigm, building upon a DiT-based text-to-image model. We arrange the edited
example and the query image into a unified four-panel composite, then apply
lightweight LoRA fine-tuning to capture complex spatial transformations from
minimal examples. Despite using only 42 training samples, Edit Transfer
substantially outperforms state-of-the-art TIE and RIE methods on diverse
non-rigid scenarios, demonstrating the effectiveness of few-shot visual
relation learning.
</details>

### [STEP: Simultaneous Tracking and Estimation of Pose for Animals and Humans](https://arxiv.org/abs/2503.13344)
*Shashikant Verma,Harish Katti,Soumyaratna Debnath,Yamuna Swamy,Shanmuganathan Raman*
<details>
  <summary>Abstract</summary>
We introduce STEP, a novel framework utilizing Transformer-based
discriminative model prediction for simultaneous tracking and estimation of
pose across diverse animal species and humans. We are inspired by the fact that
the human brain exploits spatiotemporal continuity and performs concurrent
localization and pose estimation despite the specialization of brain areas for
form and motion processing. Traditional discriminative models typically require
predefined target states for determining model weights, a challenge we address
through Gaussian Map Soft Prediction (GMSP) and Offset Map Regression Adapter
(OMRA) Modules. These modules remove the necessity of keypoint target states as
input, streamlining the process. Our method starts with a known target state
initialized through a pre-trained detector or manual initialization in the
initial frame of a given video sequence. It then seamlessly tracks the target
and estimates keypoints of anatomical importance as output for subsequent
frames. Unlike prevalent top-down pose estimation methods, our approach doesn't
rely on per-frame target detections due to its tracking capability. This
facilitates a significant advancement in inference efficiency and potential
applications. We train and validate our approach on datasets encompassing
diverse species. Our experiments demonstrate superior results compared to
existing methods, opening doors to various applications, including but not
limited to action recognition and behavioral analysis.
</details>

### [TriDF: Triplane-Accelerated Density Fields for Few-Shot Remote Sensing Novel View Synthesis](https://arxiv.org/abs/2503.13347)
*Jiaming Kang,Keyan Chen,Zhengxia Zou,Zhenwei Shi*
<details>
  <summary>Abstract</summary>
Remote sensing novel view synthesis (NVS) offers significant potential for 3D
interpretation of remote sensing scenes, with important applications in urban
planning and environmental monitoring. However, remote sensing scenes
frequently lack sufficient multi-view images due to acquisition constraints.
While existing NVS methods tend to overfit when processing limited input views,
advanced few-shot NVS methods are computationally intensive and perform
sub-optimally in remote sensing scenes. This paper presents TriDF, an efficient
hybrid 3D representation for fast remote sensing NVS from as few as 3 input
views. Our approach decouples color and volume density information, modeling
them independently to reduce the computational burden on implicit radiance
fields and accelerate reconstruction. We explore the potential of the triplane
representation in few-shot NVS tasks by mapping high-frequency color
information onto this compact structure, and the direct optimization of feature
planes significantly speeds up convergence. Volume density is modeled as
continuous density fields, incorporating reference features from neighboring
views through image-based rendering to compensate for limited input data.
Additionally, we introduce depth-guided optimization based on point clouds,
which effectively mitigates the overfitting problem in few-shot NVS.
Comprehensive experiments across multiple remote sensing scenes demonstrate
that our hybrid representation achieves a 30x speed increase compared to
NeRF-based methods, while simultaneously improving rendering quality metrics
over advanced few-shot methods (7.4% increase in PSNR, 12.2% in SSIM, and 18.7%
in LPIPS). The code is publicly available at https://github.com/kanehub/TriDF
</details>

### [Parameter-free structure-texture image decomposition by unrolling](https://arxiv.org/abs/2503.13354)
*Laura Girometti,Jean-Fran√ßois Aujol,Antoine Guennec,Yann Traonmilin*
<details>
  <summary>Abstract</summary>
In this work, we propose a parameter-free and efficient method to tackle the
structure-texture image decomposition problem. In particular, we present a
neural network LPR-NET based on the unrolling of the Low Patch Rank model. On
the one hand, this allows us to automatically learn parameters from data, and
on the other hand to be computationally faster while obtaining qualitatively
similar results compared to traditional iterative model-based methods.
Moreover, despite being trained on synthetic images, numerical experiments show
the ability of our network to generalize well when applied to natural images.
</details>

### [One-Step Residual Shifting Diffusion for Image Super-Resolution via Distillation](https://arxiv.org/abs/2503.13358)
*Daniil Selikhanovych,David Li,Aleksei Leonov,Nikita Gushchin,Sergei Kushneriuk,Alexander Filippov,Evgeny Burnaev,Iaroslav Koshelev,Alexander Korotin*
<details>
  <summary>Abstract</summary>
Diffusion models for super-resolution (SR) produce high-quality visual
results but require expensive computational costs. Despite the development of
several methods to accelerate diffusion-based SR models, some (e.g., SinSR)
fail to produce realistic perceptual details, while others (e.g., OSEDiff) may
hallucinate non-existent structures. To overcome these issues, we present RSD,
a new distillation method for ResShift, one of the top diffusion-based SR
models. Our method is based on training the student network to produce such
images that a new fake ResShift model trained on them will coincide with the
teacher model. RSD achieves single-step restoration and outperforms the teacher
by a large margin. We show that our distillation method can surpass the other
distillation-based method for ResShift - SinSR - making it on par with
state-of-the-art diffusion-based SR distillation methods. Compared to SR
methods based on pre-trained text-to-image models, RSD produces competitive
perceptual quality, provides images with better alignment to degraded input
images, and requires fewer parameters and GPU memory. We provide experimental
results on various real-world and synthetic datasets, including RealSR,
RealSet65, DRealSR, ImageNet, and DIV2K.
</details>

### [Mitigating Visual Forgetting via Take-along Visual Conditioning for Multi-modal Long CoT Reasoning](https://arxiv.org/abs/2503.13360)
*Hai-Long Sun,Zhun Sun,Houwen Peng,Han-Jia Ye*
<details>
  <summary>Abstract</summary>
Recent advancements in Large Language Models (LLMs) have demonstrated
enhanced reasoning capabilities, evolving from Chain-of-Thought (CoT) prompting
to advanced, product-oriented solutions like OpenAI o1. During our
re-implementation of this model, we noticed that in multimodal tasks requiring
visual input (e.g., geometry problems), Multimodal LLMs (MLLMs) struggle to
maintain focus on the visual information, in other words, MLLMs suffer from a
gradual decline in attention to visual information as reasoning progresses,
causing text-over-relied outputs. To investigate this, we ablate image inputs
during long-chain reasoning. Concretely, we truncate the reasoning process
midway, then re-complete the reasoning process with the input image removed. We
observe only a ~2% accuracy drop on MathVista's test-hard subset, revealing the
model's textual outputs dominate the following reasoning process. Motivated by
this, we propose Take-along Visual Conditioning (TVC), a strategy that shifts
image input to critical reasoning stages and compresses redundant visual tokens
via dynamic pruning. This methodology helps the model retain attention to the
visual components throughout the reasoning. Our approach achieves
state-of-the-art performance on average across five mathematical reasoning
benchmarks (+3.4% vs previous sota), demonstrating the effectiveness of TVC in
enhancing multimodal reasoning systems.
</details>

### [TimeZero: Temporal Video Grounding with Reasoning-Guided LVLM](https://arxiv.org/abs/2503.13377)
*Ye Wang,Boshen Xu,Zihao Yue,Zihan Xiao,Ziheng Wang,Liang Zhang,Dingyi Yang,Wenxuan Wang,Qin Jin*
<details>
  <summary>Abstract</summary>
We introduce TimeZero, a reasoning-guided LVLM designed for the temporal
video grounding (TVG) task. This task requires precisely localizing relevant
video segments within long videos based on a given language query. TimeZero
tackles this challenge by extending the inference process, enabling the model
to reason about video-language relationships solely through reinforcement
learning. To evaluate the effectiveness of TimeZero, we conduct experiments on
two benchmarks, where TimeZero achieves state-of-the-art performance on
Charades-STA. Code is available at https://github.com/www-Ye/TimeZero.
</details>

### [Cream of the Crop: Harvesting Rich, Scalable and Transferable Multi-Modal Data for Instruction Fine-Tuning](https://arxiv.org/abs/2503.13383)
*Mengyao Lyu,Yan Li,Huasong Zhong,Wenhao Yang,Hui Chen,Jungong Han,Guiguang Ding,Zhenheng Yang*
<details>
  <summary>Abstract</summary>
The hypothesis that pretrained large language models (LLMs) necessitate only
minimal supervision during the fine-tuning (SFT) stage (Zhou et al., 2024) has
been substantiated by recent advancements in data curation and selection
research. However, their stability and generalizability are compromised due to
the vulnerability to experimental setups and validation protocols, falling
short of surpassing random sampling (Diddee & Ippolito, 2024; Xia et al.,
2024b). Built upon LLMs, multi-modal LLMs (MLLMs), combined with the sheer
token volume and heightened heterogeneity of data sources, amplify both the
significance and complexity of data selection.
  To harvest multi-modal instructional data in a robust and efficient manner,
we re-define the granularity of the quality metric by decomposing it into 14
vision-language-related capabilities, and introduce multi-modal rich scorers to
evaluate the capabilities of each data candidate. To promote diversity, in
light of the inherent objective of the alignment stage, we take interaction
style as diversity indicator and use a multi-modal rich styler to identify data
instruction patterns. In doing so, our multi-modal rich scorers and styler
(mmSSR) guarantee that high-scoring information is conveyed to users in
diversified forms. Free from embedding-based clustering or greedy sampling,
mmSSR efficiently scales to millions of data with varying budget constraints,
supports customization for general or specific capability acquisition, and
facilitates training-free generalization to new domains for curation. Across
10+ experimental settings, validated by 14 multi-modal benchmarks, we
demonstrate consistent improvements over random sampling, baseline strategies
and state-of-the-art selection methods, achieving 99.1% of full performance
with only 30% of the 2.6M data.
</details>

### [Scale Efficient Training for Large Datasets](https://arxiv.org/abs/2503.13385)
*Qing Zhou,Junyu Gao,Qi Wang*
<details>
  <summary>Abstract</summary>
The rapid growth of dataset scales has been a key driver in advancing deep
learning research. However, as dataset scale increases, the training process
becomes increasingly inefficient due to the presence of low-value samples,
including excessive redundant samples, overly challenging samples, and
inefficient easy samples that contribute little to model improvement.To address
this challenge, we propose Scale Efficient Training (SeTa) for large datasets,
a dynamic sample pruning approach that losslessly reduces training time. To
remove low-value samples, SeTa first performs random pruning to eliminate
redundant samples, then clusters the remaining samples according to their
learning difficulty measured by loss. Building upon this clustering, a sliding
window strategy is employed to progressively remove both overly challenging and
inefficient easy clusters following an easy-to-hard curriculum.We conduct
extensive experiments on large-scale synthetic datasets, including ToCa, SS1M,
and ST+MJ, each containing over 3 million samples.SeTa reduces training costs
by up to 50\% while maintaining or improving performance, with minimal
degradation even at 70\% cost reduction. Furthermore, experiments on various
scale real datasets across various backbones (CNNs, Transformers, and Mambas)
and diverse tasks (instruction tuning, multi-view stereo, geo-localization,
composed image retrieval, referring image segmentation) demonstrate the
powerful effectiveness and universality of our approach. Code is available at
https://github.com/mrazhou/SeTa.
</details>

### [MicroVQA: A Multimodal Reasoning Benchmark for Microscopy-Based Scientific Research](https://arxiv.org/abs/2503.13399)
*James Burgess,Jeffrey J Nirschl,Laura Bravo-S√°nchez,Alejandro Lozano,Sanket Rajan Gupte,Jesus G. Galaz-Montoya,Yuhui Zhang,Yuchang Su,Disha Bhowmik,Zachary Coman,Sarina M. Hasan,Alexandra Johannesson,William D. Leineweber,Malvika G Nair,Ridhi Yarlagadda,Connor Zuraski,Wah Chiu,Sarah Cohen,Jan N. Hansen,Manuel D Leonetti,Chad Liu,Emma Lundberg,Serena Yeung-Levy*
<details>
  <summary>Abstract</summary>
Scientific research demands sophisticated reasoning over multimodal data, a
challenge especially prevalent in biology. Despite recent advances in
multimodal large language models (MLLMs) for AI-assisted research, existing
multimodal reasoning benchmarks only target up to college-level difficulty,
while research-level benchmarks emphasize lower-level perception, falling short
of the complex multimodal reasoning needed for scientific discovery. To bridge
this gap, we introduce MicroVQA, a visual-question answering (VQA) benchmark
designed to assess three reasoning capabilities vital in research workflows:
expert image understanding, hypothesis generation, and experiment proposal.
MicroVQA consists of 1,042 multiple-choice questions (MCQs) curated by biology
experts across diverse microscopy modalities, ensuring VQA samples represent
real scientific practice. In constructing the benchmark, we find that standard
MCQ generation methods induce language shortcuts, motivating a new two-stage
pipeline: an optimized LLM prompt structures question-answer pairs into MCQs;
then, an agent-based `RefineBot' updates them to remove shortcuts. Benchmarking
on state-of-the-art MLLMs reveal a peak performance of 53\%; models with
smaller LLMs only slightly underperform top models, suggesting that
language-based reasoning is less challenging than multimodal reasoning; and
tuning with scientific articles enhances performance. Expert analysis of
chain-of-thought responses shows that perception errors are the most frequent,
followed by knowledge errors and then overgeneralization errors. These insights
highlight the challenges in multimodal scientific reasoning, showing MicroVQA
is a valuable resource advancing AI-driven biomedical research. MicroVQA is
available at https://huggingface.co/datasets/jmhb/microvqa, and project page at
https://jmhb0.github.io/microvqa.
</details>

### [Infinite Mobility: Scalable High-Fidelity Synthesis of Articulated Objects via Procedural Generation](https://arxiv.org/abs/2503.13424)
*Xinyu Lian,Zichao Yu,Ruiming Liang,Yitong Wang,Li Ray Luo,Kaixu Chen,Yuanzhen Zhou,Qihong Tang,Xudong Xu,Zhaoyang Lyu,Bo Dai,Jiangmiao Pang*
<details>
  <summary>Abstract</summary>
Large-scale articulated objects with high quality are desperately needed for
multiple tasks related to embodied AI. Most existing methods for creating
articulated objects are either data-driven or simulation based, which are
limited by the scale and quality of the training data or the fidelity and heavy
labour of the simulation. In this paper, we propose Infinite Mobility, a novel
method for synthesizing high-fidelity articulated objects through procedural
generation. User study and quantitative evaluation demonstrate that our method
can produce results that excel current state-of-the-art methods and are
comparable to human-annotated datasets in both physics property and mesh
quality. Furthermore, we show that our synthetic data can be used as training
data for generative models, enabling next-step scaling up. Code is available at
https://github.com/Intern-Nexus/Infinite-Mobility
</details>

### [Escaping Plato's Cave: Robust Conceptual Reasoning through Interpretable 3D Neural Object Volumes](https://arxiv.org/abs/2503.13429)
*Nhi Pham,Bernt Schiele,Adam Kortylewski,Jonas Fischer*
<details>
  <summary>Abstract</summary>
With the rise of neural networks, especially in high-stakes applications,
these networks need two properties (i) robustness and (ii) interpretability to
ensure their safety. Recent advances in classifiers with 3D volumetric object
representations have demonstrated a greatly enhanced robustness in
out-of-distribution data. However, these 3D-aware classifiers have not been
studied from the perspective of interpretability. We introduce CAVE - Concept
Aware Volumes for Explanations - a new direction that unifies interpretability
and robustness in image classification. We design an inherently-interpretable
and robust classifier by extending existing 3D-aware classifiers with concepts
extracted from their volumetric representations for classification. In an array
of quantitative metrics for interpretability, we compare against different
concept-based approaches across the explainable AI literature and show that
CAVE discovers well-grounded concepts that are used consistently across images,
while achieving superior robustness.
</details>

### [AugMapNet: Improving Spatial Latent Structure via BEV Grid Augmentation for Enhanced Vectorized Online HD Map Construction](https://arxiv.org/abs/2503.13430)
*Thomas Monninger,Md Zafar Anwar,Stanislaw Antol,Steffen Staab,Sihao Ding*
<details>
  <summary>Abstract</summary>
Autonomous driving requires an understanding of the infrastructure elements,
such as lanes and crosswalks. To navigate safely, this understanding must be
derived from sensor data in real-time and needs to be represented in vectorized
form. Learned Bird's-Eye View (BEV) encoders are commonly used to combine a set
of camera images from multiple views into one joint latent BEV grid.
Traditionally, from this latent space, an intermediate raster map is predicted,
providing dense spatial supervision but requiring post-processing into the
desired vectorized form. More recent models directly derive infrastructure
elements as polylines using vectorized map decoders, providing instance-level
information. Our approach, Augmentation Map Network (AugMapNet), proposes
latent BEV grid augmentation, a novel technique that significantly enhances the
latent BEV representation. AugMapNet combines vector decoding and dense spatial
supervision more effectively than existing architectures while remaining as
straightforward to integrate and as generic as auxiliary supervision.
Experiments on nuScenes and Argoverse2 datasets demonstrate significant
improvements in vectorized map prediction performance up to 13.3% over the
StreamMapNet baseline on 60m range and greater improvements on larger ranges.
We confirm transferability by applying our method to another baseline and find
similar improvements. A detailed analysis of the latent BEV grid confirms a
more structured latent space of AugMapNet and shows the value of our novel
concept beyond pure performance improvement. The code will be released soon.
</details>

### [Less Biased Noise Scale Estimation for Threshold-Robust RANSAC](https://arxiv.org/abs/2503.13433)
*Johan Edstedt*
<details>
  <summary>Abstract</summary>
The gold-standard for robustly estimating relative pose through image
matching is RANSAC. While RANSAC is powerful, it requires setting the inlier
threshold that determines whether the error of a correspondence under an
estimated model is sufficiently small to be included in its consensus set.
Setting this threshold is typically done by hand, and is difficult to tune
without a access to ground truth data. Thus, a method capable of automatically
determining the optimal threshold would be desirable. In this paper we revisit
inlier noise scale estimation, which is an attractive approach as the inlier
noise scale is linear to the optimal threshold. We revisit the noise scale
estimation method SIMFIT and find bias in the estimate of the noise scale. In
particular, we fix underestimates from using the same data for fitting the
model as estimating the inlier noise, and from not taking the threshold itself
into account. Secondly, since the optimal threshold within a scene is
approximately constant we propose a multi-pair extension of SIMFIT++, by
filtering of estimates, which improves results. Our approach yields robust
performance across a range of thresholds, shown in Figure 1.
</details>

### [BlobCtrl: A Unified and Flexible Framework for Element-level Image Generation and Editing](https://arxiv.org/abs/2503.13434)
*Yaowei Li,Lingen Li,Zhaoyang Zhang,Xiaoyu Li,Guangzhi Wang,Hongxiang Li,Xiaodong Cun,Ying Shan,Yuexian Zou*
<details>
  <summary>Abstract</summary>
Element-level visual manipulation is essential in digital content creation,
but current diffusion-based methods lack the precision and flexibility of
traditional tools. In this work, we introduce BlobCtrl, a framework that
unifies element-level generation and editing using a probabilistic blob-based
representation. By employing blobs as visual primitives, our approach
effectively decouples and represents spatial location, semantic content, and
identity information, enabling precise element-level manipulation. Our key
contributions include: 1) a dual-branch diffusion architecture with
hierarchical feature fusion for seamless foreground-background integration; 2)
a self-supervised training paradigm with tailored data augmentation and score
functions; and 3) controllable dropout strategies to balance fidelity and
diversity. To support further research, we introduce BlobData for large-scale
training and BlobBench for systematic evaluation. Experiments show that
BlobCtrl excels in various element-level manipulation tasks while maintaining
computational efficiency, offering a practical solution for precise and
flexible visual content creation. Project page:
https://liyaowei-stu.github.io/project/BlobCtrl/
</details>

### [WideRange4D: Enabling High-Quality 4D Reconstruction with Wide-Range Movements and Scenes](https://arxiv.org/abs/2503.13435)
*Ling Yang,Kaixin Zhu,Juanxi Tian,Bohan Zeng,Mingbao Lin,Hongjuan Pei,Wentao Zhang,Shuicheng Yan*
<details>
  <summary>Abstract</summary>
With the rapid development of 3D reconstruction technology, research in 4D
reconstruction is also advancing, existing 4D reconstruction methods can
generate high-quality 4D scenes. However, due to the challenges in acquiring
multi-view video data, the current 4D reconstruction benchmarks mainly display
actions performed in place, such as dancing, within limited scenarios. In
practical scenarios, many scenes involve wide-range spatial movements,
highlighting the limitations of existing 4D reconstruction datasets.
Additionally, existing 4D reconstruction methods rely on deformation fields to
estimate the dynamics of 3D objects, but deformation fields struggle with
wide-range spatial movements, which limits the ability to achieve high-quality
4D scene reconstruction with wide-range spatial movements. In this paper, we
focus on 4D scene reconstruction with significant object spatial movements and
propose a novel 4D reconstruction benchmark, WideRange4D. This benchmark
includes rich 4D scene data with large spatial variations, allowing for a more
comprehensive evaluation of the generation capabilities of 4D generation
methods. Furthermore, we introduce a new 4D reconstruction method, Progress4D,
which generates stable and high-quality 4D results across various complex 4D
scene reconstruction tasks. We conduct both quantitative and qualitative
comparison experiments on WideRange4D, showing that our Progress4D outperforms
existing state-of-the-art 4D reconstruction methods. Project:
https://github.com/Gen-Verse/WideRange4D
</details>

### [Unified Autoregressive Visual Generation and Understanding with Continuous Tokens](https://arxiv.org/abs/2503.13436)
*Lijie Fan,Luming Tang,Siyang Qin,Tianhong Li,Xuan Yang,Siyuan Qiao,Andreas Steiner,Chen Sun,Yuanzhen Li,Tao Zhu,Michael Rubinstein,Michalis Raptis,Deqing Sun,Radu Soricut*
<details>
  <summary>Abstract</summary>
We present UniFluid, a unified autoregressive framework for joint visual
generation and understanding leveraging continuous visual tokens. Our unified
autoregressive architecture processes multimodal image and text inputs,
generating discrete tokens for text and continuous tokens for image. We find
though there is an inherent trade-off between the image generation and
understanding task, a carefully tuned training recipe enables them to improve
each other. By selecting an appropriate loss balance weight, the unified model
achieves results comparable to or exceeding those of single-task baselines on
both tasks. Furthermore, we demonstrate that employing stronger pre-trained
LLMs and random-order generation during training is important to achieve
high-fidelity image generation within this unified framework. Built upon the
Gemma model series, UniFluid exhibits competitive performance across both image
generation and understanding, demonstrating strong transferability to various
downstream tasks, including image editing for generation, as well as visual
captioning and question answering for understanding.
</details>

### [Amodal3R: Amodal 3D Reconstruction from Occluded 2D Images](https://arxiv.org/abs/2503.13439)
*Tianhao Wu,Chuanxia Zheng,Frank Guan,Andrea Vedaldi,Tat-Jen Cham*
<details>
  <summary>Abstract</summary>
Most image-based 3D object reconstructors assume that objects are fully
visible, ignoring occlusions that commonly occur in real-world scenarios. In
this paper, we introduce Amodal3R, a conditional 3D generative model designed
to reconstruct 3D objects from partial observations. We start from a
"foundation" 3D generative model and extend it to recover plausible 3D geometry
and appearance from occluded objects. We introduce a mask-weighted multi-head
cross-attention mechanism followed by an occlusion-aware attention layer that
explicitly leverages occlusion priors to guide the reconstruction process. We
demonstrate that, by training solely on synthetic data, Amodal3R learns to
recover full 3D objects even in the presence of occlusions in real scenes. It
substantially outperforms existing methods that independently perform 2D amodal
completion followed by 3D reconstruction, thereby establishing a new benchmark
for occlusion-aware 3D reconstruction.
</details>

### [MaTVLM: Hybrid Mamba-Transformer for Efficient Vision-Language Modeling](https://arxiv.org/abs/2503.13440)
*Yingyue Li,Bencheng Liao,Wenyu Liu,Xinggang Wang*
<details>
  <summary>Abstract</summary>
With the advancement of RNN models with linear complexity, the quadratic
complexity challenge of transformers has the potential to be overcome. Notably,
the emerging Mamba-2 has demonstrated competitive performance, bridging the gap
between RNN models and transformers. However, due to sequential processing and
vanishing gradients, RNN models struggle to capture long-range dependencies,
limiting contextual understanding. This results in slow convergence, high
resource demands, and poor performance on downstream understanding and complex
reasoning tasks. In this work, we present a hybrid model MaTVLM by substituting
a portion of the transformer decoder layers in a pre-trained VLM with Mamba-2
layers. Leveraging the inherent relationship between attention and Mamba-2, we
initialize Mamba-2 with corresponding attention weights to accelerate
convergence. Subsequently, we employ a single-stage distillation process, using
the pre-trained VLM as the teacher model to transfer knowledge to the MaTVLM,
further enhancing convergence speed and performance. Furthermore, we
investigate the impact of differential distillation loss within our training
framework. We evaluate the MaTVLM on multiple benchmarks, demonstrating
competitive performance against the teacher model and existing VLMs while
surpassing both Mamba-based VLMs and models of comparable parameter scales.
Remarkably, the MaTVLM achieves up to 3.6x faster inference than the teacher
model while reducing GPU memory consumption by 27.5%, all without compromising
performance. Code and models are released at http://github.com/hustvl/MaTVLM.
</details>

### [DPC: Dual-Prompt Collaboration for Tuning Vision-Language Models](https://arxiv.org/abs/2503.13443)
*Haoyang Li,Liang Wang,Chao Wang,Jing Jiang,Yan Peng,Guodong Long*
<details>
  <summary>Abstract</summary>
The Base-New Trade-off (BNT) problem universally exists during the
optimization of CLIP-based prompt tuning, where continuous fine-tuning on base
(target) classes leads to a simultaneous decrease of generalization ability on
new (unseen) classes. Existing approaches attempt to regulate the prompt tuning
process to balance BNT by appending constraints. However, imposed on the same
target prompt, these constraints fail to fully avert the mutual exclusivity
between the optimization directions for base and new. As a novel solution to
this challenge, we propose the plug-and-play Dual-Prompt Collaboration (DPC)
framework, the first that decoupling the optimization processes of base and new
tasks at the prompt level. Specifically, we clone a learnable parallel prompt
based on the backbone prompt, and introduce a variable Weighting-Decoupling
framework to independently control the optimization directions of dual prompts
specific to base or new tasks, thus avoiding the conflict in generalization.
Meanwhile, we propose a Dynamic Hard Negative Optimizer, utilizing dual prompts
to construct a more challenging optimization task on base classes for
enhancement. For interpretability, we prove the feature channel invariance of
the prompt vector during the optimization process, providing theoretical
support for the Weighting-Decoupling of DPC. Extensive experiments on multiple
backbones demonstrate that DPC can significantly improve base performance
without introducing any external knowledge beyond the base classes, while
maintaining generalization to new classes. Code is available at:
https://github.com/JREion/DPC.
</details>

### [VideoMind: A Chain-of-LoRA Agent for Long Video Reasoning](https://arxiv.org/abs/2503.13444)
*Ye Liu,Kevin Qinghong Lin,Chang Wen Chen,Mike Zheng Shou*
<details>
  <summary>Abstract</summary>
Videos, with their unique temporal dimension, demand precise grounded
understanding, where answers are directly linked to visual, interpretable
evidence. Despite significant breakthroughs in reasoning capabilities within
Large Language Models, multi-modal reasoning - especially for videos - remains
unexplored. In this work, we introduce VideoMind, a novel video-language agent
designed for temporal-grounded video understanding. VideoMind incorporates two
key innovations: (i) We identify essential capabilities for video temporal
reasoning and develop a role-based agentic workflow, including a planner for
coordinating different roles, a grounder for temporal localization, a verifier
to assess temporal interval accuracy, and an answerer for question-answering.
(ii) To efficiently integrate these diverse roles, we propose a novel
Chain-of-LoRA strategy, enabling seamless role-switching via lightweight LoRA
adaptors while avoiding the overhead of multiple models, thus balancing
efficiency and flexibility. Extensive experiments on 14 public benchmarks
demonstrate that our agent achieves state-of-the-art performance on diverse
video understanding tasks, including 3 on grounded video question-answering, 6
on video temporal grounding, and 5 on general video question-answering,
underscoring its effectiveness in advancing video agent and long-form temporal
reasoning.
</details>

### [Simulation of prosthetic vision with PRIMA system and enhancement of face representation](https://arxiv.org/abs/2503.11677)
*Jungyeon Park,Anna Kochnev Goldstein,Yueming Zhou,Nathan Jensen,Daniel Palanker*
<details>
  <summary>Abstract</summary>
Objective. Patients implanted with the PRIMA photovoltaic subretinal
prosthesis in geographic atrophy report form vision with the average acuity
matching the 100um pixel size. Although this remarkable outcome enables them to
read and write, they report difficulty with perceiving faces. This paper
provides a novel, non-pixelated algorithm for simulating prosthetic vision the
way it is experienced by PRIMA patients, compares the algorithm's predictions
to clinical perceptual outcomes, and offers computer vision and machine
learning (ML) methods to improve face representation. Approach. Our simulation
algorithm integrates a grayscale filter, spatial resolution filter, and
contrast filter. This accounts for the limited sampling density of the retinal
implant, as well as the reduced contrast sensitivity of prosthetic vision.
Patterns of Landolt C and faces created using this simulation algorithm are
compared to reports from actual PRIMA users. To recover the facial features
lost in prosthetic vision, we apply an ML facial landmarking model as well as
contrast adjusting tone curves to the face image prior to its projection onto
the implant. Main results. Simulated prosthetic vision matches the maximum
letter acuity observed in clinical studies as well as patients' subjective
descriptions. Application of the inversed contrast filter helps preserve the
contrast in prosthetic vision. Identification of the facial features using an
ML facial landmarking model and accentuating them further improve face
representation. Significance. Spatial and contrast constraints of prosthetic
vision limit resolvable features and degrade natural images. ML based methods
and contrast adjustments mitigate some limitations and improve face
representation. Even though higher spatial resolution can be expected with
implants having smaller pixels, contrast enhancement still remains essential
for face recognition.
</details>

### [CORDIC Is All You Need](https://arxiv.org/abs/2503.11685)
*Omkar Kokane,Adam Teman,Anushka Jha,Guru Prasath SL,Gopal Raut,Mukul Lokhande,S. V. Jaya Chand,Tanushree Dewangan,Santosh Kumar Vishvakarma*
<details>
  <summary>Abstract</summary>
Artificial intelligence necessitates adaptable hardware accelerators for
efficient high-throughput million operations. We present pipelined architecture
with CORDIC block for linear MAC computations and nonlinear iterative
Activation Functions (AF) such as $tanh$, $sigmoid$, and $softmax$. This
approach focuses on a Reconfigurable Processing Engine (RPE) based systolic
array, with 40\% pruning rate, enhanced throughput up to 4.64$\times$, and
reduction in power and area by 5.02 $\times$ and 4.06 $\times$ at CMOS 28 nm,
with minor accuracy loss. FPGA implementation achieves a reduction of up to 2.5
$\times$ resource savings and 3 $\times$ power compared to prior works. The
Systolic CORDIC engine for Reconfigurability and Enhanced throughput (SYCore)
deploys an output stationary dataflow with the CAESAR control engine for
diverse AI workloads such as Transformers, RNNs/LSTMs, and DNNs for
applications like image detection, LLMs, and speech recognition. The
energy-efficient and flexible approach extends the enhanced approach for edge
AI accelerators supporting emerging workloads.
</details>

### [FloPE: Flower Pose Estimation for Precision Pollination](https://arxiv.org/abs/2503.11692)
*Rashik Shrestha,Madhav Rijal,Trevor Smith,Yu Gu*
<details>
  <summary>Abstract</summary>
This study presents Flower Pose Estimation (FloPE), a real-time flower pose
estimation framework for computationally constrained robotic pollination
systems. Robotic pollination has been proposed to supplement natural
pollination to ensure global food security due to the decreased population of
natural pollinators. However, flower pose estimation for pollination is
challenging due to natural variability, flower clusters, and high accuracy
demands due to the flowers' fragility when pollinating. This method leverages
3D Gaussian Splatting to generate photorealistic synthetic datasets with
precise pose annotations, enabling effective knowledge distillation from a
high-capacity teacher model to a lightweight student model for efficient
inference. The approach was evaluated on both single and multi-arm robotic
platforms, achieving a mean pose estimation error of 0.6 cm and 19.14 degrees
within a low computational cost. Our experiments validate the effectiveness of
FloPE, achieving up to 78.75% pollination success rate and outperforming prior
robotic pollination techniques.
</details>

### [From Pixels to Histopathology: A Graph-Based Framework for Interpretable Whole Slide Image Analysis](https://arxiv.org/abs/2503.11846)
*Alexander Weers,Alexander H. Berger,Laurin Lux,Peter Sch√ºffler,Daniel Rueckert,Johannes C. Paetzold*
<details>
  <summary>Abstract</summary>
The histopathological classification of whole-slide images (WSIs) is a
fundamental task in digital pathology; yet it requires extensive time and
expertise from specialists. While deep learning methods show promising results,
they typically process WSIs by dividing them into artificial patches, which
inherently prevents a network from learning from the entire image context,
disregards natural tissue structures and compromises interpretability. Our
method overcomes this limitation through a novel graph-based framework that
constructs WSI graph representations. The WSI-graph efficiently captures
essential histopathological information in a compact form. We build tissue
representations (nodes) that follow biological boundaries rather than arbitrary
patches all while providing interpretable features for explainability. Through
adaptive graph coarsening guided by learned embeddings, we progressively merge
regions while maintaining discriminative local features and enabling efficient
global information exchange. In our method's final step, we solve the
diagnostic task through a graph attention network. We empirically demonstrate
strong performance on multiple challenging tasks such as cancer stage
classification and survival prediction, while also identifying predictive
factors using Integrated Gradients. Our implementation is publicly available at
https://github.com/HistoGraph31/pix2pathology
</details>

### [DCAT: Dual Cross-Attention Fusion for Disease Classification in Radiological Images with Uncertainty Estimation](https://arxiv.org/abs/2503.11851)
*Jutika Borah,Hidam Kumarjit Singh*
<details>
  <summary>Abstract</summary>
Accurate and reliable image classification is crucial in radiology, where
diagnostic decisions significantly impact patient outcomes. Conventional deep
learning models tend to produce overconfident predictions despite underlying
uncertainties, potentially leading to misdiagnoses. Attention mechanisms have
emerged as powerful tools in deep learning, enabling models to focus on
relevant parts of the input data. Combined with feature fusion, they can be
effective in addressing uncertainty challenges. Cross-attention has become
increasingly important in medical image analysis for capturing dependencies
across features and modalities. This paper proposes a novel dual
cross-attention fusion model for medical image analysis by addressing key
challenges in feature integration and interpretability. Our approach introduces
a bidirectional cross-attention mechanism with refined channel and spatial
attention that dynamically fuses feature maps from EfficientNetB4 and ResNet34
leveraging multi-network contextual dependencies. The refined features through
channel and spatial attention highlights discriminative patterns crucial for
accurate classification. The proposed model achieved AUC of 99.75%, 100%,
99.93% and 98.69% and AUPR of 99.81%, 100%, 99.97%, and 96.36% on Covid-19,
Tuberculosis, Pneumonia Chest X-ray images and Retinal OCT images respectively.
The entropy values and several high uncertain samples give an interpretable
visualization from the model enhancing transparency. By combining multi-scale
feature extraction, bidirectional attention and uncertainty estimation, our
proposed model strongly impacts medical image analysis.
</details>

### [Goal-Oriented Source Coding using LDPC Codes for Compressed-Domain Image Classification](https://arxiv.org/abs/2503.11954)
*Ahcen Aliouat,Elsa Dupraz*
<details>
  <summary>Abstract</summary>
In the emerging field of goal-oriented communications, the focus has shifted
from reconstructing data to directly performing specific learning tasks, such
as classification, segmentation, or pattern recognition, on the received coded
data. In the commonly studied scenario of classification from compressed
images, a key objective is to enable learning directly on entropy-coded data,
thereby bypassing the computationally intensive step of data reconstruction.
Conventional entropy-coding methods, such as Huffman and Arithmetic coding, are
effective for compression but disrupt the data structure, making them less
suitable for direct learning without decoding. This paper investigates the use
of low-density parity-check (LDPC) codes -- originally designed for channel
coding -- as an alternative entropy-coding approach. It is hypothesized that
the structured nature of LDPC codes can be leveraged more effectively by deep
learning models for tasks like classification. At the receiver side, gated
recurrent unit (GRU) models are trained to perform image classification
directly on LDPC-coded data. Experiments on datasets like MNIST, Fashion-MNIST,
and CIFAR show that LDPC codes outperform Huffman and Arithmetic coding in
classification tasks, while requiring significantly smaller learning models.
Furthermore, the paper analyzes why LDPC codes preserve data structure more
effectively than traditional entropy-coding techniques and explores the impact
of key code parameters on classification performance. These results suggest
that LDPC-based entropy coding offers an optimal balance between learning
efficiency and model complexity, eliminating the need for prior decoding.
</details>

### [Snapmoji: Instant Generation of Animatable Dual-Stylized Avatars](https://arxiv.org/abs/2503.11978)
*Eric M. Chen,Di Liu,Sizhuo Ma,Michael Vasilkovsky,Bing Zhou,Qiang Gao,Wenzhou Wang,Jiahao Luo,Dimitris N. Metaxas,Vincent Sitzmann,Jian Wang*
<details>
  <summary>Abstract</summary>
The increasing popularity of personalized avatar systems, such as Snapchat
Bitmojis and Apple Memojis, highlights the growing demand for digital
self-representation. Despite their widespread use, existing avatar platforms
face significant limitations, including restricted expressivity due to
predefined assets, tedious customization processes, or inefficient rendering
requirements. Addressing these shortcomings, we introduce Snapmoji, an avatar
generation system that instantly creates animatable, dual-stylized avatars from
a selfie. We propose Gaussian Domain Adaptation (GDA), which is pre-trained on
large-scale Gaussian models using 3D data from sources such as Objaverse and
fine-tuned with 2D style transfer tasks, endowing it with a rich 3D prior. This
enables Snapmoji to transform a selfie into a primary stylized avatar, like the
Bitmoji style, and apply a secondary style, such as Plastic Toy or Alien, all
while preserving the user's identity and the primary style's integrity. Our
system is capable of producing 3D Gaussian avatars that support dynamic
animation, including accurate facial expression transfer. Designed for
efficiency, Snapmoji achieves selfie-to-avatar conversion in just 0.9 seconds
and supports real-time interactions on mobile devices at 30 to 40 frames per
second. Extensive testing confirms that Snapmoji outperforms existing methods
in versatility and speed, making it a convenient tool for automatic avatar
creation in various styles.
</details>

### [Diffusion Dynamics Models with Generative State Estimation for Cloth Manipulation](https://arxiv.org/abs/2503.11999)
*Tongxuan Tian,Haoyang Li,Bo Ai,Xiaodi Yuan,Zhiao Huang,Hao Su*
<details>
  <summary>Abstract</summary>
Manipulating deformable objects like cloth is challenging due to their
complex dynamics, near-infinite degrees of freedom, and frequent
self-occlusions, which complicate state estimation and dynamics modeling. Prior
work has struggled with robust cloth state estimation, while dynamics models,
primarily based on Graph Neural Networks (GNNs), are limited by their locality.
Inspired by recent advances in generative models, we hypothesize that these
expressive models can effectively capture intricate cloth configurations and
deformation patterns from data. Building on this insight, we propose a
diffusion-based generative approach for both perception and dynamics modeling.
Specifically, we formulate state estimation as reconstructing the full cloth
state from sparse RGB-D observations conditioned on a canonical cloth mesh and
dynamics modeling as predicting future states given the current state and robot
actions. Leveraging a transformer-based diffusion model, our method achieves
high-fidelity state reconstruction while reducing long-horizon dynamics
prediction errors by an order of magnitude compared to GNN-based approaches.
Integrated with model-predictive control (MPC), our framework successfully
executes cloth folding on a real robotic system, demonstrating the potential of
generative models for manipulation tasks with partial observability and complex
dynamics.
</details>

### [Hydra-NeXt: Robust Closed-Loop Driving with Open-Loop Training](https://arxiv.org/abs/2503.12030)
*Zhenxin Li,Shihao Wang,Shiyi Lan,Zhiding Yu,Zuxuan Wu,Jose M. Alvarez*
<details>
  <summary>Abstract</summary>
End-to-end autonomous driving research currently faces a critical challenge
in bridging the gap between open-loop training and closed-loop deployment.
Current approaches are trained to predict trajectories in an open-loop
environment, which struggle with quick reactions to other agents in closed-loop
environments and risk generating kinematically infeasible plans due to the gap
between open-loop training and closed-loop driving. In this paper, we introduce
Hydra-NeXt, a novel multi-branch planning framework that unifies trajectory
prediction, control prediction, and a trajectory refinement network in one
model. Unlike current open-loop trajectory prediction models that only handle
general-case planning, Hydra-NeXt further utilizes a control decoder to focus
on short-term actions, which enables faster responses to dynamic situations and
reactive agents. Moreover, we propose the Trajectory Refinement module to
augment and refine the planning decisions by effectively adhering to kinematic
constraints in closed-loop environments. This unified approach bridges the gap
between open-loop training and closed-loop driving, demonstrating superior
performance of 65.89 Driving Score (DS) and 48.20% Success Rate (SR) on the
Bench2Drive dataset without relying on external experts for data collection.
Hydra-NeXt surpasses the previous state-of-the-art by 22.98 DS and 17.49 SR,
marking a significant advancement in autonomous driving. Code will be available
at https://github.com/woxihuanjiangguo/Hydra-NeXt.
</details>

### [Prosody-Enhanced Acoustic Pre-training and Acoustic-Disentangled Prosody Adapting for Movie Dubbing](https://arxiv.org/abs/2503.12042)
*Zhedong Zhang,Liang Li,Chenggang Yan,Chunshan Liu,Anton van den Hengel,Yuankai Qi*
<details>
  <summary>Abstract</summary>
Movie dubbing describes the process of transforming a script into speech that
aligns temporally and emotionally with a given movie clip while exemplifying
the speaker's voice demonstrated in a short reference audio clip. This task
demands the model bridge character performances and complicated prosody
structures to build a high-quality video-synchronized dubbing track. The
limited scale of movie dubbing datasets, along with the background noise
inherent in audio data, hinder the acoustic modeling performance of trained
models. To address these issues, we propose an acoustic-prosody disentangled
two-stage method to achieve high-quality dubbing generation with precise
prosody alignment. First, we propose a prosody-enhanced acoustic pre-training
to develop robust acoustic modeling capabilities. Then, we freeze the
pre-trained acoustic system and design a disentangled framework to model
prosodic text features and dubbing style while maintaining acoustic quality.
Additionally, we incorporate an in-domain emotion analysis module to reduce the
impact of visual domain shifts across different movies, thereby enhancing
emotion-prosody alignment. Extensive experiments show that our method performs
favorably against the state-of-the-art models on two primary benchmarks. The
demos are available at https://zzdoog.github.io/ProDubber/.
</details>

### [Enhanced Sentiment Analysis of Iranian Restaurant Reviews Utilizing Sentiment Intensity Analyzer & Fuzzy Logic](https://arxiv.org/abs/2503.12141)
*Shayan Rokhva,Babak Teimourpour,Romina Babaei*
<details>
  <summary>Abstract</summary>
This research presents an advanced sentiment analysis framework studied on
Iranian restaurant reviews, combining fuzzy logic with conventional sentiment
analysis techniques to assess both sentiment polarity and intensity. A dataset
of 1266 reviews, alongside corresponding star ratings, was compiled and
preprocessed for analysis. Initial sentiment analysis was conducted using the
Sentiment Intensity Analyzer (VADER), a rule-based tool that assigns sentiment
scores across positive, negative, and neutral categories. However, a noticeable
bias toward neutrality often led to an inaccurate representation of sentiment
intensity. To mitigate this issue, based on a fuzzy perspective, two refinement
techniques were introduced, applying square-root and fourth-root
transformations to amplify positive and negative sentiment scores while
maintaining neutrality. This led to three distinct methodologies: Approach 1,
utilizing unaltered VADER scores; Approach 2, modifying sentiment values using
the square root; and Approach 3, applying the fourth root for further
refinement. A Fuzzy Inference System incorporating comprehensive fuzzy rules
was then developed to process these refined scores and generate a single,
continuous sentiment value for each review based on each approach. Comparative
analysis, including human supervision and alignment with customer star ratings,
revealed that the refined approaches significantly improved sentiment analysis
by reducing neutrality bias and better capturing sentiment intensity. Despite
these advancements, minor over-amplification and persistent neutrality in
domain-specific cases were identified, leading us to propose several future
studies to tackle these occasional barriers. The study's methodology and
outcomes offer valuable insights for businesses seeking a more precise
understanding of consumer sentiment, enhancing sentiment analysis across
various industries.
</details>

### [DiffAD: A Unified Diffusion Modeling Approach for Autonomous Driving](https://arxiv.org/abs/2503.12170)
*Tao Wang,Cong Zhang,Xingguang Qu,Kun Li,Weiwei Liu,Chang Huang*
<details>
  <summary>Abstract</summary>
End-to-end autonomous driving (E2E-AD) has rapidly emerged as a promising
approach toward achieving full autonomy. However, existing E2E-AD systems
typically adopt a traditional multi-task framework, addressing perception,
prediction, and planning tasks through separate task-specific heads. Despite
being trained in a fully differentiable manner, they still encounter issues
with task coordination, and the system complexity remains high. In this work,
we introduce DiffAD, a novel diffusion probabilistic model that redefines
autonomous driving as a conditional image generation task. By rasterizing
heterogeneous targets onto a unified bird's-eye view (BEV) and modeling their
latent distribution, DiffAD unifies various driving objectives and jointly
optimizes all driving tasks in a single framework, significantly reducing
system complexity and harmonizing task coordination. The reverse process
iteratively refines the generated BEV image, resulting in more robust and
realistic driving behaviors. Closed-loop evaluations in Carla demonstrate the
superiority of the proposed method, achieving a new state-of-the-art Success
Rate and Driving Score. The code will be made publicly available.
</details>

### [SEAL: Semantic Aware Image Watermarking](https://arxiv.org/abs/2503.12172)
*Kasra Arabi,R. Teal Witter,Chinmay Hegde,Niv Cohen*
<details>
  <summary>Abstract</summary>
Generative models have rapidly evolved to generate realistic outputs.
However, their synthetic outputs increasingly challenge the clear distinction
between natural and AI-generated content, necessitating robust watermarking
techniques. Watermarks are typically expected to preserve the integrity of the
target image, withstand removal attempts, and prevent unauthorized replication
onto unrelated images. To address this need, recent methods embed persistent
watermarks into images produced by diffusion models using the initial noise.
Yet, to do so, they either distort the distribution of generated images or rely
on searching through a long dictionary of used keys for detection.
  In this paper, we propose a novel watermarking method that embeds semantic
information about the generated image directly into the watermark, enabling a
distortion-free watermark that can be verified without requiring a database of
key patterns. Instead, the key pattern can be inferred from the semantic
embedding of the image using locality-sensitive hashing. Furthermore,
conditioning the watermark detection on the original image content improves
robustness against forgery attacks. To demonstrate that, we consider two
largely overlooked attack strategies: (i) an attacker extracting the initial
noise and generating a novel image with the same pattern; (ii) an attacker
inserting an unrelated (potentially harmful) object into a watermarked image,
possibly while preserving the watermark. We empirically validate our method's
increased robustness to these attacks. Taken together, our results suggest that
content-aware watermarks can mitigate risks arising from image-generative
models.
</details>

### [DPCS: Path Tracing-Based Differentiable Projector-Camera Systems](https://arxiv.org/abs/2503.12174)
*Jijiang Li,Qingyue Deng,Haibin Ling,Bingyao Huang*
<details>
  <summary>Abstract</summary>
Projector-camera systems (ProCams) simulation aims to model the physical
project-and-capture process and associated scene parameters of a ProCams, and
is crucial for spatial augmented reality (SAR) applications such as ProCams
relighting and projector compensation. Recent advances use an end-to-end neural
network to learn the project-and-capture process. However, these neural
network-based methods often implicitly encapsulate scene parameters, such as
surface material, gamma, and white balance in the network parameters, and are
less interpretable and hard for novel scene simulation. Moreover, neural
networks usually learn the indirect illumination implicitly in an
image-to-image translation way which leads to poor performance in simulating
complex projection effects such as soft-shadow and interreflection. In this
paper, we introduce a novel path tracing-based differentiable projector-camera
systems (DPCS), offering a differentiable ProCams simulation method that
explicitly integrates multi-bounce path tracing. Our DPCS models the physical
project-and-capture process using differentiable physically-based rendering
(PBR), enabling the scene parameters to be explicitly decoupled and learned
using much fewer samples. Moreover, our physically-based method not only
enables high-quality downstream ProCams tasks, such as ProCams relighting and
projector compensation, but also allows novel scene simulation using the
learned scene parameters. In experiments, DPCS demonstrates clear advantages
over previous approaches in ProCams simulation, offering better
interpretability, more efficient handling of complex interreflection and
shadow, and requiring fewer training samples.
</details>

### [Bench2FreeAD: A Benchmark for Vision-based End-to-end Navigation in Unstructured Robotic Environments](https://arxiv.org/abs/2503.12180)
*Yuhang Peng,Sidong Wang,Jihaoyu Yang,Shilong Li,Han Wang,Jiangtao Gong*
<details>
  <summary>Abstract</summary>
Most current end-to-end (E2E) autonomous driving algorithms are built on
standard vehicles in structured transportation scenarios, lacking exploration
of robot navigation for unstructured scenarios such as auxiliary roads, campus
roads, and indoor settings. This paper investigates E2E robot navigation in
unstructured road environments. First, we introduce two data collection
pipelines - one for real-world robot data and another for synthetic data
generated using the Isaac Sim simulator, which together produce an unstructured
robotics navigation dataset -- FreeWorld Dataset. Second, we fine-tuned an
efficient E2E autonomous driving model -- VAD -- using our datasets to validate
the performance and adaptability of E2E autonomous driving models in these
environments. Results demonstrate that fine-tuning through our datasets
significantly enhances the navigation potential of E2E autonomous driving
models in unstructured robotic environments. Thus, this paper presents the
first dataset targeting E2E robot navigation tasks in unstructured scenarios,
and provides a benchmark based on vision-based E2E autonomous driving
algorithms to facilitate the development of E2E navigation technology for
logistics and service robots. The project is available on Github.
</details>

### [Shadow Art Kanji: Inverse Rendering Application](https://arxiv.org/abs/2503.12229)
*William Louis Rothman,Yasuyuki Matsushita*
<details>
  <summary>Abstract</summary>
Finding a balance between artistic beauty and machine-generated imagery is
always a difficult task. This project seeks to create 3D models that, when
illuminated, cast shadows resembling Kanji characters. It aims to combine
artistic expression with computational techniques, providing an accurate and
efficient approach to visualizing these Japanese characters through shadows.
</details>

### [HyperKAN: Hypergraph Representation Learning with Kolmogorov-Arnold Networks](https://arxiv.org/abs/2503.12365)
*Xiangfei Fang,Boying Wang,Chengying Huan,Shaonan Ma,Heng Zhang,Chen Zhao*
<details>
  <summary>Abstract</summary>
Hypergraph representation learning has garnered increasing attention across
various domains due to its capability to model high-order relationships.
Traditional methods often rely on hypergraph neural networks (HNNs) employing
message passing mechanisms to aggregate vertex and hyperedge features. However,
these methods are constrained by their dependence on hypergraph topology,
leading to the challenge of imbalanced information aggregation, where
high-degree vertices tend to aggregate redundant features, while low-degree
vertices often struggle to capture sufficient structural features. To overcome
the above challenges, we introduce HyperKAN, a novel framework for hypergraph
representation learning that transcends the limitations of message-passing
techniques. HyperKAN begins by encoding features for each vertex and then
leverages Kolmogorov-Arnold Networks (KANs) to capture complex nonlinear
relationships. By adjusting structural features based on similarity, our
approach generates refined vertex representations that effectively addresses
the challenge of imbalanced information aggregation. Experiments conducted on
the real-world datasets demonstrate that HyperKAN significantly outperforms
state of-the-art HNN methods, achieving nearly a 9% performance improvement on
the Senate dataset.
</details>

### [Modality-Composable Diffusion Policy via Inference-Time Distribution-level Composition](https://arxiv.org/abs/2503.12466)
*Jiahang Cao,Qiang Zhang,Hanzhong Guo,Jiaxu Wang,Hao Cheng,Renjing Xu*
<details>
  <summary>Abstract</summary>
Diffusion Policy (DP) has attracted significant attention as an effective
method for policy representation due to its capacity to model
multi-distribution dynamics. However, current DPs are often based on a single
visual modality (e.g., RGB or point cloud), limiting their accuracy and
generalization potential. Although training a generalized DP capable of
handling heterogeneous multimodal data would enhance performance, it entails
substantial computational and data-related costs. To address these challenges,
we propose a novel policy composition method: by leveraging multiple
pre-trained DPs based on individual visual modalities, we can combine their
distributional scores to form a more expressive Modality-Composable Diffusion
Policy (MCDP), without the need for additional training. Through extensive
empirical experiments on the RoboTwin dataset, we demonstrate the potential of
MCDP to improve both adaptability and performance. This exploration aims to
provide valuable insights into the flexible composition of existing DPs,
facilitating the development of generalizable cross-modality, cross-domain, and
even cross-embodiment policies. Our code is open-sourced at
https://github.com/AndyCao1125/MCDP.
</details>

### [MPBench: A Comprehensive Multimodal Reasoning Benchmark for Process Errors Identification](https://arxiv.org/abs/2503.12505)
*Zhaopan Xu,Pengfei Zhou,Jiaxin Ai,Wangbo Zhao,Kai Wang,Xiaojiang Peng,Wenqi Shao,Hongxun Yao,Kaipeng Zhang*
<details>
  <summary>Abstract</summary>
Reasoning is an essential capacity for large language models (LLMs) to
address complex tasks, where the identification of process errors is vital for
improving this ability. Recently, process-level reward models (PRMs) were
proposed to provide step-wise rewards that facilitate reinforcement learning
and data production during training and guide LLMs toward correct steps during
inference, thereby improving reasoning accuracy. However, existing benchmarks
of PRMs are text-based and focus on error detection, neglecting other scenarios
like reasoning search. To address this gap, we introduce MPBench, a
comprehensive, multi-task, multimodal benchmark designed to systematically
assess the effectiveness of PRMs in diverse scenarios. MPBench employs three
evaluation paradigms, each targeting a specific role of PRMs in the reasoning
process: (1) Step Correctness, which assesses the correctness of each
intermediate reasoning step; (2) Answer Aggregation, which aggregates multiple
solutions and selects the best one; and (3) Reasoning Process Search, which
guides the search for optimal reasoning steps during inference. Through these
paradigms, MPBench makes comprehensive evaluations and provides insights into
the development of multimodal PRMs.
</details>

### [Debiasing Diffusion Model: Enhancing Fairness through Latent Representation Learning in Stable Diffusion Model](https://arxiv.org/abs/2503.12536)
*Lin-Chun Huang,Ching Chieh Tsao,Fang-Yi Su,Jung-Hsien Chiang*
<details>
  <summary>Abstract</summary>
Image generative models, particularly diffusion-based models, have surged in
popularity due to their remarkable ability to synthesize highly realistic
images. However, since these models are data-driven, they inherit biases from
the training datasets, frequently leading to disproportionate group
representations that exacerbate societal inequities. Traditionally, efforts to
debiase these models have relied on predefined sensitive attributes,
classifiers trained on such attributes, or large language models to steer
outputs toward fairness. However, these approaches face notable drawbacks:
predefined attributes do not adequately capture complex and continuous
variations among groups. To address these issues, we introduce the Debiasing
Diffusion Model (DDM), which leverages an indicator to learn latent
representations during training, promoting fairness through balanced
representations without requiring predefined sensitive attributes. This
approach not only demonstrates its effectiveness in scenarios previously
addressed by conventional techniques but also enhances fairness without relying
on predefined sensitive attributes as conditions. In this paper, we discuss the
limitations of prior bias mitigation techniques in diffusion-based models,
elaborate on the architecture of the DDM, and validate the effectiveness of our
approach through experiments.
</details>

### [Grasping Partially Occluded Objects Using Autoencoder-Based Point Cloud Inpainting](https://arxiv.org/abs/2503.12549)
*Alexander Koebler,Ralf Gross,Florian Buettner,Ingo Thon*
<details>
  <summary>Abstract</summary>
Flexible industrial production systems will play a central role in the future
of manufacturing due to higher product individualization and customization. A
key component in such systems is the robotic grasping of known or unknown
objects in random positions. Real-world applications often come with challenges
that might not be considered in grasping solutions tested in simulation or lab
settings. Partial occlusion of the target object is the most prominent.
Examples of occlusion can be supporting structures in the camera's field of
view, sensor imprecision, or parts occluding each other due to the production
process. In all these cases, the resulting lack of information leads to
shortcomings in calculating grasping points. In this paper, we present an
algorithm to reconstruct the missing information. Our inpainting solution
facilitates the real-world utilization of robust object matching approaches for
grasping point calculation. We demonstrate the benefit of our solution by
enabling an existing grasping system embedded in a real-world industrial
application to handle occlusions in the input. With our solution, we
drastically decrease the number of objects discarded by the process.
</details>

### [Niagara: Normal-Integrated Geometric Affine Field for Scene Reconstruction from a Single View](https://arxiv.org/abs/2503.12553)
*Xianzu Wu,Zhenxin Ai,Harry Yang,Ser-Nam Lim,Jun Liu,Huan Wang*
<details>
  <summary>Abstract</summary>
Recent advances in single-view 3D scene reconstruction have highlighted the
challenges in capturing fine geometric details and ensuring structural
consistency, particularly in high-fidelity outdoor scene modeling. This paper
presents Niagara, a new single-view 3D scene reconstruction framework that can
faithfully reconstruct challenging outdoor scenes from a single input image for
the first time.
  Our approach integrates monocular depth and normal estimation as input, which
substantially improves its ability to capture fine details, mitigating common
issues like geometric detail loss and deformation.
  Additionally, we introduce a geometric affine field (GAF) and 3D
self-attention as geometry-constraint, which combines the structural properties
of explicit geometry with the adaptability of implicit feature fields, striking
a balance between efficient rendering and high-fidelity reconstruction.
  Our framework finally proposes a specialized encoder-decoder architecture,
where a depth-based 3D Gaussian decoder is proposed to predict 3D Gaussian
parameters, which can be used for novel view synthesis. Extensive results and
analyses suggest that our Niagara surpasses prior SoTA approaches such as
Flash3D in both single-view and dual-view settings, significantly enhancing the
geometric accuracy and visual fidelity, especially in outdoor scenes.
</details>

### [VISO-Grasp: Vision-Language Informed Spatial Object-centric 6-DoF Active View Planning and Grasping in Clutter and Invisibility](https://arxiv.org/abs/2503.12609)
*Yitian Shi,Di Wen,Guanqi Chen,Edgar Welte,Sheng Liu,Kunyu Peng,Rainer Stiefelhagen,Rania Rayyes*
<details>
  <summary>Abstract</summary>
We propose VISO-Grasp, a novel vision-language-informed system designed to
systematically address visibility constraints for grasping in severely occluded
environments. By leveraging Foundation Models (FMs) for spatial reasoning and
active view planning, our framework constructs and updates an instance-centric
representation of spatial relationships, enhancing grasp success under
challenging occlusions. Furthermore, this representation facilitates active
Next-Best-View (NBV) planning and optimizes sequential grasping strategies when
direct grasping is infeasible. Additionally, we introduce a multi-view
uncertainty-driven grasp fusion mechanism that refines grasp confidence and
directional uncertainty in real-time, ensuring robust and stable grasp
execution. Extensive real-world experiments demonstrate that VISO-Grasp
achieves a success rate of $87.5\%$ in target-oriented grasping with the fewest
grasp attempts outperforming baselines. To the best of our knowledge,
VISO-Grasp is the first unified framework integrating FMs into target-aware
active view planning and 6-DoF grasping in environments with severe occlusions
and entire invisibility constraints.
</details>

### [MAVEN: Multi-modal Attention for Valence-Arousal Emotion Network](https://arxiv.org/abs/2503.12623)
*Vrushank Ahire,Kunal Shah,Mudasir Nazir Khan,Nikhil Pakhale,Lownish Rai Sookha,M. A. Ganaie,Abhinav Dhall*
<details>
  <summary>Abstract</summary>
This paper introduces MAVEN (Multi-modal Attention for Valence-Arousal
Emotion Network), a novel architecture for dynamic emotion recognition through
dimensional modeling of affect. The model uniquely integrates visual, audio,
and textual modalities via a bi-directional cross-modal attention mechanism
with six distinct attention pathways, enabling comprehensive interactions
between all modality pairs. Our proposed approach employs modality-specific
encoders to extract rich feature representations from synchronized video
frames, audio segments, and transcripts. The architecture's novelty lies in its
cross-modal enhancement strategy, where each modality representation is refined
through weighted attention from other modalities, followed by self-attention
refinement through modality-specific encoders. Rather than directly predicting
valence-arousal values, MAVEN predicts emotions in a polar coordinate form,
aligning with psychological models of the emotion circumplex. Experimental
evaluation on the Aff-Wild2 dataset demonstrates the effectiveness of our
approach, with performance measured using Concordance Correlation Coefficient
(CCC). The multi-stage architecture demonstrates superior ability to capture
the complex, nuanced nature of emotional expressions in conversational videos,
advancing the state-of-the-art (SOTA) in continuous emotion recognition
in-the-wild. Code can be found at:
https://github.com/Vrushank-Ahire/MAVEN_8th_ABAW.
</details>

### [COVID 19 Diagnosis Analysis using Transfer Learning](https://arxiv.org/abs/2503.12642)
*Anjali Dharmik*
<details>
  <summary>Abstract</summary>
Coronaviruses transmit COVID-19, a rapidly spreading disease. A Coronavirus
infection (COVID-19) was first discovered in December 2019 in Wuhan, China, and
spread rapidly throughout the planet in exactly some months. because of this,
the virus can cause severe symptoms and even death, especially within the
elderly and in people with medical conditions. The virus causes acute
respiratory infections in humans. the primary case was diagnosed in China in
2019 and the pandemic started in 2020. Since the quantity of cases of COVID-19
is increasing daily, there are only a limited number of test kits available in
hospitals. So, to stop COVID-19 from spreading among people, an automatic
diagnosis system must be implemented. during this study, three pre-trained
neural networks supported convolutional neural networks (VGG16, VGG19,
ResNet50) are proposed for detecting Coronavirus pneumonia infected patients
through X-rays and computerized tomography (CT). By using cross-validation,
we've got implemented binary classifications with two classes (COVID-19, Normal
(healthy)). Taking into consideration the results obtained, the pre-trained
ResNet50 model provides the simplest classification performance (97.77%
accuracy, 100% sensitivity, 93.33% specificity, 98.00% F1-score) among the
opposite three used models over 6259 images.
</details>

### [A Continual Learning-driven Model for Accurate and Generalizable Segmentation of Clinically Comprehensive and Fine-grained Whole-body Anatomies in CT](https://arxiv.org/abs/2503.12698)
*Dazhou Guo,Zhanghexuan Ji,Yanzhou Su,Dandan Zheng,Heng Guo,Puyang Wang,Ke Yan,Yirui Wang,Qinji Yu,Zi Li,Minfeng Xu,Jianfeng Zhang,Haoshen Li,Jia Ge,Tsung-Ying Ho,Bing-Shen Huang,Tashan Ai,Kuaile Zhao,Na Shen,Qifeng Wang,Yun Bian,Tingyu Wu,Peng Du,Hua Zhang,Feng-Ming Kong,Alan L. Yuille,Cher Heng Tan,Chunyan Miao,Perry J. Pickhardt,Senxiang Yan,Ronald M. Summers,Le Lu,Dakai Jin,Xianghua Ye*
<details>
  <summary>Abstract</summary>
Precision medicine in the quantitative management of chronic diseases and
oncology would be greatly improved if the Computed Tomography (CT) scan of any
patient could be segmented, parsed and analyzed in a precise and detailed way.
However, there is no such fully annotated CT dataset with all anatomies
delineated for training because of the exceptionally high manual cost, the need
for specialized clinical expertise, and the time required to finish the task.
To this end, we proposed a novel continual learning-driven CT model that can
segment complete anatomies presented using dozens of previously partially
labeled datasets, dynamically expanding its capacity to segment new ones
without compromising previously learned organ knowledge. Existing multi-dataset
approaches are not able to dynamically segment new anatomies without
catastrophic forgetting and would encounter optimization difficulty or
infeasibility when segmenting hundreds of anatomies across the whole range of
body regions. Our single unified CT segmentation model, CL-Net, can highly
accurately segment a clinically comprehensive set of 235 fine-grained
whole-body anatomies. Composed of a universal encoder, multiple optimized and
pruned decoders, CL-Net is developed using 13,952 CT scans from 20 public and
16 private high-quality partially labeled CT datasets of various vendors,
different contrast phases, and pathologies. Extensive evaluation demonstrates
that CL-Net consistently outperforms the upper limit of an ensemble of 36
specialist nnUNets trained per dataset with the complexity of 5% model size and
significantly surpasses the segmentation accuracy of recent leading Segment
Anything-style medical image foundation models by large margins. Our continual
learning-driven CL-Net model would lay a solid foundation to facilitate many
downstream tasks of oncology and chronic diseases using the most widely adopted
CT imaging.
</details>

### [Dynamic-Dark SLAM: RGB-Thermal Cooperative Robot Vision Strategy for Multi-Person Tracking in Both Well-Lit and Low-Light Scenes](https://arxiv.org/abs/2503.12768)
*Tatsuro Sakai,Kanji Tanaka,Jonathan Tay Yu Liang,Muhammad Adil Luqman,Daiki Iwata*
<details>
  <summary>Abstract</summary>
In robot vision, thermal cameras have significant potential for recognizing
humans even in complete darkness. However, their application to multi-person
tracking (MPT) has lagged due to data scarcity and difficulties in individual
identification. In this study, we propose a cooperative MPT system that
utilizes co-located RGB and thermal cameras, using pseudo-annotations (bounding
boxes + person IDs) to train RGB and T trackers. Evaluation experiments
demonstrate that the T tracker achieves remarkable performance in both bright
and dark scenes. Furthermore, results suggest that a tracker-switching approach
using a binary brightness classifier is more suitable than a tracker-fusion
approach for information integration. This study marks a crucial first step
toward ``Dynamic-Dark SLAM," enabling effective recognition, understanding, and
reconstruction of individuals, occluding objects, and traversable areas in
dynamic environments, both bright and dark.
</details>

### [Improving Generalization of Universal Adversarial Perturbation via Dynamic Maximin Optimization](https://arxiv.org/abs/2503.12793)
*Yechao Zhang,Yingzhe Xu,Junyu Shi,Leo Yu Zhang,Shengshan Hu,Minghui Li,Yanjun Zhang*
<details>
  <summary>Abstract</summary>
Deep neural networks (DNNs) are susceptible to universal adversarial
perturbations (UAPs). These perturbations are meticulously designed to fool the
target model universally across all sample classes. Unlike instance-specific
adversarial examples (AEs), generating UAPs is more complex because they must
be generalized across a wide range of data samples and models. Our research
reveals that existing universal attack methods, which optimize UAPs using DNNs
with static model parameter snapshots, do not fully leverage the potential of
DNNs to generate more effective UAPs. Rather than optimizing UAPs against
static DNN models with a fixed training set, we suggest using dynamic
model-data pairs to generate UAPs. In particular, we introduce a dynamic
maximin optimization strategy, aiming to optimize the UAP across a variety of
optimal model-data pairs. We term this approach DM-UAP. DM-UAP utilizes an
iterative max-min-min optimization framework that refines the model-data pairs,
coupled with a curriculum UAP learning algorithm to examine the combined space
of model parameters and data thoroughly. Comprehensive experiments on the
ImageNet dataset demonstrate that the proposed DM-UAP markedly enhances both
cross-sample universality and cross-model transferability of UAPs. Using only
500 samples for UAP generation, DM-UAP outperforms the state-of-the-art
approach with an average increase in fooling ratio of 12.108%.
</details>

### [AV-Surf: Surface-Enhanced Geometry-Aware Novel-View Acoustic Synthesis](https://arxiv.org/abs/2503.12806)
*Hadam Baek,Hannie Shin,Jiyoung Seo,Chanwoo Kim,Saerom Kim,Hyeongbok Kim,Sangpil Kim*
<details>
  <summary>Abstract</summary>
Accurately modeling sound propagation with complex real-world environments is
essential for Novel View Acoustic Synthesis (NVAS). While previous studies have
leveraged visual perception to estimate spatial acoustics, the combined use of
surface normal and structural details from 3D representations in acoustic
modeling has been underexplored. Given their direct impact on sound wave
reflections and propagation, surface normals should be jointly modeled with
structural details to achieve accurate spatial acoustics. In this paper, we
propose a surface-enhanced geometry-aware approach for NVAS to improve spatial
acoustic modeling. To achieve this, we exploit geometric priors, such as image,
depth map, surface normals, and point clouds obtained using a 3D Gaussian
Splatting (3DGS) based framework. We introduce a dual cross-attention-based
transformer integrating geometrical constraints into frequency query to
understand the surroundings of the emitter. Additionally, we design a
ConvNeXt-based spectral features processing network called Spectral Refinement
Network (SRN) to synthesize realistic binaural audio. Experimental results on
the RWAVS and SoundSpace datasets highlight the necessity of our approach, as
it surpasses existing methods in novel view acoustic synthesis.
</details>

### [Dynamic Derivation and Elimination: Audio Visual Segmentation with Enhanced Audio Semantics](https://arxiv.org/abs/2503.12840)
*Chen Liu,Liying Yang,Peike Li,Dadong Wang,Lincheng Li,Xin Yu*
<details>
  <summary>Abstract</summary>
Sound-guided object segmentation has drawn considerable attention for its
potential to enhance multimodal perception. Previous methods primarily focus on
developing advanced architectures to facilitate effective audio-visual
interactions, without fully addressing the inherent challenges posed by audio
natures, \emph{\ie}, (1) feature confusion due to the overlapping nature of
audio signals, and (2) audio-visual matching difficulty from the varied sounds
produced by the same object. To address these challenges, we propose Dynamic
Derivation and Elimination (DDESeg): a novel audio-visual segmentation
framework. Specifically, to mitigate feature confusion, DDESeg reconstructs the
semantic content of the mixed audio signal by enriching the distinct semantic
information of each individual source, deriving representations that preserve
the unique characteristics of each sound. To reduce the matching difficulty, we
introduce a discriminative feature learning module, which enhances the semantic
distinctiveness of generated audio representations. Considering that not all
derived audio representations directly correspond to visual features (e.g.,
off-screen sounds), we propose a dynamic elimination module to filter out
non-matching elements. This module facilitates targeted interaction between
sounding regions and relevant audio semantics. By scoring the interacted
features, we identify and filter out irrelevant audio information, ensuring
accurate audio-visual alignment. Comprehensive experiments demonstrate that our
framework achieves superior performance in AVS datasets.
</details>

### [Robust Audio-Visual Segmentation via Audio-Guided Visual Convergent Alignment](https://arxiv.org/abs/2503.12847)
*Chen Liu,Peike Li,Liying Yang,Dadong Wang,Lincheng Li,Xin Yu*
<details>
  <summary>Abstract</summary>
Accurately localizing audible objects based on audio-visual cues is the core
objective of audio-visual segmentation. Most previous methods emphasize spatial
or temporal multi-modal modeling, yet overlook challenges from ambiguous
audio-visual correspondences such as nearby visually similar but acoustically
different objects and frequent shifts in objects' sounding status.
Consequently, they may struggle to reliably correlate audio and visual cues,
leading to over- or under-segmentation. To address these limitations, we
propose a novel framework with two primary components: an audio-guided modality
alignment (AMA) module and an uncertainty estimation (UE) module. Instead of
indiscriminately correlating audio-visual cues through a global attention
mechanism, AMA performs audio-visual interactions within multiple groups and
consolidates group features into compact representations based on their
responsiveness to audio cues, effectively directing the model's attention to
audio-relevant areas. Leveraging contrastive learning, AMA further
distinguishes sounding regions from silent areas by treating features with
strong audio responses as positive samples and weaker responses as negatives.
Additionally, UE integrates spatial and temporal information to identify
high-uncertainty regions caused by frequent changes in sound state, reducing
prediction errors by lowering confidence in these areas. Experimental results
demonstrate that our approach achieves superior accuracy compared to existing
state-of-the-art methods, particularly in challenging scenarios where
traditional approaches struggle to maintain reliable segmentation.
</details>

### [Task-Oriented Feature Compression for Multimodal Understanding via Device-Edge Co-Inference](https://arxiv.org/abs/2503.12926)
*Cheng Yuan,Zhening Liu,Jiashu Lv,Jiawei Shao,Yufei Jiang,Jun Zhang,Xuelong Li*
<details>
  <summary>Abstract</summary>
With the rapid development of large multimodal models (LMMs), multimodal
understanding applications are emerging. As most LMM inference requests
originate from edge devices with limited computational capabilities, the
predominant inference pipeline involves directly forwarding the input data to
an edge server which handles all computations. However, this approach
introduces high transmission latency due to limited uplink bandwidth of edge
devices and significant computation latency caused by the prohibitive number of
visual tokens, thus hindering delay-sensitive tasks and degrading user
experience. To address this challenge, we propose a task-oriented feature
compression (TOFC) method for multimodal understanding in a device-edge
co-inference framework, where visual features are merged by clustering and
encoded by a learnable and selective entropy model before feature projection.
Specifically, we employ density peaks clustering based on K nearest neighbors
to reduce the number of visual features, thereby minimizing both data
transmission and computational complexity. Subsequently, a learnable entropy
model with hyperprior is utilized to encode and decode merged features, further
reducing transmission overhead. To enhance compression efficiency, multiple
entropy models are adaptively selected based on the characteristics of the
visual features, enabling a more accurate estimation of the probability
distribution. Comprehensive experiments on seven visual question answering
benchmarks validate the effectiveness of the proposed TOFC method. Results show
that TOFC achieves up to 60% reduction in data transmission overhead and 50%
reduction in system latency while maintaining identical task performance,
compared with traditional image compression methods.
</details>

### [R1-VL: Learning to Reason with Multimodal Large Language Models via Step-wise Group Relative Policy Optimization](https://arxiv.org/abs/2503.12937)
*Jingyi Zhang,Jiaxing Huang,Huanjin Yao,Shunyu Liu,Xikun Zhang,Shijian Lu,Dacheng Tao*
<details>
  <summary>Abstract</summary>
Recent studies generally enhance MLLMs' reasoning capabilities via supervised
fine-tuning on high-quality chain-of-thought reasoning data, which often leads
models to merely imitate successful reasoning paths without understanding what
the wrong reasoning paths are. In this work, we aim to enhance the MLLMs'
reasoning ability beyond passively imitating positive reasoning paths. To this
end, we design Step-wise Group Relative Policy Optimization (StepGRPO), a new
online reinforcement learning framework that enables MLLMs to self-improve
reasoning ability via simple, effective and dense step-wise rewarding.
Specifically, StepGRPO introduces two novel rule-based reasoning rewards:
Step-wise Reasoning Accuracy Reward (StepRAR) and Step-wise Reasoning Validity
Reward (StepRVR). StepRAR rewards the reasoning paths that contain necessary
intermediate reasoning steps via a soft key-step matching technique, while
StepRAR rewards reasoning paths that follow a well-structured and logically
consistent reasoning process through a reasoning completeness and logic
evaluation strategy. With the proposed StepGRPO, we introduce R1-VL, a series
of MLLMs with outstanding capabilities in step-by-step reasoning. Extensive
experiments over 8 benchmarks demonstrate the superiority of our methods.
</details>

### [How Good is my Histopathology Vision-Language Foundation Model? A Holistic Benchmark](https://arxiv.org/abs/2503.12990)
*Roba Al Majzoub,Hashmat Malik,Muzammal Naseer,Zaigham Zaheer,Tariq Mahmood,Salman Khan,Fahad Khan*
<details>
  <summary>Abstract</summary>
Recently, histopathology vision-language foundation models (VLMs) have gained
popularity due to their enhanced performance and generalizability across
different downstream tasks. However, most existing histopathology benchmarks
are either unimodal or limited in terms of diversity of clinical tasks, organs,
and acquisition instruments, as well as their partial availability to the
public due to patient data privacy. As a consequence, there is a lack of
comprehensive evaluation of existing histopathology VLMs on a unified benchmark
setting that better reflects a wide range of clinical scenarios. To address
this gap, we introduce HistoVL, a fully open-source comprehensive benchmark
comprising images acquired using up to 11 various acquisition tools that are
paired with specifically crafted captions by incorporating class names and
diverse pathology descriptions. Our Histo-VL includes 26 organs, 31 cancer
types, and a wide variety of tissue obtained from 14 heterogeneous patient
cohorts, totaling more than 5 million patches obtained from over 41K WSIs
viewed under various magnification levels. We systematically evaluate existing
histopathology VLMs on Histo-VL to simulate diverse tasks performed by experts
in real-world clinical scenarios. Our analysis reveals interesting findings,
including large sensitivity of most existing histopathology VLMs to textual
changes with a drop in balanced accuracy of up to 25% in tasks such as
Metastasis detection, low robustness to adversarial attacks, as well as
improper calibration of models evident through high ECE values and low model
prediction confidence, all of which can affect their clinical implementation.
</details>

### [Knowledge Distillation: Enhancing Neural Network Compression with Integrated Gradients](https://arxiv.org/abs/2503.13008)
*David E. Hernandez,Jose Ramon Chang,Torbj√∂rn E. M. Nordling*
<details>
  <summary>Abstract</summary>
Efficient deployment of deep neural networks on resource-constrained devices
demands advanced compression techniques that preserve accuracy and
interoperability. This paper proposes a machine learning framework that
augments Knowledge Distillation (KD) with Integrated Gradients (IG), an
attribution method, to optimise the compression of convolutional neural
networks. We introduce a novel data augmentation strategy where IG maps,
precomputed from a teacher model, are overlaid onto training images to guide a
compact student model toward critical feature representations. This approach
leverages the teacher's decision-making insights, enhancing the student's
ability to replicate complex patterns with reduced parameters. Experiments on
CIFAR-10 demonstrate the efficacy of our method: a student model, compressed
4.1-fold from the MobileNet-V2 teacher, achieves 92.5% classification accuracy,
surpassing the baseline student's 91.4% and traditional KD approaches, while
reducing inference latency from 140 ms to 13 ms--a tenfold speedup. We perform
hyperparameter optimisation for efficient learning. Comprehensive ablation
studies dissect the contributions of KD and IG, revealing synergistic effects
that boost both performance and model explainability. Our method's emphasis on
feature-level guidance via IG distinguishes it from conventional KD, offering a
data-driven solution for mining transferable knowledge in neural architectures.
This work contributes to machine learning by providing a scalable,
interpretable compression technique, ideal for edge computing applications
where efficiency and transparency are paramount.
</details>

### [Dynamic Relation Inference via Verb Embeddings](https://arxiv.org/abs/2503.13021)
*Omri Suissa,Muhiim Ali,Ariana Azarbal,Hui Shen,Shekhar Pradhan*
<details>
  <summary>Abstract</summary>
CLIP has demonstrated exceptional image-text matching capabilities due to its
training on contrastive learning tasks. Past research has suggested that
whereas CLIP effectively matches text to images when the matching can be
achieved just by matching the text with the objects in the image, CLIP
struggles when the matching depends on representing the relationship among the
objects in the images (i.e., inferring relations). Previous attempts to address
this limitation by training CLIP on relation detection datasets with only
linguistic supervision have met with limited success. In this paper, we offer
insights and practical methods to advance the field of relation inference from
images. This paper approaches the task of creating a model that effectively
detects relations among the objects in images by producing text and image
embeddings that capture relationships through linguistic supervision. To this
end, we propose Dynamic Relation Inference via Verb Embeddings (DRIVE), which
augments the COCO dataset, fine-tunes CLIP with hard negatives
subject-relation-object triples and corresponding images, and introduces a
novel loss function to improve relation detection. Evaluated on multiple
CLIP-based models, our method significantly improves zero-shot relation
inference accuracy in both frozen and fine-tuned settings, significantly
outperforming CLIP and state-of-the-art models while generalizing well on
unseen data.
</details>

### [Permutation Learning with Only N Parameters: From SoftSort to Self-Organizing Gaussians](https://arxiv.org/abs/2503.13051)
*Kai Uwe Barthel,Florian Barthel,Peter Eisert*
<details>
  <summary>Abstract</summary>
Sorting and permutation learning are key concepts in optimization and machine
learning, especially when organizing high-dimensional data into meaningful
spatial layouts. The Gumbel-Sinkhorn method, while effective, requires N*N
parameters to determine a full permutation matrix, making it computationally
expensive for large datasets. Low-rank matrix factorization approximations
reduce memory requirements to 2MN (with M << N), but they still struggle with
very large problems. SoftSort, by providing a continuous relaxation of the
argsort operator, allows differentiable 1D sorting, but it faces challenges
with multidimensional data and complex permutations. In this paper, we present
a novel method for learning permutations using only N parameters, which
dramatically reduces storage costs. Our approach builds on SoftSort, but
extends it by iteratively shuffling the N indices of the elements to be sorted
through a separable learning process. This modification significantly improves
sorting quality, especially for multidimensional data and complex optimization
criteria, and outperforms pure SoftSort. Our method offers improved memory
efficiency and scalability compared to existing approaches, while maintaining
high-quality permutation learning. Its dramatically reduced memory requirements
make it particularly well-suited for large-scale optimization tasks, such as
"Self-Organizing Gaussians", where efficient and scalable permutation learning
is critical.
</details>

### [MaskSDM with Shapley values to improve flexibility, robustness, and explainability in species distribution modeling](https://arxiv.org/abs/2503.13057)
*Robin Zbinden,Nina van Tiel,Gencer Sumbul,Chiara Vanalli,Benjamin Kellenberger,Devis Tuia*
<details>
  <summary>Abstract</summary>
Species Distribution Models (SDMs) play a vital role in biodiversity
research, conservation planning, and ecological niche modeling by predicting
species distributions based on environmental conditions. The selection of
predictors is crucial, strongly impacting both model accuracy and how well the
predictions reflect ecological patterns. To ensure meaningful insights, input
variables must be carefully chosen to match the study objectives and the
ecological requirements of the target species. However, existing SDMs,
including both traditional and deep learning-based approaches, often lack key
capabilities for variable selection: (i) flexibility to choose relevant
predictors at inference without retraining; (ii) robustness to handle missing
predictor values without compromising accuracy; and (iii) explainability to
interpret and accurately quantify each predictor's contribution. To overcome
these limitations, we introduce MaskSDM, a novel deep learning-based SDM that
enables flexible predictor selection by employing a masked training strategy.
This approach allows the model to make predictions with arbitrary subsets of
input variables while remaining robust to missing data. It also provides a
clearer understanding of how adding or removing a given predictor affects model
performance and predictions. Additionally, MaskSDM leverages Shapley values for
precise predictor contribution assessments, improving upon traditional
approximations. We evaluate MaskSDM on the global sPlotOpen dataset, modeling
the distributions of 12,738 plant species. Our results show that MaskSDM
outperforms imputation-based methods and approximates models trained on
specific subsets of variables. These findings underscore MaskSDM's potential to
increase the applicability and adoption of SDMs, laying the groundwork for
developing foundation models in SDMs that can be readily applied to diverse
ecological applications.
</details>

### [Vision-based automatic fruit counting with UAV](https://arxiv.org/abs/2503.13080)
*Hubert Szolc,Mateusz Wasala,Remigiusz Mietla,Kacper Iwicki,Tomasz Kryjak*
<details>
  <summary>Abstract</summary>
The use of unmanned aerial vehicles (UAVs) for smart agriculture is becoming
increasingly popular. This is evidenced by recent scientific works, as well as
the various competitions organised on this topic. Therefore, in this work we
present a system for automatic fruit counting using UAVs. To detect them, our
solution uses a vision algorithm that processes streams from an RGB camera and
a depth sensor using classical image operations. Our system also allows the
planning and execution of flight trajectories, taking into account the
minimisation of flight time and distance covered. We tested the proposed
solution in simulation and obtained an average score of 87.27/100 points from a
total of 500 missions. We also submitted it to the UAV Competition organised as
part of the ICUAS 2024 conference, where we achieved an average score of
84.83/100 points, placing 6th in a field of 23 teams and advancing to the
finals.
</details>

### [Free-form language-based robotic reasoning and grasping](https://arxiv.org/abs/2503.13082)
*Runyu Jiao,Alice Fasoli,Francesco Giuliari,Matteo Bortolon,Sergio Povoli,Guofeng Mei,Yiming Wang,Fabio Poiesi*
<details>
  <summary>Abstract</summary>
Performing robotic grasping from a cluttered bin based on human instructions
is a challenging task, as it requires understanding both the nuances of
free-form language and the spatial relationships between objects.
Vision-Language Models (VLMs) trained on web-scale data, such as GPT-4o, have
demonstrated remarkable reasoning capabilities across both text and images. But
can they truly be used for this task in a zero-shot setting? And what are their
limitations? In this paper, we explore these research questions via the
free-form language-based robotic grasping task, and propose a novel method,
FreeGrasp, leveraging the pre-trained VLMs' world knowledge to reason about
human instructions and object spatial arrangements. Our method detects all
objects as keypoints and uses these keypoints to annotate marks on images,
aiming to facilitate GPT-4o's zero-shot spatial reasoning. This allows our
method to determine whether a requested object is directly graspable or if
other objects must be grasped and removed first. Since no existing dataset is
specifically designed for this task, we introduce a synthetic dataset
FreeGraspData by extending the MetaGraspNetV2 dataset with human-annotated
instructions and ground-truth grasping sequences. We conduct extensive analyses
with both FreeGraspData and real-world validation with a gripper-equipped
robotic arm, demonstrating state-of-the-art performance in grasp reasoning and
execution. Project website: https://tev-fbk.github.io/FreeGrasp/.
</details>

### [Multi-Platform Teach-and-Repeat Navigation by Visual Place Recognition Based on Deep-Learned Local Features](https://arxiv.org/abs/2503.13090)
*V√°clav Truhla≈ô√≠k,Tom√°≈° Pivo≈àka,Michal Kasarda,Libor P≈ôeuƒçil*
<details>
  <summary>Abstract</summary>
Uniform and variable environments still remain a challenge for stable visual
localization and mapping in mobile robot navigation. One of the possible
approaches suitable for such environments is appearance-based teach-and-repeat
navigation, relying on simplified localization and reactive robot motion
control - all without a need for standard mapping. This work brings an
innovative solution to such a system based on visual place recognition
techniques. Here, the major contributions stand in the employment of a new
visual place recognition technique, a novel horizontal shift computation
approach, and a multi-platform system design for applications across various
types of mobile robots. Secondly, a new public dataset for experimental testing
of appearance-based navigation methods is introduced. Moreover, the work also
provides real-world experimental testing and performance comparison of the
introduced navigation system against other state-of-the-art methods. The
results confirm that the new system outperforms existing methods in several
testing scenarios, is capable of operation indoors and outdoors, and exhibits
robustness to day and night scene variations.
</details>

### [MAP: Evaluation and Multi-Agent Enhancement of Large Language Models for Inpatient Pathways](https://arxiv.org/abs/2503.13205)
*Zhen Chen,Zhihao Peng,Xusheng Liang,Cheng Wang,Peigan Liang,Linsheng Zeng,Minjie Ju,Yixuan Yuan*
<details>
  <summary>Abstract</summary>
Inpatient pathways demand complex clinical decision-making based on
comprehensive patient information, posing critical challenges for clinicians.
Despite advancements in large language models (LLMs) in medical applications,
limited research focused on artificial intelligence (AI) inpatient pathways
systems, due to the lack of large-scale inpatient datasets. Moreover, existing
medical benchmarks typically concentrated on medical question-answering and
examinations, ignoring the multifaceted nature of clinical decision-making in
inpatient settings. To address these gaps, we first developed the Inpatient
Pathway Decision Support (IPDS) benchmark from the MIMIC-IV database,
encompassing 51,274 cases across nine triage departments and 17 major disease
categories alongside 16 standardized treatment options. Then, we proposed the
Multi-Agent Inpatient Pathways (MAP) framework to accomplish inpatient pathways
with three clinical agents, including a triage agent managing the patient
admission, a diagnosis agent serving as the primary decision maker at the
department, and a treatment agent providing treatment plans. Additionally, our
MAP framework includes a chief agent overseeing the inpatient pathways to guide
and promote these three clinician agents. Extensive experiments showed our MAP
improved the diagnosis accuracy by 25.10% compared to the state-of-the-art LLM
HuatuoGPT2-13B. It is worth noting that our MAP demonstrated significant
clinical compliance, outperforming three board-certified clinicians by 10%-12%,
establishing a foundation for inpatient pathways systems.
</details>

### [Dense Policy: Bidirectional Autoregressive Learning of Actions](https://arxiv.org/abs/2503.13217)
*Yue Su,Xinyu Zhan,Hongjie Fang,Han Xue,Hao-Shu Fang,Yong-Lu Li,Cewu Lu,Lixin Yang*
<details>
  <summary>Abstract</summary>
Mainstream visuomotor policies predominantly rely on generative models for
holistic action prediction, while current autoregressive policies, predicting
the next token or chunk, have shown suboptimal results. This motivates a search
for more effective learning methods to unleash the potential of autoregressive
policies for robotic manipulation. This paper introduces a bidirectionally
expanded learning approach, termed Dense Policy, to establish a new paradigm
for autoregressive policies in action prediction. It employs a lightweight
encoder-only architecture to iteratively unfold the action sequence from an
initial single frame into the target sequence in a coarse-to-fine manner with
logarithmic-time inference. Extensive experiments validate that our dense
policy has superior autoregressive learning capabilities and can surpass
existing holistic generative policies. Our policy, example data, and training
code will be publicly available upon publication. Project page: https:
//selen-suyue.github.io/DspNet/.
</details>

### [Mind the Gap: Confidence Discrepancy Can Guide Federated Semi-Supervised Learning Across Pseudo-Mismatch](https://arxiv.org/abs/2503.13227)
*Yijie Liu,Xinyi Shang,Yiqun Zhang,Yang Lu,Chen Gong,Jing-Hao Xue,Hanzi Wang*
<details>
  <summary>Abstract</summary>
Federated Semi-Supervised Learning (FSSL) aims to leverage unlabeled data
across clients with limited labeled data to train a global model with strong
generalization ability. Most FSSL methods rely on consistency regularization
with pseudo-labels, converting predictions from local or global models into
hard pseudo-labels as supervisory signals. However, we discover that the
quality of pseudo-label is largely deteriorated by data heterogeneity, an
intrinsic facet of federated learning. In this paper, we study the problem of
FSSL in-depth and show that (1) heterogeneity exacerbates pseudo-label
mismatches, further degrading model performance and convergence, and (2) local
and global models' predictive tendencies diverge as heterogeneity increases.
Motivated by these findings, we propose a simple and effective method called
Semi-supervised Aggregation for Globally-Enhanced Ensemble (SAGE), that can
flexibly correct pseudo-labels based on confidence discrepancies. This strategy
effectively mitigates performance degradation caused by incorrect pseudo-labels
and enhances consensus between local and global models. Experimental results
demonstrate that SAGE outperforms existing FSSL methods in both performance and
convergence. Our code is available at https://github.com/Jay-Codeman/SAGE
</details>

### [Gradient Extrapolation for Debiased Representation Learning](https://arxiv.org/abs/2503.13236)
*Ihab Asaad,Maha Shadaydeh,Joachim Denzler*
<details>
  <summary>Abstract</summary>
Machine learning classification models trained with empirical risk
minimization (ERM) often inadvertently rely on spurious correlations. When
absent in the test data, these unintended associations between non-target
attributes and target labels lead to poor generalization. This paper addresses
this problem from a model optimization perspective and proposes a novel method,
Gradient Extrapolation for Debiased Representation Learning (GERNE), designed
to learn debiased representations in both known and unknown attribute training
cases. GERNE uses two distinct batches with different amounts of spurious
correlations to define the target gradient as the linear extrapolation of two
gradients computed from each batch's loss. It is demonstrated that the
extrapolated gradient, if directed toward the gradient of the batch with fewer
amount of spurious correlation, can guide the training process toward learning
a debiased model. GERNE can serve as a general framework for debiasing with
methods, such as ERM, reweighting, and resampling, being shown as special
cases. The theoretical upper and lower bounds of the extrapolation factor are
derived to ensure convergence. By adjusting this factor, GERNE can be adapted
to maximize the Group-Balanced Accuracy (GBA) or the Worst-Group Accuracy. The
proposed approach is validated on five vision and one NLP benchmarks,
demonstrating competitive and often superior performance compared to
state-of-the-art baseline methods.
</details>

### [Artificial Intelligence-Driven Prognostic Classification of COVID-19 Using Chest X-rays: A Deep Learning Approach](https://arxiv.org/abs/2503.13277)
*Alfred Simbun,Suresh Kumar*
<details>
  <summary>Abstract</summary>
Background: The COVID-19 pandemic has overwhelmed healthcare systems,
emphasizing the need for AI-driven tools to assist in rapid and accurate
patient prognosis. Chest X-ray imaging is a widely available diagnostic tool,
but existing methods for prognosis classification lack scalability and
efficiency. Objective: This study presents a high-accuracy deep learning model
for classifying COVID-19 severity (Mild, Moderate, and Severe) using Chest
X-ray images, developed on Microsoft Azure Custom Vision. Methods: Using a
dataset of 1,103 confirmed COVID-19 X-ray images from AIforCOVID, we trained
and validated a deep learning model leveraging Convolutional Neural Networks
(CNNs). The model was evaluated on an unseen dataset to measure accuracy,
precision, and recall. Results: Our model achieved an average accuracy of 97%,
with specificity of 99%, sensitivity of 87%, and an F1-score of 93.11%. When
classifying COVID-19 severity, the model achieved accuracies of 89.03% (Mild),
95.77% (Moderate), and 81.16% (Severe). These results demonstrate the model's
potential for real-world clinical applications, aiding in faster
decision-making and improved resource allocation. Conclusion: AI-driven
prognosis classification using deep learning can significantly enhance COVID-19
patient management, enabling early intervention and efficient triaging. Our
study provides a scalable, high-accuracy AI framework for integrating deep
learning into routine clinical workflows. Future work should focus on expanding
datasets, external validation, and regulatory compliance to facilitate clinical
adoption.
</details>

### [Integrating AI for Human-Centric Breast Cancer Diagnostics: A Multi-Scale and Multi-View Swin Transformer Framework](https://arxiv.org/abs/2503.13309)
*Farnoush Bayatmakou,Reza Taleei,Milad Amir Toutounchian,Arash Mohammadi*
<details>
  <summary>Abstract</summary>
Despite advancements in Computer-Aided Diagnosis (CAD) systems, breast cancer
remains one of the leading causes of cancer-related deaths among women
worldwide. Recent breakthroughs in Artificial Intelligence (AI) have shown
significant promise in development of advanced Deep Learning (DL) architectures
for breast cancer diagnosis through mammography. In this context, the paper
focuses on the integration of AI within a Human-Centric workflow to enhance
breast cancer diagnostics. Key challenges are, however, largely overlooked such
as reliance on detailed tumor annotations and susceptibility to missing views,
particularly during test time. To address these issues, we propose a hybrid,
multi-scale and multi-view Swin Transformer-based framework (MSMV-Swin) that
enhances diagnostic robustness and accuracy. The proposed MSMV-Swin framework
is designed to work as a decision-support tool, helping radiologists analyze
multi-view mammograms more effectively. More specifically, the MSMV-Swin
framework leverages the Segment Anything Model (SAM) to isolate the breast
lobe, reducing background noise and enabling comprehensive feature extraction.
The multi-scale nature of the proposed MSMV-Swin framework accounts for
tumor-specific regions as well as the spatial characteristics of tissues
surrounding the tumor, capturing both localized and contextual information. The
integration of contextual and localized data ensures that MSMV-Swin's outputs
align with the way radiologists interpret mammograms, fostering better human-AI
interaction and trust. A hybrid fusion structure is then designed to ensure
robustness against missing views, a common occurrence in clinical practice when
only a single mammogram view is available.
</details>

### [LEAVS: An LLM-based Labeler for Abdominal CT Supervision](https://arxiv.org/abs/2503.13330)
*Ricardo Bigolin Lanfredi,Yan Zhuang,Mark Finkelstein,Praveen Thoppey Srinivasan Balamuralikrishna,Luke Krembs,Brandon Khoury,Arthi Reddy,Pritam Mukherjee,Neil M. Rofsky,Ronald M. Summers*
<details>
  <summary>Abstract</summary>
Extracting structured labels from radiology reports has been employed to
create vision models to simultaneously detect several types of abnormalities.
However, existing works focus mainly on the chest region. Few works have been
investigated on abdominal radiology reports due to more complex anatomy and a
wider range of pathologies in the abdomen. We propose LEAVS (Large language
model Extractor for Abdominal Vision Supervision). This labeler can annotate
the certainty of presence and the urgency of seven types of abnormalities for
nine abdominal organs on CT radiology reports. To ensure broad coverage, we
chose abnormalities that encompass most of the finding types from CT reports.
Our approach employs a specialized chain-of-thought prompting strategy for a
locally-run LLM using sentence extraction and multiple-choice questions in a
tree-based decision system. We demonstrate that the LLM can extract several
abnormality types across abdominal organs with an average F1 score of 0.89,
significantly outperforming competing labelers and humans. Additionally, we
show that extraction of urgency labels achieved performance comparable to human
annotations. Finally, we demonstrate that the abnormality labels contain
valuable information for training a single vision model that classifies several
organs as normal or abnormal. We release our code and structured annotations
for a public CT dataset containing over 1,000 CT volumes.
</details>

### [Sightation Counts: Leveraging Sighted User Feedback in Building a BLV-aligned Dataset of Diagram Descriptions](https://arxiv.org/abs/2503.13369)
*Wan Ju Kang,Eunki Kim,Na Min An,Sangryul Kim,Haemin Choi,Ki Hoon Kwak,James Thorne*
<details>
  <summary>Abstract</summary>
Often, the needs and visual abilities differ between the annotator group and
the end user group. Generating detailed diagram descriptions for blind and
low-vision (BLV) users is one such challenging domain. Sighted annotators could
describe visuals with ease, but existing studies have shown that direct
generations by them are costly, bias-prone, and somewhat lacking by BLV
standards. In this study, we ask sighted individuals to assess -- rather than
produce -- diagram descriptions generated by vision-language models (VLM) that
have been guided with latent supervision via a multi-pass inference. The
sighted assessments prove effective and useful to professional educators who
are themselves BLV and teach visually impaired learners. We release Sightation,
a collection of diagram description datasets spanning 5k diagrams and 137k
samples for completion, preference, retrieval, question answering, and
reasoning training purposes and demonstrate their fine-tuning potential in
various downstream tasks.
</details>

### [U2AD: Uncertainty-based Unsupervised Anomaly Detection Framework for Detecting T2 Hyperintensity in MRI Spinal Cord](https://arxiv.org/abs/2503.13400)
*Qi Zhang,Xiuyuan Chen,Ziyi He,Kun Wang,Lianming Wu,Hongxing Shen,Jianqi Sun*
<details>
  <summary>Abstract</summary>
T2 hyperintensities in spinal cord MR images are crucial biomarkers for
conditions such as degenerative cervical myelopathy. However, current clinical
diagnoses primarily rely on manual evaluation. Deep learning methods have shown
promise in lesion detection, but most supervised approaches are heavily
dependent on large, annotated datasets. Unsupervised anomaly detection (UAD)
offers a compelling alternative by eliminating the need for abnormal data
annotations. However, existing UAD methods rely on curated normal datasets and
their performance frequently deteriorates when applied to clinical datasets due
to domain shifts. We propose an Uncertainty-based Unsupervised Anomaly
Detection framework, termed U2AD, to address these limitations. Unlike
traditional methods, U2AD is designed to be trained and tested within the same
clinical dataset, following a "mask-and-reconstruction" paradigm built on a
Vision Transformer-based architecture. We introduce an uncertainty-guided
masking strategy to resolve task conflicts between normal reconstruction and
anomaly detection to achieve an optimal balance. Specifically, we employ a
Monte-Carlo sampling technique to estimate reconstruction uncertainty mappings
during training. By iteratively optimizing reconstruction training under the
guidance of both epistemic and aleatoric uncertainty, U2AD reduces overall
reconstruction variance while emphasizing regions. Experimental results
demonstrate that U2AD outperforms existing supervised and unsupervised methods
in patient-level identification and segment-level localization tasks. This
framework establishes a new benchmark for incorporating uncertainty guidance
into UAD, highlighting its clinical utility in addressing domain shifts and
task conflicts in medical image anomaly detection. Our code is available:
https://github.com/zhibaishouheilab/U2AD
</details>

### [Humanoid Policy ~ Human Policy](https://arxiv.org/abs/2503.13441)
*Ri-Zhao Qiu,Shiqi Yang,Xuxin Cheng,Chaitanya Chawla,Jialong Li,Tairan He,Ge Yan,Lars Paulsen,Ge Yang,Sha Yi,Guanya Shi,Xiaolong Wang*
<details>
  <summary>Abstract</summary>
Training manipulation policies for humanoid robots with diverse data enhances
their robustness and generalization across tasks and platforms. However,
learning solely from robot demonstrations is labor-intensive, requiring
expensive tele-operated data collection which is difficult to scale. This paper
investigates a more scalable data source, egocentric human demonstrations, to
serve as cross-embodiment training data for robot learning. We mitigate the
embodiment gap between humanoids and humans from both the data and modeling
perspectives. We collect an egocentric task-oriented dataset (PH2D) that is
directly aligned with humanoid manipulation demonstrations. We then train a
human-humanoid behavior policy, which we term Human Action Transformer (HAT).
The state-action space of HAT is unified for both humans and humanoid robots
and can be differentiably retargeted to robot actions. Co-trained with
smaller-scale robot data, HAT directly models humanoid robots and humans as
different embodiments without additional supervision. We show that human data
improves both generalization and robustness of HAT with significantly better
data collection efficiency. Code and data: https://human-as-robot.github.io/
</details>

### [MoManipVLA: Transferring Vision-language-action Models for General Mobile Manipulation](https://arxiv.org/abs/2503.13446)
*Zhenyu Wu,Yuheng Zhou,Xiuwei Xu,Ziwei Wang,Haibin Yan*
<details>
  <summary>Abstract</summary>
Mobile manipulation is the fundamental challenge for robotics to assist
humans with diverse tasks and environments in everyday life. However,
conventional mobile manipulation approaches often struggle to generalize across
different tasks and environments because of the lack of large-scale training.
In contrast, recent advances in vision-language-action (VLA) models have shown
impressive generalization capabilities, but these foundation models are
developed for fixed-base manipulation tasks. Therefore, we propose an efficient
policy adaptation framework named MoManipVLA to transfer pre-trained VLA models
of fix-base manipulation to mobile manipulation, so that high generalization
ability across tasks and environments can be achieved in mobile manipulation
policy. Specifically, we utilize pre-trained VLA models to generate waypoints
of the end-effector with high generalization ability. We design motion planning
objectives for the mobile base and the robot arm, which aim at maximizing the
physical feasibility of the trajectory. Finally, we present an efficient
bi-level objective optimization framework for trajectory generation, where the
upper-level optimization predicts waypoints for base movement to enhance the
manipulator policy space, and the lower-level optimization selects the optimal
end-effector trajectory to complete the manipulation task. In this way,
MoManipVLA can adjust the position of the robot base in a zero-shot manner,
thus making the waypoints predicted from the fixed-base VLA models feasible.
Extensive experimental results on OVMM and the real world demonstrate that
MoManipVLA achieves a 4.2% higher success rate than the state-of-the-art mobile
manipulation, and only requires 50 training cost for real world deployment due
to the strong generalization ability in the pre-trained VLA models.
</details>