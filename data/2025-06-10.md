<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 98]
- [cs.CL](#cs.CL) [Total: 2]
- [cs.AI](#cs.AI) [Total: 2]
- [q-bio.BM](#q-bio.BM) [Total: 1]
- [cs.CY](#cs.CY) [Total: 1]
- [math.NA](#math.NA) [Total: 1]
- [cs.HC](#cs.HC) [Total: 2]
- [cs.LG](#cs.LG) [Total: 8]
- [q-bio.NC](#q-bio.NC) [Total: 1]
- [cs.GR](#cs.GR) [Total: 5]
- [eess.IV](#eess.IV) [Total: 7]
- [cs.RO](#cs.RO) [Total: 1]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Towards Reliable AR-Guided Surgical Navigation: Interactive Deformation Modeling with Data-Driven Biomechanics and Prompts](https://arxiv.org/abs/2506.08048)
*Zheng Han,Jun Zhou,Jialun Pei,Jing Qin,Yingfang Fan,Qi Dou*

Main category: cs.CV

TL;DR: 提出结合数据驱动生物力学算法和人机交互框架，提升AR手术导航的形变建模精度与效率


<details>
  <summary>Details</summary>
Motivation: 解决传统有限元方法计算成本高和无法处理术中大解剖形变的局限性，提升AR配准准确性

Method: 开发数据驱动生物力学算法保持FEM精度并提升效率，引入外科医生实时交互校正机制

Result: 配准误差从3.42mm降至2.78mm，计算效率和体积精度超越现有方法

Conclusion: 实现了高效精准的形变建模与人机协同手术决策，推动计算机辅助手术发展

Abstract: In augmented reality (AR)-guided surgical navigation, preoperative organ
models are superimposed onto the patient's intraoperative anatomy to visualize
critical structures such as vessels and tumors. Accurate deformation modeling
is essential to maintain the reliability of AR overlays by ensuring alignment
between preoperative models and the dynamically changing anatomy. Although the
finite element method (FEM) offers physically plausible modeling, its high
computational cost limits intraoperative applicability. Moreover, existing
algorithms often fail to handle large anatomical changes, such as those induced
by pneumoperitoneum or ligament dissection, leading to inaccurate anatomical
correspondences and compromised AR guidance. To address these challenges, we
propose a data-driven biomechanics algorithm that preserves FEM-level accuracy
while improving computational efficiency. In addition, we introduce a novel
human-in-the-loop mechanism into the deformation modeling process. This enables
surgeons to interactively provide prompts to correct anatomical misalignments,
thereby incorporating clinical expertise and allowing the model to adapt
dynamically to complex surgical scenarios. Experiments on a publicly available
dataset demonstrate that our algorithm achieves a mean target registration
error of 3.42 mm. Incorporating surgeon prompts through the interactive
framework further reduces the error to 2.78 mm, surpassing state-of-the-art
methods in volumetric accuracy. These results highlight the ability of our
framework to deliver efficient and accurate deformation modeling while
enhancing surgeon-algorithm collaboration, paving the way for safer and more
reliable computer-assisted surgeries.

</details>


### [2] [ReCogDrive: A Reinforced Cognitive Framework for End-to-End Autonomous Driving](https://arxiv.org/abs/2506.08052)
*Yongkang Li,Kaixin Xiong,Xiangyu Guo,Fang Li,Sixu Yan,Gangwei Xu,Lijun Zhou,Long Chen,Haiyang Sun,Bing Wang,Guang Chen,Hangjun Ye,Wenyu Liu,Xinggang Wang*

Main category: cs.CV

TL;DR: 提出ReCogDrive系统整合视觉语言模型与扩散规划器，三阶段训练显著提升自动驾驶在长尾场景的表现


<details>
  <summary>Details</summary>
Motivation: 现有端到端自动驾驶在长尾场景性能不足，视觉语言模型存在领域差距、维度失配、模仿学习局限三大问题

Method: 三阶段训练范式：驾驶问答数据集预训练VLM→扩散规划器隐式学习→强化学习微调轨迹生成

Result: NAVSIM基准测试PDMS达89.6，超越视觉SOTA方法5.6分

Conclusion: 多阶段训练有效整合世界知识与连续动作空间，生成更安全、拟人的驾驶轨迹，推动自动驾驶技术发展

Abstract: Although end-to-end autonomous driving has made remarkable progress, its
performance degrades significantly in rare and long-tail scenarios. Recent
approaches attempt to address this challenge by leveraging the rich world
knowledge of Vision-Language Models (VLMs), but these methods suffer from
several limitations: (1) a significant domain gap between the pre-training data
of VLMs and real-world driving data, (2) a dimensionality mismatch between the
discrete language space and the continuous action space, and (3) imitation
learning tends to capture the average behavior present in the dataset, which
may be suboptimal even dangerous. In this paper, we propose ReCogDrive, an
autonomous driving system that integrates VLMs with diffusion planner, which
adopts a three-stage paradigm for training. In the first stage, we use a
large-scale driving question-answering datasets to train the VLMs, mitigating
the domain discrepancy between generic content and real-world driving
scenarios. In the second stage, we employ a diffusion-based planner to perform
imitation learning, mapping representations from the latent language space to
continuous driving actions. Finally, we fine-tune the diffusion planner using
reinforcement learning with NAVSIM non-reactive simulator, enabling the model
to generate safer, more human-like driving trajectories. We evaluate our
approach on the planning-oriented NAVSIM benchmark, achieving a PDMS of 89.6
and setting a new state-of-the-art that surpasses the previous vision-only SOTA
by 5.6 PDMS.

</details>


### [3] [CuRe: Cultural Gaps in the Long Tail of Text-to-Image Systems](https://arxiv.org/abs/2506.08071)
*Aniket Rege,Zinnia Nie,Mahesh Ramesh,Unmesh Raskar,Zhuoran Yu,Aditya Kusupati,Yong Jae Lee,Ramya Korlakai Vinayak*

Main category: cs.CV

TL;DR: 提出CuRe基准测试套件，评估文本生成图像系统的文化代表性偏差


<details>
  <summary>Details</summary>
Motivation: 主流文本生成图像系统训练数据过度偏重欧美文化，缺乏全球南方文化代表性

Method: 基于维基媒体知识图谱构建层次化数据集（6大文化轴/32子类/300文物），通过属性指定边际效用作为评估代理

Result: 在SigLIP2、DALL-E3等主流模型中，CuRe指标与人类判定的感知相似性/图文对齐/文化多样性高度相关

Conclusion: CuRe为量化评估和改进AI生成内容的文化包容性提供了标准化工具

Abstract: Popular text-to-image (T2I) systems are trained on web-scraped data, which is
heavily Amero and Euro-centric, underrepresenting the cultures of the Global
South. To analyze these biases, we introduce CuRe, a novel and scalable
benchmarking and scoring suite for cultural representativeness that leverages
the marginal utility of attribute specification to T2I systems as a proxy for
human judgments. Our CuRe benchmark dataset has a novel categorical hierarchy
built from the crowdsourced Wikimedia knowledge graph, with 300 cultural
artifacts across 32 cultural subcategories grouped into six broad cultural axes
(food, art, fashion, architecture, celebrations, and people). Our dataset's
categorical hierarchy enables CuRe scorers to evaluate T2I systems by analyzing
their response to increasing the informativeness of text conditioning, enabling
fine-grained cultural comparisons. We empirically observe much stronger
correlations of our class of scorers to human judgments of perceptual
similarity, image-text alignment, and cultural diversity across image encoders
(SigLIP 2, AIMV2 and DINOv2), vision-language models (OpenCLIP, SigLIP 2,
Gemini 2.0 Flash) and state-of-the-art text-to-image systems, including three
variants of Stable Diffusion (1.5, XL, 3.5 Large), FLUX.1 [dev], Ideogram 2.0,
and DALL-E 3. The code and dataset is open-sourced and available at
https://aniketrege.github.io/cure/.

</details>


### [4] [IGraSS: Learning to Identify Infrastructure Networks from Satellite Imagery by Iterative Graph-constrained Semantic Segmentation](https://arxiv.org/abs/2506.08137)
*Oishee Bintey Hoque,Abhijin Adiga,Aniruddha Adiga,Siddharth Chaudhary,Madhav V. Marathe,S. S. Ravi,Kirti Rajagopalan,Amanda Wilson,Samarth Swarup*

Main category: cs.CV

TL;DR: 提出IGraSS框架，结合语义分割和图优化提升运河网络映射准确率


<details>
  <summary>Details</summary>
Motivation: 现有基础设施映射方法依赖完整标注数据，实际应用中地面实况不完整会影响模型效果，需利用基础设施的图结构特性改进标注

Method: 迭代框架整合多模态（RGB/NDWI/DEM）语义分割模块与图结构优化模块，通过可达性约束迭代修正标注

Result: 不可达运河段从18%降至3%，改进后的标注数据使识别精度显著提升，并在道路网络验证了方法通用性

Conclusion: IGraSS为噪声标注修正和遥感基础设施映射提供通用框架，对水利管理和城市规划具有应用价值

Abstract: Accurate canal network mapping is essential for water management, including
irrigation planning and infrastructure maintenance. State-of-the-art semantic
segmentation models for infrastructure mapping, such as roads, rely on large,
well-annotated remote sensing datasets. However, incomplete or inadequate
ground truth can hinder these learning approaches. Many infrastructure networks
have graph-level properties such as reachability to a source (like canals) or
connectivity (roads) that can be leveraged to improve these existing ground
truth. This paper develops a novel iterative framework IGraSS, combining a
semantic segmentation module-incorporating RGB and additional modalities (NDWI,
DEM)-with a graph-based ground-truth refinement module. The segmentation module
processes satellite imagery patches, while the refinement module operates on
the entire data viewing the infrastructure network as a graph. Experiments show
that IGraSS reduces unreachable canal segments from around 18% to 3%, and
training with refined ground truth significantly improves canal identification.
IGraSS serves as a robust framework for both refining noisy ground truth and
mapping canal networks from remote sensing imagery. We also demonstrate the
effectiveness and generalizability of IGraSS using road networks as an example,
applying a different graph-theoretic constraint to complete road networks.

</details>


### [5] [Spectral Domain Neural Reconstruction for Passband FMCW Radars](https://arxiv.org/abs/2506.08163)
*Harshvardhan Takawale,Nirupam Roy*

Main category: cs.CV

TL;DR: 提出SpINRv2神经网络框架，通过频域前向建模解决高频率FMCW雷达体积重建中的相位混叠问题


<details>
  <summary>Details</summary>
Motivation: 扩展SpINR框架以应对高起始频率下的相位混叠和子仓模糊挑战，提升雷达成像精度

Method: 构建可微分频域前向模型结合隐式神经表示(INR)，引入频谱稀疏性和平滑性正则化消除模糊

Result: 在高频场景下显著超越传统方法和学习基准，计算效率提升并保持频谱保真度

Conclusion: 建立神经雷达3D成像新标准，为高精度雷达体积重建提供有效解决方案

Abstract: We present SpINRv2, a neural framework for high-fidelity volumetric
reconstruction using Frequency-Modulated Continuous-Wave (FMCW) radar.
Extending our prior work (SpINR), this version introduces enhancements that
allow accurate learning under high start frequencies-where phase aliasing and
sub-bin ambiguity become prominent. Our core contribution is a fully
differentiable frequency-domain forward model that captures the complex radar
response using closed-form synthesis, paired with an implicit neural
representation (INR) for continuous volumetric scene modeling. Unlike
time-domain baselines, SpINRv2 directly supervises the complex frequency
spectrum, preserving spectral fidelity while drastically reducing computational
overhead. Additionally, we introduce sparsity and smoothness regularization to
disambiguate sub-bin ambiguities that arise at fine range resolutions.
Experimental results show that SpINRv2 significantly outperforms both classical
and learning-based baselines, especially under high-frequency regimes,
establishing a new benchmark for neural radar-based 3D imaging.

</details>


### [6] [Surgeon Style Fingerprinting and Privacy Risk Quantification via Discrete Diffusion Models in a Vision-Language-Action Framework](https://arxiv.org/abs/2506.08185)
*Huixin Zhan,Jason H. Moore*

Main category: cs.CV

TL;DR: 提出基于扩散框架的视觉-语言-动作模型，实现保留外科医生个性化手术风格的同时暴露隐私风险


<details>
  <summary>Details</summary>
Motivation: 现有AI系统忽视外科医生个性化操作风格，需在保持手术行为特征与用户隐私间取得平衡

Method: 离散扩散框架整合多模态输入，通过自然语言提示编码隐私感知的身份嵌入实现个性化手势预测

Result: 在JIGSAWS数据集实现精准手势重建，发现表现力更强的表征提升性能但增加53%身份泄露风险

Conclusion: 个性化建模需权衡性能增益与隐私风险，为手术AI系统设计提供重要安全启示

Abstract: Surgeons exhibit distinct operating styles due to differences in training,
experience, and motor behavior - yet current AI systems often ignore this
personalization signal. We propose a novel approach to model fine-grained,
surgeon-specific fingerprinting in robotic surgery using a discrete diffusion
framework integrated with a vision-language-action (VLA) pipeline. Our method
formulates gesture prediction as a structured sequence denoising task,
conditioned on multimodal inputs including endoscopic video, surgical intent
language, and a privacy-aware embedding of surgeon identity and skill.
Personalized surgeon fingerprinting is encoded through natural language prompts
using third-party language models, allowing the model to retain individual
behavioral style without exposing explicit identity. We evaluate our method on
the JIGSAWS dataset and demonstrate that it accurately reconstructs gesture
sequences while learning meaningful motion fingerprints unique to each surgeon.
To quantify the privacy implications of personalization, we perform membership
inference attacks and find that more expressive embeddings improve task
performance but simultaneously increase susceptibility to identity leakage.
These findings demonstrate that while personalized embeddings improve
performance, they also increase vulnerability to identity leakage, revealing
the importance of balancing personalization with privacy risk in surgical
modeling. Code is available at:
https://github.com/huixin-zhan-ai/Surgeon_style_fingerprinting.

</details>


### [7] [Open World Scene Graph Generation using Vision Language Models](https://arxiv.org/abs/2506.08189)
*Amartya Dutta,Kazi Sajeed Mehrab,Medha Sawhney,Abhilash Neog,Mridul Khurana,Sepideh Fatemi,Aanish Pradhan,M. Maruf,Ismini Lourentzou,Arka Daw,Anuj Karpatne*

Main category: cs.CV

TL;DR: 提出无需训练的开放世界场景图生成框架，利用预训练视觉语言模型进行零样本推理


<details>
  <summary>Details</summary>
Motivation: 传统场景图生成方法依赖特定数据集监督，难以处理新对象和关系，限制开放世界应用

Method: 结合多模态提示、嵌入对齐和轻量级配对优化策略，实现零样本结构化推理

Result: 在Visual Genome等数据集验证了预训练模型无需训练即可完成关系理解

Conclusion: 证明了预训练VLMs的开放世界场景理解潜力，推动更通用的关系识别技术发展

Abstract: Scene-Graph Generation (SGG) seeks to recognize objects in an image and
distill their salient pairwise relationships. Most methods depend on
dataset-specific supervision to learn the variety of interactions, restricting
their usefulness in open-world settings, involving novel objects and/or
relations. Even methods that leverage large Vision Language Models (VLMs)
typically require benchmark-specific fine-tuning. We introduce Open-World SGG,
a training-free, efficient, model-agnostic framework that taps directly into
the pretrained knowledge of VLMs to produce scene graphs with zero additional
learning. Casting SGG as a zero-shot structured-reasoning problem, our method
combines multimodal prompting, embedding alignment, and a lightweight
pair-refinement strategy, enabling inference over unseen object vocabularies
and relation sets. To assess this setting, we formalize an Open-World
evaluation protocol that measures performance when no SGG-specific data have
been observed either in terms of objects and relations. Experiments on Visual
Genome, Open Images V6, and the Panoptic Scene Graph (PSG) dataset demonstrate
the capacity of pretrained VLMs to perform relational understanding without
task-level training.

</details>


### [8] [Generative Learning of Differentiable Object Models for Compositional Interpretation of Complex Scenes](https://arxiv.org/abs/2506.08191)
*Antoni Nowinowski,Krzysztof Krawiec*

Main category: cs.CV

TL;DR: 扩展DVP架构以处理多物体场景，改进训练方法并建立新基准，显著提升重建与分解性能。


<details>
  <summary>Details</summary>
Motivation: 解决原DVP无法处理多物体的问题，并通过潜在空间优化缓解图像重建损失的高原现象。

Method: 引入多物体处理能力，利用解码器生成训练样本，设计图像与潜在空间联合损失函数。

Result: 新基准测试中优于MONet和LIVE，重建质量提升，重叠物体分解能力显著增强。

Conclusion: 验证了扩展DVP的有效性，揭示了梯度对训练的影响，探讨了可微分渲染的局限性及改进方向。

Abstract: This study builds on the architecture of the Disentangler of Visual Priors
(DVP), a type of autoencoder that learns to interpret scenes by decomposing the
perceived objects into independent visual aspects of shape, size, orientation,
and color appearance. These aspects are expressed as latent parameters which
control a differentiable renderer that performs image reconstruction, so that
the model can be trained end-to-end with gradient using reconstruction loss. In
this study, we extend the original DVP so that it can handle multiple objects
in a scene. We also exploit the interpretability of its latent by using the
decoder to sample additional training examples and devising alternative
training modes that rely on loss functions defined not only in the image space,
but also in the latent space. This significantly facilitates training, which is
otherwise challenging due to the presence of extensive plateaus in the
image-space reconstruction loss. To examine the performance of this approach,
we propose a new benchmark featuring multiple 2D objects, which subsumes the
previously proposed Multi-dSprites dataset while being more parameterizable. We
compare the DVP extended in these ways with two baselines (MONet and LIVE) and
demonstrate its superiority in terms of reconstruction quality and capacity to
decompose overlapping objects. We also analyze the gradients induced by the
considered loss functions, explain how they impact the efficacy of training,
and discuss the limitations of differentiable rendering in autoencoders and the
ways in which they can be addressed.

</details>


### [9] [GIQ: Benchmarking 3D Geometric Reasoning of Vision Foundation Models with Simulated and Real Polyhedra](https://arxiv.org/abs/2506.08194)
*Mateusz Michalkiewicz,Anekha Sokhal,Tadeusz Michalkiewicz,Piotr Pawlikowski,Mahsa Baktashmotlagh,Varun Jampani,Guha Balakrishnan*

Main category: cs.CV

TL;DR: 提出几何推理基准GIQ揭示当前视觉模型在三维形状理解上的重大缺陷


<details>
  <summary>Details</summary>
Motivation: 现有单目3D重建和视觉语言模型在几何理解方面存在认知盲区，缺乏系统评估框架

Method: 构建包含224种多面体的GIQ基准，通过3D重建、对称检测、心理旋转和零样本分类四个维度评估

Result: SOTA模型在基础几何重建错误率高达40%，VLMs对复合结构识别准确率仅13.7%，心理旋转任务准确率不足随机水平

Conclusion: GIQ暴露几何智能关键缺陷，为开发几何感知的表征学习提供结构化评估平台

Abstract: Monocular 3D reconstruction methods and vision-language models (VLMs)
demonstrate impressive results on standard benchmarks, yet their true
understanding of geometric properties remains unclear. We introduce GIQ , a
comprehensive benchmark specifically designed to evaluate the geometric
reasoning capabilities of vision and vision-language foundation models. GIQ
comprises synthetic and real-world images of 224 diverse polyhedra - including
Platonic, Archimedean, Johnson, and Catalan solids, as well as stellations and
compound shapes - covering varying levels of complexity and symmetry. Through
systematic experiments involving monocular 3D reconstruction, 3D symmetry
detection, mental rotation tests, and zero-shot shape classification tasks, we
reveal significant shortcomings in current models. State-of-the-art
reconstruction algorithms trained on extensive 3D datasets struggle to
reconstruct even basic geometric forms accurately. While foundation models
effectively detect specific 3D symmetry elements via linear probing, they
falter significantly in tasks requiring detailed geometric differentiation,
such as mental rotation. Moreover, advanced vision-language assistants exhibit
remarkably low accuracy on complex polyhedra, systematically misinterpreting
basic properties like face geometry, convexity, and compound structures. GIQ is
publicly available, providing a structured platform to highlight and address
critical gaps in geometric intelligence, facilitating future progress in
robust, geometry-aware representation learning.

</details>


### [10] [A Comprehensive Study of Decoder-Only LLMs for Text-to-Image Generation](https://arxiv.org/abs/2506.08210)
*Andrew Z. Wang,Songwei Ge,Tero Karras,Ming-Yu Liu,Yogesh Balaji*

Main category: cs.CV

TL;DR: 研究发现使用全层归一化嵌入的LLM文本编码器可提升文本到图像生成质量


<details>
  <summary>Details</summary>
Motivation: 探索先进的大型语言模型替代传统T5/CLIP作为文本编码器的可行性

Method: 构建标准化训练框架，对比测试12种文本编码器在27个模型中的表现，分析嵌入提取方式和模型规模的影响

Result: 全层归一化平均嵌入显著提升提示对齐能力，多数LLM性能超越T5基准模型

Conclusion: LLM作为文本编码器能增强视觉语言推理能力，为文本到图像系统优化提供新方向

Abstract: Both text-to-image generation and large language models (LLMs) have made
significant advancements. However, many text-to-image models still employ the
somewhat outdated T5 and CLIP as their text encoders. In this work, we
investigate the effectiveness of using modern decoder-only LLMs as text
encoders for text-to-image diffusion models. We build a standardized training
and evaluation pipeline that allows us to isolate and evaluate the effect of
different text embeddings. We train a total of 27 text-to-image models with 12
different text encoders to analyze the critical aspects of LLMs that could
impact text-to-image generation, including the approaches to extract
embeddings, different LLMs variants, and model sizes. Our experiments reveal
that the de facto way of using last-layer embeddings as conditioning leads to
inferior performance. Instead, we explore embeddings from various layers and
find that using layer-normalized averaging across all layers significantly
improves alignment with complex prompts. Most LLMs with this conditioning
outperform the baseline T5 model, showing enhanced performance in advanced
visio-linguistic reasoning skills.

</details>


### [11] [Using Satellite Images And Self-supervised Machine Learning Networks To Detect Water Hidden Under Vegetation](https://arxiv.org/abs/2506.08214)
*Ioannis Iakovidis,Zahra Kalantari,Amir Hossein Payberah,Fernando Jaramillo,Francisco Pena Escobar*

Main category: cs.CV

TL;DR: 提出自监督集成模型实现无标注雷达影像水体分割，IoU指标提升0.02


<details>
  <summary>Details</summary>
Motivation: 解决湿地遥感监测中人工标注成本高、效率低的问题

Method: 结合深度聚类与负采样进行自监督训练，采用集成方法降低方差

Result: 自监督集成模型在IoU指标上优于全监督单模型0.02

Conclusion: 证明了无监督方法在遥感图像分割中的有效性，降低了对人工标注的依赖

Abstract: In recent years the wide availability of high-resolution radar satellite
images along with the advancement of computer vision models have enabled the
remote monitoring of the surface area of wetlands. However, these models
require large amounts of manually annotated satellite images, which are slow
and expensive to produce. To overcome this problem, self-supervised training
methods have been deployed to train models without using annotated data. In
this paper we use a combination of deep clustering and negative sampling to
train a model to segment radar satellite images into areas that separate water
from land without the use of any manual annotations. Furthermore, we implement
an ensemble version of the model to reduce variance and improve performance.
Compared to a single fully-supervised model using the same architecture, our
ensemble of self-supervised models achieves a 0.02 improvement in the
Intersection Over Union metric over our test dataset.

</details>


### [12] [Jamais Vu: Exposing the Generalization Gap in Supervised Semantic Correspondence](https://arxiv.org/abs/2506.08220)
*Octave Mariotti,Zhipeng Du,Yash Bhalgat,Oisin Mac Aodha,Hakan Bilen*

Main category: cs.CV

TL;DR: 提出通过3D规范流形学习稠密语义对应的方法，显著提升未标注关键点的泛化能力


<details>
  <summary>Details</summary>
Motivation: 现有监督方法受限于稀疏标注关键点，无法有效处理未标注区域

Method: 利用单目深度估计将2D关键点映射到无监督的3D规范空间，构建连续几何流形

Result: 新方法在未见过关键点上超越监督基线71.3%，发现无监督方法跨数据集表现更优

Conclusion: 结合3D几何表示和无监督学习能有效提升语义对应的泛化能力，SPair-U数据集推动领域评估

Abstract: Semantic correspondence (SC) aims to establish semantically meaningful
matches across different instances of an object category. We illustrate how
recent supervised SC methods remain limited in their ability to generalize
beyond sparsely annotated training keypoints, effectively acting as keypoint
detectors. To address this, we propose a novel approach for learning dense
correspondences by lifting 2D keypoints into a canonical 3D space using
monocular depth estimation. Our method constructs a continuous canonical
manifold that captures object geometry without requiring explicit 3D
supervision or camera annotations. Additionally, we introduce SPair-U, an
extension of SPair-71k with novel keypoint annotations, to better assess
generalization. Experiments not only demonstrate that our model significantly
outperforms supervised baselines on unseen keypoints, highlighting its
effectiveness in learning robust correspondences, but that unsupervised
baselines outperform supervised counterparts when generalized across different
datasets.

</details>


### [13] [A Good CREPE needs more than just Sugar: Investigating Biases in Compositional Vision-Language Benchmarks](https://arxiv.org/abs/2506.08227)
*Vishaal Udandarao,Mehdi Cherti,Shyamgopal Karthik,Jenia Jitsev,Samuel Albanie,Matthias Bethge*

Main category: cs.CV

TL;DR: 现有视觉语言组合理解评测基准存在设计偏差，简单启发式方法可达到CLIP模型性能，需改进评估方法


<details>
  <summary>Details</summary>
Motivation: 揭示常用视觉语言组合理解评测基准的设计缺陷及其对模型能力评估的影响

Method: 分析17个基准的数据来源、负样本构建方法，发现分布不对称问题并进行系统评估

Result: 启发式方法表现匹敌CLIP，基准测试有效性存疑；正负样本分布不对称是核心问题

Conclusion: 需优化基准设计流程以构建更可靠的评测标准，避免被简单特征攻击

Abstract: We investigate 17 benchmarks (e.g. SugarCREPE, VALSE) commonly used for
measuring compositional understanding capabilities of vision-language models
(VLMs). We scrutinize design choices in their construction, including data
source (e.g. MS-COCO) and curation procedures (e.g. constructing negative
images/captions), uncovering several inherent biases across most benchmarks. We
find that blind heuristics (e.g. token-length, log-likelihood under a language
model) perform on par with CLIP models, indicating that these benchmarks do not
effectively measure compositional understanding. We demonstrate that the
underlying factor is a distribution asymmetry between positive and negative
images/captions, induced by the benchmark construction procedures. To mitigate
these issues, we provide a few key recommendations for constructing more robust
vision-language compositional understanding benchmarks, that would be less
prone to such simple attacks.

</details>


### [14] [Highly Compressed Tokenizer Can Generate Without Training](https://arxiv.org/abs/2506.08257)
*L. Lao Beyer,T. Li,X. Chen,S. Karaman,K. He*

Main category: cs.CV

TL;DR: 提出基于一维图像分词器的梯度优化图像生成方法，无需训练生成模型即可实现图像编辑与生成


<details>
  <summary>Details</summary>
Motivation: 探索高度压缩的1D图像分词器在隐空间表达特性，挖掘其启发式操作潜力

Method: 结合向量量化1D分词器与基于梯度的测试时优化，使用重建损失和CLIP相似度损失函数

Result: 通过简单的token操作实现细粒度图像编辑，在修复和文本引导编辑中生成多样逼真结果

Conclusion: 1D图像表征在保持语义表达能力的同时，为免训练生成方法开辟了新路径

Abstract: Commonly used image tokenizers produce a 2D grid of spatially arranged
tokens. In contrast, so-called 1D image tokenizers represent images as highly
compressed one-dimensional sequences of as few as 32 discrete tokens. We find
that the high degree of compression achieved by a 1D tokenizer with vector
quantization enables image editing and generative capabilities through
heuristic manipulation of tokens, demonstrating that even very crude
manipulations -- such as copying and replacing tokens between latent
representations of images -- enable fine-grained image editing by transferring
appearance and semantic attributes. Motivated by the expressivity of the 1D
tokenizer's latent space, we construct an image generation pipeline leveraging
gradient-based test-time optimization of tokens with plug-and-play loss
functions such as reconstruction or CLIP similarity. Our approach is
demonstrated for inpainting and text-guided image editing use cases, and can
generate diverse and realistic samples without requiring training of any
generative model.

</details>


### [15] [Seeing Voices: Generating A-Roll Video from Audio with Mirage](https://arxiv.org/abs/2506.08279)
*Aditi Sundararaman,Amogh Adishesha,Andrew Jaegle,Dan Bigioi,Hyoung-Kyu Song,Jon Kyl,Justin Mao,Kevin Lan,Mojtaba Komeili,ShahRukh Athar,Sheila Babayan,Stanislau Beliasau,William Buchwalter*

Main category: cs.CV

TL;DR: 提出音频到视频生成基础模型Mirage，通过自注意力机制实现音视融合的高质量视频生成


<details>
  <summary>Details</summary>
Motivation: 现有视频生成方法忽视音频或局限于特定领域，需实现泛用的音视整合生成方法

Method: 基于自注意力架构的统一训练方法，支持从头训练或迁移学习，结合文本转语音技术

Result: 生成结果主观质量优于专用架构方法，可呈现语音驱动的逼真人物表现

Conclusion: 为多模态生成提供通用解决方案，突破现有方法的领域限制，推动音视协同生成发展

Abstract: From professional filmmaking to user-generated content, creators and
consumers have long recognized that the power of video depends on the
harmonious integration of what we hear (the video's audio track) with what we
see (the video's image sequence). Current approaches to video generation either
ignore sound to focus on general-purpose but silent image sequence generation
or address both visual and audio elements but focus on restricted application
domains such as re-dubbing. We introduce Mirage, an audio-to-video foundation
model that excels at generating realistic, expressive output imagery from
scratch given an audio input. When integrated with existing methods for speech
synthesis (text-to-speech, or TTS), Mirage results in compelling multimodal
video. When trained on audio-video footage of people talking (A-roll) and
conditioned on audio containing speech, Mirage generates video of people
delivering a believable interpretation of the performance implicit in input
audio. Our central technical contribution is a unified method for training
self-attention-based audio-to-video generation models, either from scratch or
given existing weights. This methodology allows Mirage to retain generality as
an approach to audio-to-video generation while producing outputs of superior
subjective quality to methods that incorporate audio-specific architectures or
loss components specific to people, speech, or details of how images or audio
are captured. We encourage readers to watch and listen to the results of Mirage
for themselves (see paper and comments for links).

</details>


### [16] [SEMA: a Scalable and Efficient Mamba like Attention via Token Localization and Averaging](https://arxiv.org/abs/2506.08297)
*Nhat Thanh Tran,Fanghui Xue,Shuai Zhang,Jiancheng Lyu,Yunling Zheng,Yingyong Qi,Jack Xin*

Main category: cs.CV

TL;DR: 提出SEMA注意力机制，结合Mamba架构解决传统注意力计算复杂度高和聚焦不足的问题


<details>
  <summary>Details</summary>
Motivation: 传统softmax注意力存在平方计算复杂度问题，线性注意力又缺乏聚焦能力，制约视觉任务性能

Method: 建立广义注意力数学框架，基于分散性理论设计SEMA，结合token定位保持焦点，算术平均捕获全局特征

Result: 在ImageNet-1k分类任务中超越现有视觉Mamba模型，在大尺寸图像上表现更优

Conclusion: SEMA为视觉任务提供了可扩展的高效注意力新范式，证明了理论框架指导架构设计的有效性

Abstract: Attention is the critical component of a transformer. Yet the quadratic
computational complexity of vanilla full attention in the input size and the
inability of its linear attention variant to focus have been challenges for
computer vision tasks. We provide a mathematical definition of generalized
attention and formulate both vanilla softmax attention and linear attention
within the general framework. We prove that generalized attention disperses,
that is, as the number of keys tends to infinity, the query assigns equal
weights to all keys. Motivated by the dispersion property and recent
development of Mamba form of attention, we design Scalable and Efficient Mamba
like Attention (SEMA) which utilizes token localization to avoid dispersion and
maintain focusing, complemented by theoretically consistent arithmetic
averaging to capture global aspect of attention. We support our approach on
Imagenet-1k where classification results show that SEMA is a scalable and
effective alternative beyond linear attention, outperforming recent vision
Mamba models on increasingly larger scales of images at similar model parameter
sizes.

</details>


### [17] [OpenRR-1k: A Scalable Dataset for Real-World Reflection Removal](https://arxiv.org/abs/2506.08299)
*Kangning Yang,Ling Ouyang,Huiming Sun,Jie Cai,Lan Fu,Jiaming Ding,Chiu Man Ho,Zibo Meng*

Main category: cs.CV

TL;DR: 提出OpenRR-1k高质量反射去除数据集，通过新范式解决现有数据不足问题


<details>
  <summary>Details</summary>
Motivation: 现有反射去除技术缺乏高质量真实场景数据集

Method: 设计便捷、经济的采集范式，构建1000对真实场景的像素级对齐透射-反射图像数据集

Result: 实验证明该数据集有效提升算法在真实复杂场景的鲁棒性

Conclusion: 开源数据集为反射去除技术的实际应用提供更可靠的基准支持

Abstract: Reflection removal technology plays a crucial role in photography and
computer vision applications. However, existing techniques are hindered by the
lack of high-quality in-the-wild datasets. In this paper, we propose a novel
paradigm for collecting reflection datasets from a fresh perspective. Our
approach is convenient, cost-effective, and scalable, while ensuring that the
collected data pairs are of high quality, perfectly aligned, and represent
natural and diverse scenarios. Following this paradigm, we collect a
Real-world, Diverse, and Pixel-aligned dataset (named OpenRR-1k dataset), which
contains 1,000 high-quality transmission-reflection image pairs collected in
the wild. Through the analysis of several reflection removal methods and
benchmark evaluation experiments on our dataset, we demonstrate its
effectiveness in improving robustness in challenging real-world environments.
Our dataset is available at https://github.com/caijie0620/OpenRR-1k.

</details>


### [18] [Hyperspectral Image Classification via Transformer-based Spectral-Spatial Attention Decoupling and Adaptive Gating](https://arxiv.org/abs/2506.08324)
*Guandong Li,Mengxia Ye*

Main category: cs.CV

TL;DR: 提出STNet网络架构，通过双注意力解耦和门控机制增强高光谱图像分类性能


<details>
  <summary>Details</summary>
Motivation: 解决高光谱图像分类中数据维度高、地物稀疏、光谱冗余导致的过拟合和泛化不足问题

Method: 设计空间-光谱Transformer模块：显式解耦空间/光谱注意力，引入自适应注意力融合门控和GFFN特征转换门控

Result: 在IN/UP/KSC数据集上超越主流方法，平衡模型表征能力与过拟合风险

Conclusion: 通过注意力机制创新实现高效特征提取，为非增量式网络设计提供新思路

Abstract: Deep neural networks face several challenges in hyperspectral image
classification, including high-dimensional data, sparse distribution of ground
objects, and spectral redundancy, which often lead to classification
overfitting and limited generalization capability. To more effectively extract
and fuse spatial context with fine spectral information in hyperspectral image
(HSI) classification, this paper proposes a novel network architecture called
STNet. The core advantage of STNet stems from the dual innovative design of its
Spatial-Spectral Transformer module: first, the fundamental explicit decoupling
of spatial and spectral attention ensures targeted capture of key information
in HSI; second, two functionally distinct gating mechanisms perform intelligent
regulation at both the fusion level of attention flows (adaptive attention
fusion gating) and the internal level of feature transformation (GFFN). This
characteristic demonstrates superior feature extraction and fusion capabilities
compared to traditional convolutional neural networks, while reducing
overfitting risks in small-sample and high-noise scenarios. STNet enhances
model representation capability without increasing network depth or width. The
proposed method demonstrates superior performance on IN, UP, and KSC datasets,
outperforming mainstream hyperspectral image classification approaches.

</details>


### [19] [Locating Tennis Ball Impact on the Racket in Real Time Using an Event Camera](https://arxiv.org/abs/2506.08327)
*Yuto Kase,Kai Ishibe,Ryoma Yasuda,Yudai Washida,Sakiko Hashimoto*

Main category: cs.CV

TL;DR: 提出基于事件相机的实时网球击球定位方法，解决传统高速相机内存消耗大和人工检测误差问题


<details>
  <summary>Details</summary>
Motivation: 传统高速相机内存消耗高且依赖人工标注，无法持续捕捉运动场景影响运动员表现分析

Method: 三阶段识别流程（挥拍时间段/击球时刻/球拍轮廓检测），结合传统CV与原创极性不对称时间对称性分析法(PATS)

Result: 测量误差符合网球性能分析允许范围，计算时间满足实时性需求

Conclusion: 事件相机实现低功耗高速监测，为个性化运动装备设计提供可靠技术支持

Abstract: In racket sports, such as tennis, locating the ball's position at impact is
important in clarifying player and equipment characteristics, thereby aiding in
personalized equipment design. High-speed cameras are used to measure the
impact location; however, their excessive memory consumption limits prolonged
scene capture, and manual digitization for position detection is time-consuming
and prone to human error. These limitations make it difficult to effectively
capture the entire playing scene, hindering the ability to analyze the player's
performance. We propose a method for locating the tennis ball impact on the
racket in real time using an event camera. Event cameras efficiently measure
brightness changes (called `events') with microsecond accuracy under high-speed
motion while using lower memory consumption. These cameras enable users to
continuously monitor their performance over extended periods. Our method
consists of three identification steps: time range of swing, timing at impact,
and contours of ball and racket. Conventional computer vision techniques are
utilized along with an original event-based processing to detect the timing at
impact (PATS: the amount of polarity asymmetry in time symmetry). The results
of the experiments were within the permissible range for measuring tennis
players' performance. Moreover, the computation time was sufficiently short for
real-time applications.

</details>


### [20] [How Much To Guide: Revisiting Adaptive Guidance in Classifier-Free Guidance Text-to-Vision Diffusion Models](https://arxiv.org/abs/2506.08351)
*Huixuan Zhang,Junzhe Zhang,Xiaojun Wan*

Main category: cs.CV

TL;DR: 提出自适应去噪策略Step AG，在保持生成质量的同时将扩散模型推理速度提升20%-30%


<details>
  <summary>Details</summary>
Motivation: 传统无分类器引导方法需要双倍前向步骤导致效率低下

Method: 仅在前几个去噪步骤应用分类器引导的通用自适应策略

Result: 在图像质量/文本对齐指标达标的条件下实现20-30%加速，视频生成等多场景验证有效

Conclusion: 为扩散模型提供了一种高效且通用的条件生成解决方案

Abstract: With the rapid development of text-to-vision generation diffusion models,
classifier-free guidance has emerged as the most prevalent method for
conditioning. However, this approach inherently requires twice as many steps
for model forwarding compared to unconditional generation, resulting in
significantly higher costs. While previous study has introduced the concept of
adaptive guidance, it lacks solid analysis and empirical results, making
previous method unable to be applied to general diffusion models. In this work,
we present another perspective of applying adaptive guidance and propose Step
AG, which is a simple, universally applicable adaptive guidance strategy. Our
evaluations focus on both image quality and image-text alignment. whose results
indicate that restricting classifier-free guidance to the first several
denoising steps is sufficient for generating high-quality, well-conditioned
images, achieving an average speedup of 20% to 30%. Such improvement is
consistent across different settings such as inference steps, and various
models including video generation models, highlighting the superiority of our
method.

</details>


### [21] [MedMoE: Modality-Specialized Mixture of Experts for Medical Vision-Language Understanding](https://arxiv.org/abs/2506.08356)
*Shivang Chopra,Lingchao Mao,Gabriela Sanchez-Rodriguez,Andrew J Feola,Jing Li,Zsolt Kira*

Main category: cs.CV

TL;DR: 提出动态适应医学影像模态的MedMoE框架，通过专家混合模块提升视觉-语言对齐效果。


<details>
  <summary>Details</summary>
Motivation: 针对医学影像多模态特性，解决现有方法采用统一特征提取策略忽略模态特异性需求的问题。

Method: 基于报告类型条件化的MoE模块，通过Swin Transformer特征金字塔实现多尺度专家分支路由，捕获模态特异性视觉语义。

Result: 在多个医学基准测试中提高了跨模态对齐和检索性能，验证了专业视觉表征的价值。

Conclusion: 证明了临床视觉语言系统中模态专用表征的重要性，为自适应医学图像理解提供了有效框架。

Abstract: Different medical imaging modalities capture diagnostic information at
varying spatial resolutions, from coarse global patterns to fine-grained
localized structures. However, most existing vision-language frameworks in the
medical domain apply a uniform strategy for local feature extraction,
overlooking the modality-specific demands. In this work, we present MedMoE, a
modular and extensible vision-language processing framework that dynamically
adapts visual representation based on the diagnostic context. MedMoE
incorporates a Mixture-of-Experts (MoE) module conditioned on the report type,
which routes multi-scale image features through specialized expert branches
trained to capture modality-specific visual semantics. These experts operate
over feature pyramids derived from a Swin Transformer backbone, enabling
spatially adaptive attention to clinically relevant regions. This framework
produces localized visual representations aligned with textual descriptions,
without requiring modality-specific supervision at inference. Empirical results
on diverse medical benchmarks demonstrate that MedMoE improves alignment and
retrieval performance across imaging modalities, underscoring the value of
modality-specialized visual representations in clinical vision-language
systems.

</details>


### [22] [Image Demoir茅ing Using Dual Camera Fusion on Mobile Phones](https://arxiv.org/abs/2506.08361)
*Yanting Mei,Zhilu Zhang,Xiaohe Wu,Wangmeng Zuo*

Main category: cs.CV

TL;DR: 提出双摄像头融合的DCID方法，利用超广角图像消除广角图像摩尔纹


<details>
  <summary>Details</summary>
Motivation: 现有方法难以去除严重摩尔纹，现代智能手机双摄系统可利用不同焦段特性互补

Method: 轻量化超广角编码器+两阶段快速图像对齐，构建9000样本的真实数据集

Result: 实验显示优于现有SOTA方法，提供开源代码与首个大规模真实摩尔纹数据集

Conclusion: 验证了双摄融合策略的有效性，为移动端图像修复提供实用解决方案

Abstract: When shooting electronic screens, moir\'e patterns usually appear in captured
images, which seriously affects the image quality. Existing image demoir\'eing
methods face great challenges in removing large and heavy moir\'e. To address
the issue, we propose to utilize Dual Camera fusion for Image Demoir\'eing
(DCID), \ie, using the ultra-wide-angle (UW) image to assist the moir\'e
removal of wide-angle (W) image. This is inspired by two motivations: (1) the
two lenses are commonly equipped with modern smartphones, (2) the UW image
generally can provide normal colors and textures when moir\'e exists in the W
image mainly due to their different focal lengths. In particular, we propose an
efficient DCID method, where a lightweight UW image encoder is integrated into
an existing demoir\'eing network and a fast two-stage image alignment manner is
present. Moreover, we construct a large-scale real-world dataset with diverse
mobile phones and monitors, containing about 9,000 samples. Experiments on the
dataset show our method performs better than state-of-the-art methods. Code and
dataset are available at https://github.com/Mrduckk/DCID.

</details>


### [23] [SECOND: Mitigating Perceptual Hallucination in Vision-Language Models via Selective and Contrastive Decoding](https://arxiv.org/abs/2506.08391)
*Woohyeon Park,Woojin Kim,Jaeik Kim,Jaeyoung Do*

Main category: cs.CV

TL;DR: 提出SECOND方法，通过选择性对比解码整合多尺度视觉信息，有效减少视觉语言模型中的物体幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型(VLM)因物体幻觉问题导致视觉理解受限，需改进多尺度信息利用方式。

Method: 采用以物体为中心的选择性对比解码机制，渐进整合多尺度视觉特征并进行迭代对比。

Result: 跨基准测试表现优异，证实多尺度优先对比策略显著降低感知幻觉且优于现有方法。

Conclusion: 揭示了多尺度视觉信息优化整合的潜力，为提升VLM的视觉理解准确性提供新方向。

Abstract: Despite significant advancements in Vision-Language Models (VLMs), the
performance of existing VLMs remains hindered by object hallucination, a
critical challenge to achieving accurate visual understanding. To address this
issue, we propose SECOND: Selective and Contrastive Decoding, a novel approach
that enables VLMs to effectively leverage multi-scale visual information with
an object-centric manner, closely aligning with human visual perception. SECOND
progressively selects and integrates multi-scale visual information,
facilitating a more precise interpretation of images. By contrasting these
visual information iteratively, SECOND significantly reduces perceptual
hallucinations and outperforms a wide range of benchmarks. Our theoretical
analysis and experiments highlight the largely unexplored potential of
multi-scale application in VLMs, showing that prioritizing and contrasting
across scales outperforms existing methods.

</details>


### [24] [RadioDUN: A Physics-Inspired Deep Unfolding Network for Radio Map Estimation](https://arxiv.org/abs/2506.08418)
*Taiqin Chen,Zikun Zhou,Zheng Fang,Wenzhen Zou,Kanjun Liu,Ke Chen,Yongbing Zhang,Yaowei Wang*

Main category: cs.CV

TL;DR: 提出深度展开网络RadioDUN，结合物理传播特性和动态重加权模块，解决稀疏样本下无线电地图估计问题


<details>
  <summary>Details</summary>
Motivation: 现有基于深度学习的无线电地图估计方法难以融合物理传播特性，且稀疏样本导致重建困难

Method: 将问题建模为稀疏信号恢复，通过物理模型分解优化子问题，提出集成动态重加权模块(DRM)和阴影损耗的RadioDUN网络

Result: 实验证明方法在无线电地图估计精度上优于现有技术，阴影损耗有效提升模型性能

Conclusion: 通过深度融合物理传播模型和深度学习，为无线通信系统的资源优化提供了更精确的电磁环境建模方案

Abstract: The radio map represents the spatial distribution of spectrum resources
within a region, supporting efficient resource allocation and interference
mitigation. However, it is difficult to construct a dense radio map as a
limited number of samples can be measured in practical scenarios. While
existing works have used deep learning to estimate dense radio maps from sparse
samples, they are hard to integrate with the physical characteristics of the
radio map. To address this challenge, we cast radio map estimation as the
sparse signal recovery problem. A physical propagation model is further
incorporated to decompose the problem into multiple factor optimization
sub-problems, thereby reducing recovery complexity. Inspired by the existing
compressive sensing methods, we propose the Radio Deep Unfolding Network
(RadioDUN) to unfold the optimization process, achieving adaptive parameter
adjusting and prior fitting in a learnable manner. To account for the radio
propagation characteristics, we develop a dynamic reweighting module (DRM) to
adaptively model the importance of each factor for the radio map. Inspired by
the shadowing factor in the physical propagation model, we integrate
obstacle-related factors to express the obstacle-induced signal stochastic
decay. The shadowing loss is further designed to constrain the factor
prediction and act as a supplementary supervised objective, which enhances the
performance of RadioDUN. Extensive experiments have been conducted to
demonstrate that the proposed method outperforms the state-of-the-art methods.
Our code will be made publicly available upon publication.

</details>


### [25] [Better Reasoning with Less Data: Enhancing VLMs Through Unified Modality Scoring](https://arxiv.org/abs/2506.08429)
*Mingjie Xu,Andrew Estornell,Hongzheng Yang,Yuzhi Zhao,Zhaowei Zhu,Qi Xuan,Jiaheng Wei*

Main category: cs.CV

TL;DR: 提出SCALE跨模态评估框架，通过质量驱动筛选提升视觉语言模型训练数据质量


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型训练数据存在图文对齐噪声和文本模糊问题，制约模型性能提升

Method: 整合跨模态评估框架，包含任务分类、多维度描述生成（场景/对象/风格等），以及基于5个指标（对齐度/清晰度/任务稀缺性等）的质量评估

Result: 发现单模态评估可能低估关键样本价值，验证生成描述可将多模态任务有效转换为统一文本模态

Conclusion: SCALE框架为构建高质量多模态数据集提供新范式，通过科学筛选机制推动视觉语言模型发展

Abstract: The application of visual instruction tuning and other post-training
techniques has significantly enhanced the capabilities of Large Language Models
(LLMs) in visual understanding, enriching Vision-Language Models (VLMs) with
more comprehensive visual language datasets. However, the effectiveness of VLMs
is highly dependent on large-scale, high-quality datasets that ensure precise
recognition and accurate reasoning. Two key challenges hinder progress: (1)
noisy alignments between images and the corresponding text, which leads to
misinterpretation, and (2) ambiguous or misleading text, which obscures visual
content. To address these challenges, we propose SCALE (Single modality data
quality and Cross modality Alignment Evaluation), a novel quality-driven data
selection pipeline for VLM instruction tuning datasets. Specifically, SCALE
integrates a cross-modality assessment framework that first assigns each data
entry to its appropriate vision-language task, generates general and
task-specific captions (covering scenes, objects, style, etc.), and evaluates
the alignment, clarity, task rarity, text coherence, and image clarity of each
entry based on the generated captions. We reveal that: (1) current unimodal
quality assessment methods evaluate one modality while overlooking the rest,
which can underestimate samples essential for specific tasks and discard the
lower-quality instances that help build model robustness; and (2) appropriately
generated image captions provide an efficient way to transfer the image-text
multimodal task into a unified text modality.

</details>


### [26] [Enhancing Motion Dynamics of Image-to-Video Models via Adaptive Low-Pass Guidance](https://arxiv.org/abs/2506.08456)
*June Suk Choi,Kyungmin Lee,Sihyun Yu,Yisol Choi,Jinwoo Shin,Kimin Lee*

Main category: cs.CV

TL;DR: 提出自适应低通引导(ALG)，通过调整输入图像高频细节的暴露时机，改善图像到视频生成模型的动态性


<details>
  <summary>Details</summary>
Motivation: 现有图像到视频生成模型因过早接触高频细节导致视频动态性下降的现象

Method: 在去噪采样早期阶段使用自适应低通滤波调节条件图像频域信息

Result: 动态度平均提升36%，同时保持帧质量与文本对齐，VBench-I2V评测显著改进

Conclusion: 通过时序控制高频信息暴露，实现动态性与静态保真度的有效平衡，推动可控视频生成发展

Abstract: Recent text-to-video (T2V) models have demonstrated strong capabilities in
producing high-quality, dynamic videos. To improve the visual controllability,
recent works have considered fine-tuning pre-trained T2V models to support
image-to-video (I2V) generation. However, such adaptation frequently suppresses
motion dynamics of generated outputs, resulting in more static videos compared
to their T2V counterparts. In this work, we analyze this phenomenon and
identify that it stems from the premature exposure to high-frequency details in
the input image, which biases the sampling process toward a shortcut trajectory
that overfits to the static appearance of the reference image. To address this,
we propose adaptive low-pass guidance (ALG), a simple fix to the I2V model
sampling procedure to generate more dynamic videos without compromising
per-frame image quality. Specifically, ALG adaptively modulates the frequency
content of the conditioning image by applying low-pass filtering at the early
stage of denoising. Extensive experiments demonstrate that ALG significantly
improves the temporal dynamics of generated videos, while preserving image
fidelity and text alignment. Especially, under VBench-I2V test suite, ALG
achieves an average improvement of 36% in dynamic degree without a significant
drop in video quality or image fidelity.

</details>


### [27] [MARMOT: Masked Autoencoder for Modeling Transient Imaging](https://arxiv.org/abs/2506.08470)
*Siyuan Shen,Ziheng Wang,Xingyue Peng,Suan Xia,Ruiqian Li,Shiying Li,Jingyi Yu*

Main category: cs.CV

TL;DR: 提出基于掩码自编码器的MARMOT模型，用于非视距瞬态成像的自监督预训练和迁移学习


<details>
  <summary>Details</summary>
Motivation: 解决现有非视距成像方法缺乏有效数据先验利用的问题，将预训练范式引入瞬态成像领域

Method: 使用Transformer编码器-解码器架构，通过扫描模式掩码的遮蔽-重构机制，在50万合成瞬态数据集TransVerse上进行预训练

Result: 定量和定性实验表明模型在成像任务中优于现有方法，证明了自监督特征学习和解码器微调的有效性

Conclusion: 首次展示了自监督预训练在瞬态成像领域的潜力，为复杂非视距应用提供了新的解决方案

Abstract: Pretrained models have demonstrated impressive success in many modalities
such as language and vision. Recent works facilitate the pretraining paradigm
in imaging research. Transients are a novel modality, which are captured for an
object as photon counts versus arrival times using a precisely time-resolved
sensor. In particular for non-line-of-sight (NLOS) scenarios, transients of
hidden objects are measured beyond the sensor's direct line of sight. Using
NLOS transients, the majority of previous works optimize volume density or
surfaces to reconstruct the hidden objects and do not transfer priors learned
from datasets. In this work, we present a masked autoencoder for modeling
transient imaging, or MARMOT, to facilitate NLOS applications. Our MARMOT is a
self-supervised model pretrianed on massive and diverse NLOS transient
datasets. Using a Transformer-based encoder-decoder, MARMOT learns features
from partially masked transients via a scanning pattern mask (SPM), where the
unmasked subset is functionally equivalent to arbitrary sampling, and predicts
full measurements. Pretrained on TransVerse-a synthesized transient dataset of
500K 3D models-MARMOT adapts to downstream imaging tasks using direct feature
transfer or decoder finetuning. Comprehensive experiments are carried out in
comparisons with state-of-the-art methods. Quantitative and qualitative results
demonstrate the efficiency of our MARMOT.

</details>


### [28] [Context-aware TFL: A Universal Context-aware Contrastive Learning Framework for Temporal Forgery Localization](https://arxiv.org/abs/2506.08493)
*Qilin Yin,Wei Lu,Xiangyang Luo,Xiaochun Cao*

Main category: cs.CV

TL;DR: 提出通用上下文感知对比学习框架UniCaCLF，显著提升视频片段篡改定位性能


<details>
  <summary>Details</summary>
Motivation: 现有深度伪造检测仅关注整体分类，无法应对真实场景中视频片段局部篡改的定位需求

Method: 结合异构激活操作与自适应上下文更新器构建对比目标，通过监督式逐样本对比编码增强特征区分度

Result: 在五个公开数据集上超越当前最佳算法

Conclusion: 首次将上下文感知对比学习引入时序伪造定位，为实际取证应用提供了有效解决方案

Abstract: Most research efforts in the multimedia forensics domain have focused on
detecting forgery audio-visual content and reached sound achievements. However,
these works only consider deepfake detection as a classification task and
ignore the case where partial segments of the video are tampered with. Temporal
forgery localization (TFL) of small fake audio-visual clips embedded in real
videos is still challenging and more in line with realistic application
scenarios. To resolve this issue, we propose a universal context-aware
contrastive learning framework (UniCaCLF) for TFL. Our approach leverages
supervised contrastive learning to discover and identify forged instants by
means of anomaly detection, allowing for the precise localization of temporal
forged segments. To this end, we propose a novel context-aware perception layer
that utilizes a heterogeneous activation operation and an adaptive context
updater to construct a context-aware contrastive objective, which enhances the
discriminability of forged instant features by contrasting them with genuine
instant features in terms of their distances to the global context. An
efficient context-aware contrastive coding is introduced to further push the
limit of instant feature distinguishability between genuine and forged instants
in a supervised sample-by-sample manner, suppressing the cross-sample influence
to improve temporal forgery localization performance. Extensive experimental
results over five public datasets demonstrate that our proposed UniCaCLF
significantly outperforms the state-of-the-art competing algorithms.

</details>


### [29] [MLVTG: Mamba-Based Feature Alignment and LLM-Driven Purification for Multi-Modal Video Temporal Grounding](https://arxiv.org/abs/2506.08512)
*Zhiyi Zhu,Xiaoyu Wu,Zihao Liu,Linlin Yang*

Main category: cs.CV

TL;DR: 提出MLVTG框架，结合Mamba结构增强时序建模与LLM语义先验，提升视频时间定位精度


<details>
  <summary>Details</summary>
Motivation: 针对Transformer模型在视频时序定位任务中存在的冗余注意力计算和多模态对齐不足问题

Method: MambaAligner使用Vision Mamba建模时序依赖，LLMRefiner利用冻结LLM层传递语义先验，实现双模态对齐

Result: 在QVHighlights等三个基准测试中达到SOTA，显著超越现有基线方法

Conclusion: 通过结构化状态空间建模和文本先验语义提取的双重对齐策略，为多模态视频理解任务提供了新思路

Abstract: Video Temporal Grounding (VTG), which aims to localize video clips
corresponding to natural language queries, is a fundamental yet challenging
task in video understanding. Existing Transformer-based methods often suffer
from redundant attention and suboptimal multi-modal alignment. To address these
limitations, we propose MLVTG, a novel framework that integrates two key
modules: MambaAligner and LLMRefiner. MambaAligner uses stacked Vision Mamba
blocks as a backbone instead of Transformers to model temporal dependencies and
extract robust video representations for multi-modal alignment. LLMRefiner
leverages the specific frozen layer of a pre-trained Large Language Model (LLM)
to implicitly transfer semantic priors, enhancing multi-modal alignment without
fine-tuning. This dual alignment strategy, temporal modeling via structured
state-space dynamics and semantic purification via textual priors, enables more
precise localization. Extensive experiments on QVHighlights, Charades-STA, and
TVSum demonstrate that MLVTG achieves state-of-the-art performance and
significantly outperforms existing baselines.

</details>


### [30] [Robust Visual Localization via Semantic-Guided Multi-Scale Transformer](https://arxiv.org/abs/2506.08526)
*Zhongtao Tian,Wenhao Huang,Zhidong Chen,Xiao Wei Sun*

Main category: cs.CV

TL;DR: 提出结合多尺度特征与语义理解的Transformer框架，提升动态环境下视觉定位鲁棒性


<details>
  <summary>Details</summary>
Motivation: 现有姿态回归方法在动态环境（光照变化/移动物体）中表现不佳，需提升环境适应性

Method: 分层Transformer+跨尺度注意力融合几何细节与上下文，神经场景表示进行语义监督学习视角不变特征

Result: 在TartanAir数据集上超越现有方法，尤其在动态对象/光照变化/遮挡场景表现优异

Conclusion: 多尺度处理与语义引导的结合为现实动态环境提供鲁棒视觉定位方案

Abstract: Visual localization remains challenging in dynamic environments where
fluctuating lighting, adverse weather, and moving objects disrupt appearance
cues. Despite advances in feature representation, current absolute pose
regression methods struggle to maintain consistency under varying conditions.
To address this challenge, we propose a framework that synergistically combines
multi-scale feature learning with semantic scene understanding. Our approach
employs a hierarchical Transformer with cross-scale attention to fuse geometric
details and contextual cues, preserving spatial precision while adapting to
environmental changes. We improve the performance of this architecture with
semantic supervision via neural scene representation during training, guiding
the network to learn view-invariant features that encode persistent structural
information while suppressing complex environmental interference. Experiments
on TartanAir demonstrate that our approach outperforms existing pose regression
methods in challenging scenarios with dynamic objects, illumination changes,
and occlusions. Our findings show that integrating multi-scale processing with
semantic guidance offers a promising strategy for robust visual localization in
real-world dynamic environments.

</details>


### [31] [LiftVSR: Lifting Image Diffusion to Video Super-Resolution via Hybrid Temporal Modeling with Only 4$\times$RTX 4090s](https://arxiv.org/abs/2506.08529)
*Xijun Wang,Xin Li,Bingchen Li,Zhibo Chen*

Main category: cs.CV

TL;DR: 提出高效视频超分辨率框架LiftVSR，结合动态时间注意力与记忆缓存机制，在降低计算成本的同时保持长时一致性。


<details>
  <summary>Details</summary>
Motivation: 现有视频超分辨率方法存在长时一致性差和计算成本高的问题，尤其是处理长视频时需消耗大量资源。

Method: 基于PixArt-α的图像扩散先验，设计混合时序建模机制：DTA处理短片段细粒度时序关系，AMC通过缓存单元聚合跨片段长时信息，并引入非对称采样策略稳定推理。

Result: 在多个基准测试中取得SOTA性能，仅需4块RTX 4090 GPU即可实现，计算成本显著低于传统方法（原需8块A100-80G）。

Conclusion: LiftVSR通过高效时序建模平衡质量与计算开销，为长视频处理提供实用解决方案，推动扩散模型在视频领域的应用落地。

Abstract: Diffusion models have significantly advanced video super-resolution (VSR) by
enhancing perceptual quality, largely through elaborately designed temporal
modeling to ensure inter-frame consistency. However, existing methods usually
suffer from limited temporal coherence and prohibitively high computational
costs (e.g., typically requiring over 8 NVIDIA A100-80G GPUs), especially for
long videos. In this work, we propose LiftVSR, an efficient VSR framework that
leverages and elevates the image-wise diffusion prior from PixArt-$\alpha$,
achieving state-of-the-art results using only 4$\times$RTX 4090 GPUs. To
balance long-term consistency and efficiency, we introduce a hybrid temporal
modeling mechanism that decomposes temporal learning into two complementary
components: (i) Dynamic Temporal Attention (DTA) for fine-grained temporal
modeling within short frame segment ($\textit{i.e.}$, low complexity), and (ii)
Attention Memory Cache (AMC) for long-term temporal modeling across segments
($\textit{i.e.}$, consistency). Specifically, DTA identifies multiple token
flows across frames within multi-head query and key tokens to warp inter-frame
contexts in the value tokens. AMC adaptively aggregates historical segment
information via a cache unit, ensuring long-term coherence with minimal
overhead. To further stabilize the cache interaction during inference, we
introduce an asymmetric sampling strategy that mitigates feature mismatches
arising from different diffusion sampling steps. Extensive experiments on
several typical VSR benchmarks have demonstrated that LiftVSR achieves
impressive performance with significantly lower computational costs.

</details>


### [32] [TrajFlow: Multi-modal Motion Prediction via Flow Matching](https://arxiv.org/abs/2506.08541)
*Qi Yan,Brian Zhang,Yutong Zhang,Daniel Yang,Joshua White,Di Chen,Jiachao Liu,Langechuan Liu,Binnan Zhuang,Shaoshuai Shi,Renjie Liao*

Main category: cs.CV

TL;DR: 提出基于流匹配的TrajFlow框架，显著提升自动驾驶轨迹预测效率与多模态性能


<details>
  <summary>Details</summary>
Motivation: 解决现有生成式轨迹预测方法在扩展性、计算效率和不确定性估计方面的不足，满足自动驾驶安全决策需求

Method: 结合流匹配实现单次多轨迹预测，提出Plackett-Luce排序损失改进不确定性评估，设计自我条件训练技术加速推理

Result: 在Waymo数据集关键指标达到SOTA，推理速度提升且保持预测一致性

Conclusion: 通过流匹配框架与创新训练技术，为安全关键场景提供高效可靠的多模态轨迹预测解决方案

Abstract: Efficient and accurate motion prediction is crucial for ensuring safety and
informed decision-making in autonomous driving, particularly under dynamic
real-world conditions that necessitate multi-modal forecasts. We introduce
TrajFlow, a novel flow matching-based motion prediction framework that
addresses the scalability and efficiency challenges of existing generative
trajectory prediction methods. Unlike conventional generative approaches that
employ i.i.d. sampling and require multiple inference passes to capture diverse
outcomes, TrajFlow predicts multiple plausible future trajectories in a single
pass, significantly reducing computational overhead while maintaining coherence
across predictions. Moreover, we propose a ranking loss based on the
Plackett-Luce distribution to improve uncertainty estimation of predicted
trajectories. Additionally, we design a self-conditioning training technique
that reuses the model's own predictions to construct noisy inputs during a
second forward pass, thereby improving generalization and accelerating
inference. Extensive experiments on the large-scale Waymo Open Motion Dataset
(WOMD) demonstrate that TrajFlow achieves state-of-the-art performance across
various key metrics, underscoring its effectiveness for safety-critical
autonomous driving applications. The code and other details are available on
the project website https://traj-flow.github.io/.

</details>


### [33] [Convergence of Spectral Principal Paths: How Deep Networks Distill Linear Representations from Noisy Inputs](https://arxiv.org/abs/2506.08543)
*Bowei Tian,Xuntao Lyu,Meng Liu,Hongyi Wang,Ang Li*

Main category: cs.CV

TL;DR: 提出输入空间线性假设和频谱主路径框架，解释深度网络如何形成概念对齐的表征并验证其多模态鲁棒性


<details>
  <summary>Details</summary>
Motivation: 基于线性表征假设，探索深度网络如何从输入空间构建人类可解释的语义方向，以提升AI透明度和可控性

Method: 提出输入空间线性假设(ISLH)和频谱主路径(SPP)框架，形式化深度网络中线性表征的渐进提炼过程

Result: 在视觉语言模型中验证了线性表征的多模态鲁棒性，揭示了表征沿主要频谱方向逐步强化的机制

Conclusion: 为深度网络表征形成建立结构化理论框架，推动提升AI系统的鲁棒性、公平性和可解释性发展

Abstract: High-level representations have become a central focus in enhancing AI
transparency and control, shifting attention from individual neurons or
circuits to structured semantic directions that align with human-interpretable
concepts. Motivated by the Linear Representation Hypothesis (LRH), we propose
the Input-Space Linearity Hypothesis (ISLH), which posits that concept-aligned
directions originate in the input space and are selectively amplified with
increasing depth. We then introduce the Spectral Principal Path (SPP)
framework, which formalizes how deep networks progressively distill linear
representations along a small set of dominant spectral directions. Building on
this framework, we further demonstrate the multimodal robustness of these
representations in Vision-Language Models (VLMs). By bridging theoretical
insights with empirical validation, this work advances a structured theory of
representation formation in deep networks, paving the way for improving AI
robustness, fairness, and transparency.

</details>


### [34] [From Pixels to Graphs: using Scene and Knowledge Graphs for HD-EPIC VQA Challenge](https://arxiv.org/abs/2506.08553)
*Agnese Taluzzi,Davide Gesualdi,Riccardo Santambrogio,Chiara Plizzari,Francesca Palermo,Simone Mentasti,Matteo Matteucci*

Main category: cs.CV

TL;DR: 提出SceneNet与KnowledgeNet融合方法，通过场景图与外部知识提升复杂第一视角视觉问答性能


<details>
  <summary>Details</summary>
Motivation: 解决复杂自我中心视角VQA任务需要同时处理细粒度视觉特征与常识推理能力

Method: SceneNet用多模态大模型构建场景图捕捉时空关系，KnowledgeNet整合ConceptNet知识建立语义连接

Result: 在HD-EPIC七大类别中取得差异化优势，组合框架达到44.21%整体准确率

Conclusion: 多模态特征与外部知识融合有效提升复杂视觉推理任务的性能验证

Abstract: This report presents SceneNet and KnowledgeNet, our approaches developed for
the HD-EPIC VQA Challenge 2025. SceneNet leverages scene graphs generated with
a multi-modal large language model (MLLM) to capture fine-grained object
interactions, spatial relationships, and temporally grounded events. In
parallel, KnowledgeNet incorporates ConceptNet's external commonsense knowledge
to introduce high-level semantic connections between entities, enabling
reasoning beyond directly observable visual evidence. Each method demonstrates
distinct strengths across the seven categories of the HD-EPIC benchmark, and
their combination within our framework results in an overall accuracy of 44.21%
on the challenge, highlighting its effectiveness for complex egocentric VQA
tasks.

</details>


### [35] [Towards Cross-Subject EMG Pattern Recognition via Dual-Branch Adversarial Feature Disentanglement](https://arxiv.org/abs/2506.08555)
*Xinyue Niu,Akira Furui*

Main category: cs.CV

TL;DR: 提出双分支对抗神经网络实现无需校准的跨被试肌电模式识别


<details>
  <summary>Details</summary>
Motivation: 解决传统跨被试EMG识别依赖用户校准数据的问题，消除模型校准需求

Method: 端到端双分支对抗网络解耦特征，分离模式相关和用户相关特征分量

Result: 在未见用户数据上表现优于基线方法，支持任务不变的生物特征识别

Conclusion: 为无校准跨被试EMG识别提供新范式，拓展了生物特征系统的应用潜力

Abstract: Cross-subject electromyography (EMG) pattern recognition faces significant
challenges due to inter-subject variability in muscle anatomy, electrode
placement, and signal characteristics. Traditional methods rely on
subject-specific calibration data to adapt models to new users, an approach
that is both time-consuming and impractical for large-scale, real-world
deployment. This paper presents an approach to eliminate calibration
requirements through feature disentanglement, enabling effective cross-subject
generalization. We propose an end-to-end dual-branch adversarial neural network
that simultaneously performs pattern recognition and individual identification
by disentangling EMG features into pattern-specific and subject-specific
components. The pattern-specific components facilitate robust pattern
recognition for new users without model calibration, while the subject-specific
components enable downstream applications such as task-invariant biometric
identification. Experimental results demonstrate that the proposed model
achieves robust performance on data from unseen users, outperforming various
baseline methods in cross-subject scenarios. Overall, this study offers a new
perspective for cross-subject EMG pattern recognition without model calibration
and highlights the proposed model's potential for broader applications, such as
task-independent biometric systems.

</details>


### [36] [Hierarchical Neural Collapse Detection Transformer for Class Incremental Object Detection](https://arxiv.org/abs/2506.08562)
*Duc Thanh Pham,Hong Dang Nguyen,Nhat Minh Nguyen Quoc,Linh Ngo Van,Sang Dinh Viet,Duc Anh Nguyen*

Main category: cs.CV

TL;DR: 提出Hier-DETR框架，利用神经崩溃和层次类别关系提升增量目标检测的效率和性能


<details>
  <summary>Details</summary>
Motivation: 现实场景中新物体不断出现，现有增量目标检测模型存在性能不足和推理时间长的问题，需要更实用的解决方案

Method: 结合神经崩溃理论处理数据不平衡问题，利用类别标签的层次关系构建检测框架

Result: 实现高效推理并保持竞争优势，同时解决灾难性遗忘问题

Conclusion: Hier-DETR为持续学习场景下的目标检测提供了更实用的解决方案，平衡了效率与准确性

Abstract: Recently, object detection models have witnessed notable performance
improvements, particularly with transformer-based models. However, new objects
frequently appear in the real world, requiring detection models to continually
learn without suffering from catastrophic forgetting. Although Incremental
Object Detection (IOD) has emerged to address this challenge, these existing
models are still not practical due to their limited performance and prolonged
inference time. In this paper, we introduce a novel framework for IOD, called
Hier-DETR: Hierarchical Neural Collapse Detection Transformer, ensuring both
efficiency and competitive performance by leveraging Neural Collapse for
imbalance dataset and Hierarchical relation of classes' labels.

</details>


### [37] [Generating Vision-Language Navigation Instructions Incorporated Fine-Grained Alignment Annotations](https://arxiv.org/abs/2506.08566)
*Yibo Cui,Liang Xie,Yu Zhao,Jiawei Sun,Erwei Yin*

Main category: cs.CV

TL;DR: 提出生成框架FCA-NIG，构建首个细粒度跨模态对齐数据集FCA-R2R，显著提升VLN代理性能。


<details>
  <summary>Details</summary>
Motivation: 现有VLN数据集缺乏子指令和实体级对齐标注，导致导航决策精度不足。

Method: 通过轨迹分割、GLIP地标检测、指令生成和CLIP实体选择构建子指令-轨迹对齐数据，整合为FCA-R2R数据集。

Result: FCA-R2R训练使SF、EnvDrop等模型性能显著提升，状态感知与泛化能力增强。

Conclusion: 无需人工标注即可生成高质量数据，推进复杂导航任务中细粒度跨模态对齐研究。

Abstract: Vision-Language Navigation (VLN) enables intelligent agents to navigate
environments by integrating visual perception and natural language
instructions, yet faces significant challenges due to the scarcity of
fine-grained cross-modal alignment annotations. Existing datasets primarily
focus on global instruction-trajectory matching, neglecting
sub-instruction-level and entity-level alignments critical for accurate
navigation action decision-making. To address this limitation, we propose
FCA-NIG, a generative framework that automatically constructs navigation
instructions with dual-level fine-grained cross-modal annotations. In this
framework, an augmented trajectory is first divided into sub-trajectories,
which are then processed through GLIP-based landmark detection, crafted
instruction construction, OFA-Speaker based R2R-like instruction generation,
and CLIP-powered entity selection, generating sub-instruction-trajectory pairs
with entity-landmark annotations. Finally, these sub-pairs are aggregated to
form a complete instruction-trajectory pair. The framework generates the
FCA-R2R dataset, the first large-scale augmentation dataset featuring precise
sub-instruction-sub-trajectory and entity-landmark alignments. Extensive
experiments demonstrate that training with FCA-R2R significantly improves the
performance of multiple state-of-the-art VLN agents, including SF, EnvDrop,
RecBERT, and HAMT. Incorporating sub-instruction-trajectory alignment enhances
agents' state awareness and decision accuracy, while entity-landmark alignment
further boosts navigation performance and generalization. These results
highlight the effectiveness of FCA-NIG in generating high-quality, scalable
training data without manual annotation, advancing fine-grained cross-modal
learning in complex navigation tasks.

</details>


### [38] [Diversity-Guided MLP Reduction for Efficient Large Vision Transformers](https://arxiv.org/abs/2506.08591)
*Chengchao Shen,Hourun Zhu,Gongfan Fang,Jianxin Wang,Xinchao Wang*

Main category: cs.CV

TL;DR: 提出DGMR方法通过多样性引导的MLP压缩，大幅降低视觉Transformer参数量且保持性能


<details>
  <summary>Details</summary>
Motivation: 针对大型视觉Transformer中MLP模块参数冗余导致的运算成本过高问题

Method: 采用Gram-Schmidt权重剪枝策略消除MLP冗余神经元，配合权重多样性保持技术实现高效蒸馏恢复

Result: 在EVA-CLIP-E等模型上实现71.5%参数/计算量压缩，仅需0.06%无标签数据即可恢复原始性能

Conclusion: 首次在大规模视觉Transformer中实现超过70%的压缩率且性能无损，为降低大模型部署成本提供新方案

Abstract: Transformer models achieve excellent scaling property, where the performance
is improved with the increment of model capacity. However, large-scale model
parameters lead to an unaffordable cost of computing and memory. We analyze
popular transformer architectures and find that multilayer perceptron (MLP)
modules take up the majority of model parameters. To this end, we focus on the
recoverability of the compressed models and propose a Diversity-Guided MLP
Reduction (DGMR) method to significantly reduce the parameters of large vision
transformers with only negligible performance degradation. Specifically, we
conduct a Gram-Schmidt weight pruning strategy to eliminate redundant neurons
of MLP hidden layer, while preserving weight diversity for better performance
recover during distillation. Compared to the model trained from scratch, our
pruned model only requires 0.06\% data of LAION-2B (for the training of large
vision transformers) without labels (ImageNet-1K) to recover the original
performance. Experimental results on several state-of-the-art large vision
transformers demonstrate that our method achieves a more than 57.0\% parameter
and FLOPs reduction in a near lossless manner. Notably, for EVA-CLIP-E (4.4B),
our method accomplishes a 71.5\% parameter and FLOPs reduction without
performance degradation. The source code and trained weights are available at
https://github.com/visresearch/DGMR.

</details>


### [39] [Transformers Meet Hyperspectral Imaging: A Comprehensive Study of Models, Challenges and Open Problems](https://arxiv.org/abs/2506.08596)
*Guyang Zhang,Waleed Abdulla*

Main category: cs.CV

TL;DR: 首个关于Transformer在高光谱成像分类中的端到端系统调研，分析技术路径与挑战


<details>
  <summary>Details</summary>
Motivation: 针对Transformer模型虽擅长长程依赖但HSI应用尚不成熟的现状，旨在系统梳理技术演进路线

Method: 从预处理到损失设计的全流程解剖，对比不同组件设计在空间-光谱特征提取中的适用性

Result: 揭示当前面临标注数据稀缺、超高维处理、算力消耗和可解释性四大核心挑战

Conclusion: 为下一代HSI开发指明方向：构建基准数据集、边缘计算优化、环境鲁棒性提升及可解释注意力机制设计

Abstract: Transformers have become the architecture of choice for learning long-range
dependencies, yet their adoption in hyperspectral imaging (HSI) is still
emerging. We reviewed more than 300 papers published up to 2025 and present the
first end-to-end survey dedicated to Transformer-based HSI classification. The
study categorizes every stage of a typical pipeline-pre-processing, patch or
pixel tokenization, positional encoding, spatial-spectral feature extraction,
multi-head self-attention variants, skip connections, and loss design-and
contrasts alternative design choices with the unique spatial-spectral
properties of HSI. We map the field's progress against persistent obstacles:
scarce labeled data, extreme spectral dimensionality, computational overhead,
and limited model explainability. Finally, we outline a research agenda
prioritizing valuable public data sets, lightweight on-edge models,
illumination and sensor shifts robustness, and intrinsically interpretable
attention mechanisms. Our goal is to guide researchers in selecting, combining,
or extending Transformer components that are truly fit for purpose for
next-generation HSI applications.

</details>


### [40] [Towards Class-wise Fair Adversarial Training via Anti-Bias Soft Label Distillation](https://arxiv.org/abs/2506.08611)
*Shiji Zhao,Chi Chen,Ranjie Duan,Xizhe Wang,Xingxing Wei*

Main category: cs.CV

TL;DR: 提出ABSLD方法通过调整类别软标签平滑度解决对抗训练的鲁棒公平性问题


<details>
  <summary>Details</summary>
Motivation: 现有对抗训练方法存在类别间鲁棒性差异（易类/难类性能悬殊）的公平性问题

Method: 基于知识蒸馏框架，通过动态分配类间不同温度参数调整软标签平滑度，降低跨类误差差距

Result: 实验表明ABSLD在鲁棒性和公平性综合指标上优于现有方法

Conclusion: 揭示了软标签平滑度与鲁棒公平性的内在联系，提出的标签平滑调整机制有效提升模型公平性

Abstract: Adversarial Training (AT) is widely recognized as an effective approach to
enhance the adversarial robustness of Deep Neural Networks. As a variant of AT,
Adversarial Robustness Distillation (ARD) has shown outstanding performance in
enhancing the robustness of small models. However, both AT and ARD face robust
fairness issue: these models tend to display strong adversarial robustness
against some classes (easy classes) while demonstrating weak adversarial
robustness against others (hard classes). This paper explores the underlying
factors of this problem and points out the smoothness degree of soft labels for
different classes significantly impacts the robust fairness from both empirical
observation and theoretical analysis. Based on the above exploration, we
propose Anti-Bias Soft Label Distillation (ABSLD) within the Knowledge
Distillation framework to enhance the adversarial robust fairness.
Specifically, ABSLD adaptively reduces the student's error risk gap between
different classes, which is accomplished by adjusting the class-wise smoothness
degree of teacher's soft labels during the training process, and the adjustment
is managed by assigning varying temperatures to different classes.
Additionally, as a label-based approach, ABSLD is highly adaptable and can be
integrated with the sample-based methods. Extensive experiments demonstrate
ABSLD outperforms state-of-the-art methods on the comprehensive performance of
robustness and fairness.

</details>


### [41] [Data-Efficient Challenges in Visual Inductive Priors: A Retrospective](https://arxiv.org/abs/2506.08612)
*Robert-Jan Bruintjes,Attila Lengyel,Osman Semih Kayhan,Davide Zambrano,Nergis T枚men,Hadi Jamali-Rad,Jan van Gemert*

Main category: cs.CV

TL;DR: 通过数据稀缺场景挑战赛发现，结合Transformer与CNN的模型集成和增强数据方法有效提升深度学习效率


<details>
  <summary>Details</summary>
Motivation: 解决数据不足导致深度学习模型性能下降问题，通过限制训练数据量和禁用迁移学习的挑战赛激发创新方法开发

Method: 组织四届数据稀缺挑战赛，强制要求参赛者仅使用少量样本从头训练模型，禁用预训练权重与迁移学习

Result: 成功方案采用Transformer-CNN混合集成模型、高强度数据增强，部分案例验证了基于先验知识方法的有效性

Conclusion: 挑战赛机制推动数据高效学习方法发展，证明模型架构创新与先验知识整合对突破数据瓶颈具有关键作用

Abstract: Deep Learning requires large amounts of data to train models that work well.
In data-deficient settings, performance can be degraded. We investigate which
Deep Learning methods benefit training models in a data-deficient setting, by
organizing the "VIPriors: Visual Inductive Priors for Data-Efficient Deep
Learning" workshop series, featuring four editions of data-impaired challenges.
These challenges address the problem of training deep learning models for
computer vision tasks with limited data. Participants are limited to training
models from scratch using a low number of training samples and are not allowed
to use any form of transfer learning. We aim to stimulate the development of
novel approaches that incorporate prior knowledge to improve the data
efficiency of deep learning models. Successful challenge entries make use of
large model ensembles that mix Transformers and CNNs, as well as heavy data
augmentation. Novel prior knowledge-based methods contribute to success in some
entries.

</details>


### [42] [SAMSelect: A Spectral Index Search for Marine Debris Visualization using Segment Anything](https://arxiv.org/abs/2506.08613)
*Joost van Dalen,Yuki M. Asano,Marc Russwurm*

Main category: cs.CV

TL;DR: 提出SAMSelect算法，通过Segment Anything模型自动选择最优三通道组合，提升海洋漂浮碎片可视化效果


<details>
  <summary>Details</summary>
Motivation: 海洋漂浮碎片因成分异质性和中等分辨率影像影响难以可视化，现有方法依赖专家经验手动选择波段组合效率低下

Method: 利用Segment Anything模型在标注数据上筛选分类准确率最高的三通道组合，假设最优分割结果对应最佳可视化效果

Result: 在加纳和南非场景测试中发现新型B8-B2归一化指数组合，性能超越文献常用指数

Conclusion: 算法为海洋领域提供自动化的可视化解决方案，开源代码将推动基于视觉解译的遥感研究

Abstract: This work proposes SAMSelect, an algorithm to obtain a salient three-channel
visualization for multispectral images. We develop SAMSelect and show its use
for marine scientists visually interpreting floating marine debris in
Sentinel-2 imagery. These debris are notoriously difficult to visualize due to
their compositional heterogeneity in medium-resolution imagery. Out of these
difficulties, a visual interpretation of imagery showing marine debris remains
a common practice by domain experts, who select bands and spectral indices on a
case-by-case basis informed by common practices and heuristics. SAMSelect
selects the band or index combination that achieves the best classification
accuracy on a small annotated dataset through the Segment Anything Model. Its
central assumption is that the three-channel visualization achieves the most
accurate segmentation results also provide good visual information for
photo-interpretation.
  We evaluate SAMSelect in three Sentinel-2 scenes containing generic marine
debris in Accra, Ghana, and Durban, South Africa, and deployed plastic targets
from the Plastic Litter Project. This reveals the potential of new previously
unused band combinations (e.g., a normalized difference index of B8, B2), which
demonstrate improved performance compared to literature-based indices. We
describe the algorithm in this paper and provide an open-source code repository
that will be helpful for domain scientists doing visual photo interpretation,
especially in the marine field.

</details>


### [43] [A Probability-guided Sampler for Neural Implicit Surface Rendering](https://arxiv.org/abs/2506.08619)
*Gon莽alo Dias Pais,Valter Piedade,Moitreya Chatterjee,Marcus Greiff,Pedro Miraldo*

Main category: cs.CV

TL;DR: 通过3D投影空间概率密度函数采样与新型表面重建损失，提升神经隐式表面渲染的细节重建与渲染质量


<details>
  <summary>Details</summary>
Motivation: 现有NeRF变体因计算限制无法穷举所有输入数据，传统均匀采样效率低下，需针对兴趣区域优化采样策略

Method: 结合隐式表面表示建立3D投影空间概率密度函数进行射线定向采样，提出融合近表面/空白空间信息的新表面重建损失

Result: 在现有SOTA模型上实现了更精确的三维重建和图像渲染，尤其在场景兴趣区域提升显著

Conclusion: 系统性的采样策略改进配合新型损失函数，为神经隐式渲染提供了更高效准确的三维场景建模框架

Abstract: Several variants of Neural Radiance Fields (NeRFs) have significantly
improved the accuracy of synthesized images and surface reconstruction of 3D
scenes/objects. In all of these methods, a key characteristic is that none can
train the neural network with every possible input data, specifically, every
pixel and potential 3D point along the projection rays due to scalability
issues. While vanilla NeRFs uniformly sample both the image pixels and 3D
points along the projection rays, some variants focus only on guiding the
sampling of the 3D points along the projection rays. In this paper, we leverage
the implicit surface representation of the foreground scene and model a
probability density function in a 3D image projection space to achieve a more
targeted sampling of the rays toward regions of interest, resulting in improved
rendering. Additionally, a new surface reconstruction loss is proposed for
improved performance. This new loss fully explores the proposed 3D image
projection space model and incorporates near-to-surface and empty space
components. By integrating our novel sampling strategy and novel loss into
current state-of-the-art neural implicit surface renderers, we achieve more
accurate and detailed 3D reconstructions and improved image rendering,
especially for the regions of interest in any given scene.

</details>


### [44] [ECMNet:Lightweight Semantic Segmentation with Efficient CNN-Mamba Network](https://arxiv.org/abs/2506.08629)
*Feixiang Du,Shengkun Wu*

Main category: cs.CV

TL;DR: 提出结合CNN与Mamba的轻量级语义分割网络ECMNet，平衡精度与效率。


<details>
  <summary>Details</summary>
Motivation: 传统CNN和Transformer在语义分割中全局上下文建模不足，Mamba的长程依赖建模优势可弥补此缺陷。

Method: 采用胶囊框架融合CNN与Mamba，设计EDAB轻量瓶颈、MSAU多尺度特征聚合模块及Mamba增强的FFM特征融合模块。

Result: 在Cityscapes和CamVid数据集分别达70.6%和73.6% mIoU，参数量0.87M，计算量8.27G FLOPs。

Conclusion: ECMNet有效结合CNN局部特征与Mamba全局建模能力，为轻量级实时分割任务提供新方案。

Abstract: In the past decade, Convolutional Neural Networks (CNNs) and Transformers
have achieved wide applicaiton in semantic segmentation tasks. Although CNNs
with Transformer models greatly improve performance, the global context
modeling remains inadequate. Recently, Mamba achieved great potential in vision
tasks, showing its advantages in modeling long-range dependency. In this paper,
we propose a lightweight Efficient CNN-Mamba Network for semantic segmentation,
dubbed as ECMNet. ECMNet combines CNN with Mamba skillfully in a capsule-based
framework to address their complementary weaknesses. Specifically, We design a
Enhanced Dual-Attention Block (EDAB) for lightweight bottleneck. In order to
improve the representations ability of feature, We devise a Multi-Scale
Attention Unit (MSAU) to integrate multi-scale feature aggregation, spatial
aggregation and channel aggregation. Moreover, a Mamba enhanced Feature Fusion
Module (FFM) merges diverse level feature, significantly enhancing segmented
accuracy. Extensive experiments on two representative datasets demonstrate that
the proposed model excels in accuracy and efficiency balance, achieving 70.6%
mIoU on Cityscapes and 73.6% mIoU on CamVid test datasets, with 0.87M
parameters and 8.27G FLOPs on a single RTX 3090 GPU platform.

</details>


### [45] [RoboSwap: A GAN-driven Video Diffusion Framework For Unsupervised Robot Arm Swapping](https://arxiv.org/abs/2506.08632)
*Yang Bai,Liudi Yang,George Eskandar,Fengyi Shen,Dong Chen,Mohammad Altillawi,Ziyuan Liu,Gitta Kutyniok*

Main category: cs.CV

TL;DR: 提出RoboSwap框架解决跨平台机器人学习中机械臂替换难题，结合GAN与扩散模型实现非配对数据下的视频编辑


<details>
  <summary>Details</summary>
Motivation: 机器人学习中跨平台泛化受限于高质量配对数据稀缺，现有方法依赖同环境视频数据制约应用范围

Method: 分阶段架构：分割机械臂→非配对GAN跨设备转换→扩散模型融合背景增强运动连贯性，两阶段独立训练策略

Result: 在三大基准测试中结构连贯性与运动一致性超越SOTA模型，生成数据有效性验证

Conclusion: 为跨具身学习提供高质量数据生成方案，突破环境与设备限制推动机器人学习发展

Abstract: Recent advancements in generative models have revolutionized video synthesis
and editing. However, the scarcity of diverse, high-quality datasets continues
to hinder video-conditioned robotic learning, limiting cross-platform
generalization. In this work, we address the challenge of swapping a robotic
arm in one video with another: a key step for crossembodiment learning. Unlike
previous methods that depend on paired video demonstrations in the same
environmental settings, our proposed framework, RoboSwap, operates on unpaired
data from diverse environments, alleviating the data collection needs. RoboSwap
introduces a novel video editing pipeline integrating both GANs and diffusion
models, combining their isolated advantages. Specifically, we segment robotic
arms from their backgrounds and train an unpaired GAN model to translate one
robotic arm to another. The translated arm is blended with the original video
background and refined with a diffusion model to enhance coherence, motion
realism and object interaction. The GAN and diffusion stages are trained
independently. Our experiments demonstrate that RoboSwap outperforms
state-of-the-art video and image editing models on three benchmarks in terms of
both structural coherence and motion consistency, thereby offering a robust
solution for generating reliable, cross-embodiment data in robotic learning.

</details>


### [46] [SurfR: Surface Reconstruction with Multi-scale Attention](https://arxiv.org/abs/2506.08635)
*Siddhant Ranade,Gon莽alo Dias Pais,Ross Tyler Whitaker,Jacinto C. Nascimento,Pedro Miraldo,Srikumar Ramalingam*

Main category: cs.CV

TL;DR: 提出结合速度与精度的隐式三维重建算法，通过惰性查询、多尺度网格和跨尺度注意力实现


<details>
  <summary>Details</summary>
Motivation: 现有隐式重建方法在单物体细节与泛化速度之间存在矛盾，需要平衡精度与效率

Method: 惰性查询特征提取+并行多尺度网格特征+跨尺度注意力机制

Result: 在最优分辨率下速度优于基线，精度与SOTA相近，实现最佳速度-精度平衡

Conclusion: 为三维重建提供高效通用解决方案，在保持细节的同时显著提升推理速度

Abstract: We propose a fast and accurate surface reconstruction algorithm for
unorganized point clouds using an implicit representation. Recent learning
methods are either single-object representations with small neural models that
allow for high surface details but require per-object training or generalized
representations that require larger models and generalize to newer shapes but
lack details, and inference is slow. We propose a new implicit representation
for general 3D shapes that is faster than all the baselines at their optimum
resolution, with only a marginal loss in performance compared to the
state-of-the-art. We achieve the best accuracy-speed trade-off using three key
contributions. Many implicit methods extract features from the point cloud to
classify whether a query point is inside or outside the object. First, to speed
up the reconstruction, we show that this feature extraction does not need to
use the query point at an early stage (lazy query). Second, we use a parallel
multi-scale grid representation to develop robust features for different noise
levels and input resolutions. Finally, we show that attention across scales can
provide improved reconstruction results.

</details>


### [47] [Orientation Matters: Making 3D Generative Models Orientation-Aligned](https://arxiv.org/abs/2506.08640)
*Yichong Lu,Yuzhuo Tian,Zijin Jiang,Yikun Zhao,Yuanbo Yang,Hao Ouyang,Haoji Hu,Huimin Yu,Yujun Shen,Yiyi Liao*

Main category: cs.CV

TL;DR: 提出方向对齐的3D对象生成任务，通过构建专用数据集和模型微调解决现有生成模型方向不一致问题


<details>
  <summary>Details</summary>
Motivation: 现有3D生成模型因训练数据不一致导致方向错位，影响下游应用，需解决跨类别方位一致性生成问题

Method: 构建14k+对齐3D模型数据集Objaverse-OA，基于多视角扩散模型和3D-VAE框架进行模型微调

Result: 方法优于后处理对齐方案，支持零样本方位估计和高效旋转操作等下游应用

Conclusion: 通过对齐数据集构建和模型调整实现方位一致生成，显著提升3D生成模型的实用性和应用潜力

Abstract: Humans intuitively perceive object shape and orientation from a single image,
guided by strong priors about canonical poses. However, existing 3D generative
models often produce misaligned results due to inconsistent training data,
limiting their usability in downstream tasks. To address this gap, we introduce
the task of orientation-aligned 3D object generation: producing 3D objects from
single images with consistent orientations across categories. To facilitate
this, we construct Objaverse-OA, a dataset of 14,832 orientation-aligned 3D
models spanning 1,008 categories. Leveraging Objaverse-OA, we fine-tune two
representative 3D generative models based on multi-view diffusion and 3D
variational autoencoder frameworks to produce aligned objects that generalize
well to unseen objects across various categories. Experimental results
demonstrate the superiority of our method over post-hoc alignment approaches.
Furthermore, we showcase downstream applications enabled by our aligned object
generation, including zero-shot object orientation estimation via
analysis-by-synthesis and efficient arrow-based object rotation manipulation.

</details>


### [48] [Enhancing Video Memorability Prediction with Text-Motion Cross-modal Contrastive Loss and Its Application in Video Summarization](https://arxiv.org/abs/2506.08649)
*Zhiyi Zhu,Xiaoyu Wu,Youwei Lu*

Main category: cs.CV

TL;DR: 提出跨模态对比损失模型TMCCL增强视频运动特征表征，并开发MWCVS降低视频摘要标注主观性。


<details>
  <summary>Details</summary>
Motivation: 现有视频记忆性预测模型因标注数据不足导致运动特征表征不充分，且应用场景未被充分探索。

Method: 通过文本相似性构建运动样本对比集(TMCCL)，并利用记忆性预测权重校正视频摘要标签(MWCVS)。

Result: 在两个视频记忆性数据集取得SOTA性能，MWCVS在摘要任务中验证有效性。

Conclusion: 增强运动表征提升预测精度，首度将记忆性预测应用于视频摘要，拓展了应用场景价值。

Abstract: Video memorability refers to the ability of videos to be recalled after
viewing, playing a crucial role in creating content that remains memorable.
Existing models typically focus on extracting multimodal features to predict
video memorability scores but often fail to fully utilize motion cues. The
representation of motion features is compromised during the fine-tuning phase
of the motion feature extractor due to a lack of labeled data. In this paper,
we introduce the Text-Motion Cross-modal Contrastive Loss (TMCCL), a multimodal
video memorability prediction model designed to enhance the representation of
motion features. We tackle the challenge of improving motion feature
representation by leveraging text description similarities across videos to
establish positive and negative motion sample sets for a given target. This
enhancement allows the model to learn similar feature representations for
semantically related motion content, resulting in more accurate memorability
predictions. Our model achieves state-of-the-art performance on two video
memorability prediction datasets. Moreover, the potential applications of video
memorability prediction have been underexplored. To address this gap, we
present Memorability Weighted Correction for Video Summarization (MWCVS), using
video memorability prediction to reduce subjectivity in video summarization
labels. Experimental results on two video summarization datasets demonstrate
the effectiveness of MWCVS, showcasing the promising applications of video
memorability prediction.

</details>


### [49] [Beyond Calibration: Physically Informed Learning for Raw-to-Raw Mapping](https://arxiv.org/abs/2506.08650)
*Peter Gr枚nquist,Stepan Tulyakov,Dengxin Dai*

Main category: cs.CV

TL;DR: 提出神经物理模型(NPM)实现跨设备色彩一致性，适应复杂光照并超越现有方法


<details>
  <summary>Details</summary>
Motivation: 解决多摄像机系统在传感器差异导致的色彩还原不一致问题，现有方法存在光照适应性差、计算成本高等缺陷

Method: 使用物理启发的轻量化模型模拟特定光照条件下的原始图像，支持物理测量初始化和有无配对数据训练

Result: 在NUS和BeyondRGB数据集上超越SOTA方法，实现跨传感器系统的稳健色彩一致性

Conclusion: NPM为图像融合和ISP兼容性提供了实用的解决方案，显著提升设备间的色彩还原性能

Abstract: Achieving consistent color reproduction across multiple cameras is essential
for seamless image fusion and Image Processing Pipeline (ISP) compatibility in
modern devices, but it is a challenging task due to variations in sensors and
optics. Existing raw-to-raw conversion methods face limitations such as poor
adaptability to changing illumination, high computational costs, or impractical
requirements such as simultaneous camera operation and overlapping
fields-of-view. We introduce the Neural Physical Model (NPM), a lightweight,
physically-informed approach that simulates raw images under specified
illumination to estimate transformations between devices. The NPM effectively
adapts to varying illumination conditions, can be initialized with physical
measurements, and supports training with or without paired data. Experiments on
public datasets like NUS and BeyondRGB demonstrate that NPM outperforms recent
state-of-the-art methods, providing robust chromatic consistency across
different sensors and optical systems.

</details>


### [50] [LLaVA-c: Continual Improved Visual Instruction Tuning](https://arxiv.org/abs/2506.08666)
*Wenzhuo Liu,Fei Zhu,Haiyang Guo,Longhui Wei,Cheng-Lin Liu*

Main category: cs.CV

TL;DR: 提出LLaVA-c方法通过频谱感知整合和无监督正则化，在持续学习中保持通用能力并超越多任务联合训练


<details>
  <summary>Details</summary>
Motivation: 解决多任务学习的任务平衡问题和持续学习中的基础模型退化问题，提升增量学习效果

Method: 在LLaVA-1.5上改进：频谱感知整合优化任务平衡，无监督查询正则化防止模型退化

Result: 持续学习首次达到匹配/超越多任务联合训练效果，基准性能提升且保留通用能力

Conclusion: 为多模态持续学习提供有效解决方案，通过增量学习实现高性能且避免重复训练，推动领域发展

Abstract: Multimodal models like LLaVA-1.5 achieve state-of-the-art visual
understanding through visual instruction tuning on multitask datasets, enabling
strong instruction-following and multimodal performance. However, multitask
learning faces challenges such as task balancing, requiring careful adjustment
of data proportions, and expansion costs, where new tasks risk catastrophic
forgetting and need costly retraining. Continual learning provides a promising
alternative to acquiring new knowledge incrementally while preserving existing
capabilities. However, current methods prioritize task-specific performance,
neglecting base model degradation from overfitting to specific instructions,
which undermines general capabilities. In this work, we propose a simple but
effective method with two modifications on LLaVA-1.5: spectral-aware
consolidation for improved task balance and unsupervised inquiry regularization
to prevent base model degradation. We evaluate both general and task-specific
performance across continual pretraining and fine-tuning. Experiments
demonstrate that LLaVA-c consistently enhances standard benchmark performance
and preserves general capabilities. For the first time, we show that
task-by-task continual learning can achieve results that match or surpass
multitask joint learning. The code will be publicly released.

</details>


### [51] [ATAS: Any-to-Any Self-Distillation for Enhanced Open-Vocabulary Dense Prediction](https://arxiv.org/abs/2506.08678)
*Juan Yeo,Soonwoo Cha,Jiwoo Song,Hyunbin Jin,Taesup Kim*

Main category: cs.CV

TL;DR: 提出ATAS自蒸馏方法，提升CLIP的细粒度区域理解能力，无需额外模块或标注数据


<details>
  <summary>Details</summary>
Motivation: CLIP在细粒度区域理解和语义连贯性方面存在不足，现有方法常牺牲语义连贯性且依赖额外模块

Method: 通过全表征层级的自蒸馏过程，利用未标注图像和模型自身知识，保持局部语义一致性同时增强细节识别

Result: 在开放词汇检测和分割基准上显著超越基线CLIP模型

Conclusion: 验证了联合保持语义连贯与细粒度对齐的重要性，为开放词汇密集预测提供了有效解决方案

Abstract: Vision-language models such as CLIP have recently propelled open-vocabulary
dense prediction tasks by enabling recognition of a broad range of visual
concepts. However, CLIP still struggles with fine-grained, region-level
understanding, hindering its effectiveness on these dense prediction tasks. We
identify two pivotal factors required to address this limitation: semantic
coherence and fine-grained vision-language alignment. Current adaptation
methods often improve fine-grained alignment at the expense of semantic
coherence, and often rely on extra modules or supervised fine-tuning. To
overcome these issues, we propose Any-to-Any Self-Distillation (ATAS), a novel
approach that simultaneously enhances semantic coherence and fine-grained
alignment by leveraging own knowledge of a model across all representation
levels. Unlike prior methods, ATAS uses only unlabeled images and an internal
self-distillation process to refine representations of CLIP vision encoders,
preserving local semantic consistency while sharpening local detail
recognition. On open-vocabulary object detection and semantic segmentation
benchmarks, ATAS achieves substantial performance gains, outperforming baseline
CLIP models. These results validate the effectiveness of our approach and
underscore the importance of jointly maintaining semantic coherence and
fine-grained alignment for advanced open-vocabulary dense prediction.

</details>


### [52] [CanadaFireSat: Toward high-resolution wildfire forecasting with multiple modalities](https://arxiv.org/abs/2506.08690)
*Hugo Porta,Emanuele Dalsasso,Jessica L. McCarty,Devis Tuia*

Main category: cs.CV

TL;DR: 提出CanadaFireSat数据集和多模态深度学习方法，实现100米分辨率加拿大野火预测


<details>
  <summary>Details</summary>
Motivation: 气候变化导致北方生态系统火灾季节延长加剧，需提升高分辨率野火预测能力

Method: 融合Sentinel-2、MODIS和ERA5多源数据，开发深度学习模型进行时空预测

Result: 多模态输入模型F1分数达60.3%，在未见过的2023年火灾季验证有效

Conclusion: 首次展示了大陆尺度高分辨率野火预测可行性，为气候适应提供新工具

Abstract: Canada experienced in 2023 one of the most severe wildfire seasons in recent
history, causing damage across ecosystems, destroying communities, and emitting
large quantities of CO2. This extreme wildfire season is symptomatic of a
climate-change-induced increase in the length and severity of the fire season
that affects the boreal ecosystem. Therefore, it is critical to empower
wildfire management in boreal communities with better mitigation solutions.
Wildfire probability maps represent an important tool for understanding the
likelihood of wildfire occurrence and the potential severity of future
wildfires. The massive increase in the availability of Earth observation data
has enabled the development of deep learning-based wildfire forecasting models,
aiming at providing precise wildfire probability maps at different spatial and
temporal scales. A main limitation of such methods is their reliance on
coarse-resolution environmental drivers and satellite products, leading to
wildfire occurrence prediction of reduced resolution, typically around $\sim
0.1${\deg}. This paper presents a benchmark dataset: CanadaFireSat, and
baseline methods for high-resolution: 100 m wildfire forecasting across Canada,
leveraging multi-modal data from high-resolution multi-spectral satellite
images (Sentinel-2 L1C), mid-resolution satellite products (MODIS), and
environmental factors (ERA5 reanalysis data). Our experiments consider two
major deep learning architectures. We observe that using multi-modal temporal
inputs outperforms single-modal temporal inputs across all metrics, achieving a
peak performance of 60.3% in F1 score for the 2023 wildfire season, a season
never seen during model training. This demonstrates the potential of
multi-modal deep learning models for wildfire forecasting at high-resolution
and continental scale.

</details>


### [53] [VReST: Enhancing Reasoning in Large Vision-Language Models through Tree Search and Self-Reward Mechanism](https://arxiv.org/abs/2506.08691)
*Congzhi Zhang,Jiawei Peng,Zhenglin Wang,Yilong Lai,Haowen Sun,Heng Chang,Fei Ma,Weijiang Yu*

Main category: cs.CV

TL;DR: 提出无需训练的VReST方法，通过蒙特卡洛树搜索和自奖励机制增强多模态模型推理能力


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型在复杂视觉推理任务中的表现受限，尤其是思维链提示技术的应用效果不足

Method: 构建搜索树组织推理步骤，结合多模态自奖励机制（子问题效用、答案正确性、图文线索相关性）进行质量评估

Result: 在三个多模态数学推理基准测试中取得SOTA，验证了测试时扩展定律的有效性

Conclusion: 为多模态推理任务提供无监督优化路径，证明搜索策略与自评估机制结合的可行性

Abstract: Large Vision-Language Models (LVLMs) have shown exceptional performance in
multimodal tasks, but their effectiveness in complex visual reasoning is still
constrained, especially when employing Chain-of-Thought prompting techniques.
In this paper, we propose VReST, a novel training-free approach that enhances
Reasoning in LVLMs through Monte Carlo Tree Search and Self-Reward mechanisms.
VReST meticulously traverses the reasoning landscape by establishing a search
tree, where each node encapsulates a reasoning step, and each path delineates a
comprehensive reasoning sequence. Our innovative multimodal Self-Reward
mechanism assesses the quality of reasoning steps by integrating the utility of
sub-questions, answer correctness, and the relevance of vision-language clues,
all without the need for additional models. VReST surpasses current prompting
methods and secures state-of-the-art performance across three multimodal
mathematical reasoning benchmarks. Furthermore, it substantiates the efficacy
of test-time scaling laws in multimodal tasks, offering a promising direction
for future research.

</details>


### [54] [MoSiC: Optimal-Transport Motion Trajectory for Dense Self-Supervised Learning](https://arxiv.org/abs/2506.08694)
*Mohammadreza Salehi,Shashanka Venkataramanan,Ioana Simion,Efstratios Gavves,Cees G. M. Snoek,Yuki M Asano*

Main category: cs.CV

TL;DR: 提出运动引导自监督框架MoSiC，通过点轨迹聚类提升视频表征的时空一致性


<details>
  <summary>Details</summary>
Motivation: 现有视频自监督方法因静态增强无法处理动态场景的形变、遮挡和相机运动问题

Method: 利用点跟踪器提取运动轨迹，结合动量编码器最优传输聚类，并通过轨迹传播保证时间一致性

Result: 在6个数据集4个基准上提升SOTA 1%-6%，GitHub开源实现

Conclusion: 运动信号作为隐式监督可增强跨帧泛化能力，显著提升动态场景鲁棒性

Abstract: Dense self-supervised learning has shown great promise for learning pixel-
and patch-level representations, but extending it to videos remains challenging
due to the complexity of motion dynamics. Existing approaches struggle as they
rely on static augmentations that fail under object deformations, occlusions,
and camera movement, leading to inconsistent feature learning over time. We
propose a motion-guided self-supervised learning framework that clusters dense
point tracks to learn spatiotemporally consistent representations. By
leveraging an off-the-shelf point tracker, we extract long-range motion
trajectories and optimize feature clustering through a momentum-encoder-based
optimal transport mechanism. To ensure temporal coherence, we propagate cluster
assignments along tracked points, enforcing feature consistency across views
despite viewpoint changes. Integrating motion as an implicit supervisory
signal, our method learns representations that generalize across frames,
improving robustness in dynamic scenes and challenging occlusion scenarios. By
initializing from strong image-pretrained models and leveraging video data for
training, we improve state-of-the-art by 1% to 6% on six image and video
datasets and four evaluation benchmarks. The implementation is publicly
available at our GitHub repository: https://github.com/SMSD75/MoSiC/tree/main

</details>


### [55] [ArrowPose: Segmentation, Detection, and 5 DoF Pose Estimation Network for Colorless Point Clouds](https://arxiv.org/abs/2506.08699)
*Frederik Hagelskjaer*

Main category: cs.CV

TL;DR: 提出基于无色点云的快速5自由度姿态估计网络ArrowPose，推理速度达250毫秒


<details>
  <summary>Details</summary>
Motivation: 现有无色点云姿态估计方法在实时性和精度上的不足

Method: 通过神经网络预测物体中心点和顶点，结合合成数据训练实现端到端姿态估计

Result: 在基准测试中达到SOTA性能，推理速度提升至250ms，优于所有无色点云方法

Conclusion: 证明了基于关键点回归的轻量化网络在实时姿态估计任务中的有效性，拓宽了工业应用场景

Abstract: This paper presents a fast detection and 5 DoF (Degrees of Freedom) pose
estimation network for colorless point clouds. The pose estimation is
calculated from center and top points of the object, predicted by the neural
network. The network is trained on synthetic data, and tested on a benchmark
dataset, where it demonstrates state-of-the-art performance and outperforms all
colorless methods. The network is able to run inference in only 250
milliseconds making it usable in many scenarios. Project page with code at
arrowpose.github.io

</details>


### [56] [TraGraph-GS: Trajectory Graph-based Gaussian Splatting for Arbitrary Large-Scale Scene Rendering](https://arxiv.org/abs/2506.08704)
*Xiaohan Zhang,Sitong Wang,Yushen Yan,Yi Yang,Mingda Xu,Qi Liu*

Main category: cs.CV

TL;DR: 提出基于轨迹图的TraGraph-GS方法，解决大规模场景新视角合成的空间分割与渲染难题


<details>
  <summary>Details</summary>
Motivation: 现有方法采用刚性空间分割导致相机轨迹适应性差，区域合并引发高斯重叠破坏纹理细节

Method: 构建轨迹图进行场景分割，引入正则化约束增强纹理渲染，采用渐进渲染策略消除高斯重叠伪影

Result: 在航拍和地面数据集分别提升PSNR 1.86dB/1.62dB，渲染效率显著优于SOTA方法

Conclusion: 突破传统空间分割限制，实现任意规模场景的高效高精度渲染，具有广泛适用性

Abstract: High-quality novel view synthesis for large-scale scenes presents a
challenging dilemma in 3D computer vision. Existing methods typically partition
large scenes into multiple regions, reconstruct a 3D representation using
Gaussian splatting for each region, and eventually merge them for novel view
rendering. They can accurately render specific scenes, yet they do not
generalize effectively for two reasons: (1) rigid spatial partition techniques
struggle with arbitrary camera trajectories, and (2) the merging of regions
results in Gaussian overlap to distort texture details. To address these
challenges, we propose TraGraph-GS, leveraging a trajectory graph to enable
high-precision rendering for arbitrarily large-scale scenes. We present a
spatial partitioning method for large-scale scenes based on graphs, which
incorporates a regularization constraint to enhance the rendering of textures
and distant objects, as well as a progressive rendering strategy to mitigate
artifacts caused by Gaussian overlap. Experimental results demonstrate its
superior performance both on four aerial and four ground datasets and highlight
its remarkable efficiency: our method achieves an average improvement of 1.86
dB in PSNR on aerial datasets and 1.62 dB on ground datasets compared to
state-of-the-art approaches.

</details>


### [57] [SceneSplat++: A Large Dataset and Comprehensive Benchmark for Language Gaussian Splatting](https://arxiv.org/abs/2506.08710)
*Mengjiao Ma,Qi Ma,Yue Li,Jiahuan Cheng,Runyi Yang,Bin Ren,Nikola Popovic,Mingqiang Wei,Nicu Sebe,Luc Van Gool,Theo Gevers,Martin R. Oswald,Danda Pani Paudel*

Main category: cs.CV

TL;DR: 提出了首个面向3D高斯溅射的大规模基准测试GaussianWorld-49K数据集，系统评估不同语言建模方法的3D场景理解能力


<details>
  <summary>Details</summary>
Motivation: 现有语言高斯溅射方法多在2D视角局限评估，缺乏对全息3D理解的系统验证

Method: 构建跨4个数据集的1060场景基准，引入包含49K场景的GaussianWorld数据集验证泛化方法

Result: 泛化范式在场景泛化、推理速度和分割精度方面优势显著，数据先验可显著提升性能

Conclusion: 通用化3DGS方法展现强大潜力，开放基准与数据集将加速三维场景认知研究发展

Abstract: 3D Gaussian Splatting (3DGS) serves as a highly performant and efficient
encoding of scene geometry, appearance, and semantics. Moreover, grounding
language in 3D scenes has proven to be an effective strategy for 3D scene
understanding. Current Language Gaussian Splatting line of work fall into three
main groups: (i) per-scene optimization-based, (ii) per-scene
optimization-free, and (iii) generalizable approach. However, most of them are
evaluated only on rendered 2D views of a handful of scenes and viewpoints close
to the training views, limiting ability and insight into holistic 3D
understanding. To address this gap, we propose the first large-scale benchmark
that systematically assesses these three groups of methods directly in 3D
space, evaluating on 1060 scenes across three indoor datasets and one outdoor
dataset. Benchmark results demonstrate a clear advantage of the generalizable
paradigm, particularly in relaxing the scene-specific limitation, enabling fast
feed-forward inference on novel scenes, and achieving superior segmentation
performance. We further introduce GaussianWorld-49K a carefully curated 3DGS
dataset comprising around 49K diverse indoor and outdoor scenes obtained from
multiple sources, with which we demonstrate the generalizable approach could
harness strong data priors. Our codes, benchmark, and datasets will be made
public to accelerate research in generalizable 3DGS scene understanding.

</details>


### [58] [Geometric deep learning for local growth prediction on abdominal aortic aneurysm surfaces](https://arxiv.org/abs/2506.08729)
*Dieuwertje Alblas,Patryk Rygiel,Julian Suk,Kaj O. Kappe,Marieke Hofman,Christoph Brune,Kak Khee Yeung,Jelmer M. Wolterink*

Main category: cs.CV

TL;DR: 基于SE(3)-对称变换器的血管表面多物理特征模型实现腹主动脉瘤个性化生长预测


<details>
  <summary>Details</summary>
Motivation: 现有基于最大直径的监测方案忽略3D形态与生长的复杂关系，需开发个性化预测方法优化监测策略

Method: 提出SE(3)-对称变换器模型，在保留血管解剖结构的基础上，利用24名患者113次CTA扫描数据训练，直接处理不规则时间间隔的3D表面多物理特征

Result: 预测直径误差中值1.18mm，2年手术指征预测精度达0.93，外部验证集显示良好泛化能力

Conclusion: 血管表面局部定向生长预测具有可行性，为个性化监测策略提供新方向

Abstract: Abdominal aortic aneurysms (AAAs) are progressive focal dilatations of the
abdominal aorta. AAAs may rupture, with a survival rate of only 20\%. Current
clinical guidelines recommend elective surgical repair when the maximum AAA
diameter exceeds 55 mm in men or 50 mm in women. Patients that do not meet
these criteria are periodically monitored, with surveillance intervals based on
the maximum AAA diameter. However, this diameter does not take into account the
complex relation between the 3D AAA shape and its growth, making standardized
intervals potentially unfit. Personalized AAA growth predictions could improve
monitoring strategies. We propose to use an SE(3)-symmetric transformer model
to predict AAA growth directly on the vascular model surface enriched with
local, multi-physical features. In contrast to other works which have
parameterized the AAA shape, this representation preserves the vascular
surface's anatomical structure and geometric fidelity. We train our model using
a longitudinal dataset of 113 computed tomography angiography (CTA) scans of 24
AAA patients at irregularly sampled intervals. After training, our model
predicts AAA growth to the next scan moment with a median diameter error of
1.18 mm. We further demonstrate our model's utility to identify whether a
patient will become eligible for elective repair within two years (acc = 0.93).
Finally, we evaluate our model's generalization on an external validation set
consisting of 25 CTAs from 7 AAA patients from a different hospital. Our
results show that local directional AAA growth prediction from the vascular
surface is feasible and may contribute to personalized surveillance strategies.

</details>


### [59] [InceptionMamba: An Efficient Hybrid Network with Large Band Convolution and Bottleneck Mamba](https://arxiv.org/abs/2506.08735)
*Yuhang Wang,Jun Li,Zhijian Wu,Jianhua Xu*

Main category: cs.CV

TL;DR: 提出InceptionMamba架构，通过正交带卷积和Mamba模块提升空间建模与全局上下文建模能力。


<details>
  <summary>Details</summary>
Motivation: 解决InceptionNeXt在空间依赖捕获和全局上下文建模上的局限性。

Method: 采用正交带卷积增强局部空间建模，结合瓶颈Mamba模块融合跨通道信息并扩展感受野。

Result: 在分类及下游任务中实现SOTA性能，且参数和计算效率更优。

Conclusion: 结合空间建模和全局上下文建模优势，为视觉任务提供了高效解决方案。

Abstract: Within the family of convolutional neural networks, InceptionNeXt has shown
excellent competitiveness in image classification and a number of downstream
tasks. Built on parallel one-dimensional strip convolutions, however, it
suffers from limited ability of capturing spatial dependencies along different
dimensions and fails to fully explore spatial modeling in local neighborhood.
Besides, inherent locality constraints of convolution operations are
detrimental to effective global context modeling. To overcome these
limitations, we propose a novel backbone architecture termed InceptionMamba in
this study. More specifically, the traditional one-dimensional strip
convolutions are replaced by orthogonal band convolutions in our InceptionMamba
to achieve cohesive spatial modeling. Furthermore, global contextual modeling
can be achieved via a bottleneck Mamba module, facilitating enhanced
cross-channel information fusion and enlarged receptive field. Extensive
evaluations on classification and various downstream tasks demonstrate that the
proposed InceptionMamba achieves state-of-the-art performance with superior
parameter and computational efficiency. The source code will be available at
https://github.com/Wake1021/InceptionMamba.

</details>


### [60] [RS-MTDF: Multi-Teacher Distillation and Fusion for Remote Sensing Semi-Supervised Semantic Segmentation](https://arxiv.org/abs/2506.08772)
*Jiayi Song,Kaiyu Li,Xiangyong Cao,Deyu Meng*

Main category: cs.CV

TL;DR: 提出基于多教师视觉基础模型的知识蒸馏融合框架RS-MTDF，显著提升遥感图像半监督分割性能


<details>
  <summary>Details</summary>
Motivation: 解决遥感图像语义分割中标注数据稀缺与标注分布不一致导致的泛化能力受限问题

Method: 采用DINOv2/CLIP等冻结VFM作为教师模型，通过特征级蒸馏对齐学生特征，并在解码器中融合蒸馏知识

Result: 在三个遥感数据集上实现SOTA性能，特别是在LoveDA不同标注比例下全面超越现有方法

Conclusion: 验证了多教师VFM指导能有效增强遥感分割的泛化能力和语义理解，消融实验证明各模块有效性

Abstract: Semantic segmentation in remote sensing images is crucial for various
applications, yet its performance is heavily reliant on large-scale,
high-quality pixel-wise annotations, which are notoriously expensive and
time-consuming to acquire. Semi-supervised semantic segmentation (SSS) offers a
promising alternative to mitigate this data dependency. However, existing SSS
methods often struggle with the inherent distribution mismatch between limited
labeled data and abundant unlabeled data, leading to suboptimal generalization.
We propose that Vision Foundation Models (VFMs), pre-trained on vast and
diverse datasets, possess robust generalization capabilities that can
effectively bridge this distribution gap and provide strong semantic priors for
SSS. Inspired by this, we introduce RS-MTDF (Multi-Teacher Distillation and
Fusion), a novel framework that leverages the powerful semantic knowledge
embedded in VFMs to guide semi-supervised learning in remote sensing.
Specifically, RS-MTDF employs multiple frozen VFMs (\textit{e.g.}, DINOv2 and
CLIP) as expert teachers, utilizing feature-level distillation to align student
features with their robust representations. To further enhance discriminative
power, the distilled knowledge is seamlessly fused into the student decoder.
Extensive experiments on three challenging remote sensing datasets (ISPRS
Potsdam, LoveDA, and DeepGlobe) demonstrate that RS-MTDF consistently achieves
state-of-the-art performance. Notably, our method outperforms existing
approaches across various label ratios on LoveDA and secures the highest IoU in
the majority of semantic categories. These results underscore the efficacy of
multi-teacher VFM guidance in significantly enhancing both generalization and
semantic understanding for remote sensing segmentation. Ablation studies
further validate the contribution of each proposed module.

</details>


### [61] [Gaussian2Scene: 3D Scene Representation Learning via Self-supervised Learning with 3D Gaussian Splatting](https://arxiv.org/abs/2506.08777)
*Keyi Liu,Weidong Yang,Ben Fei,Ying He*

Main category: cs.CV

TL;DR: 提出基于3D高斯泼溅的Gaussian2Scene框架，通过显式场景重建改进点云预训练效果


<details>
  <summary>Details</summary>
Motivation: 解决现有场景级自监督学习依赖隐式表征、内存消耗高、缺乏3D几何捕捉能力的问题

Method: 采用3D高斯泼溅进行显式场景重建，构建双阶段训练框架：双分支掩码自编码器学习多模态表征，结合高斯基元几何位置监督

Result: 在多个3D目标检测任务中持续超越现有预训练方法

Conclusion: 显式3D重建与跨模态学习策略有效增强了网络几何理解能力，为下游任务提供更优预训练模型

Abstract: Self-supervised learning (SSL) for point cloud pre-training has become a
cornerstone for many 3D vision tasks, enabling effective learning from
large-scale unannotated data. At the scene level, existing SSL methods often
incorporate volume rendering into the pre-training framework, using RGB-D
images as reconstruction signals to facilitate cross-modal learning. This
strategy promotes alignment between 2D and 3D modalities and enables the model
to benefit from rich visual cues in the RGB-D inputs. However, these approaches
are limited by their reliance on implicit scene representations and high memory
demands. Furthermore, since their reconstruction objectives are applied only in
2D space, they often fail to capture underlying 3D geometric structures. To
address these challenges, we propose Gaussian2Scene, a novel scene-level SSL
framework that leverages the efficiency and explicit nature of 3D Gaussian
Splatting (3DGS) for pre-training. The use of 3DGS not only alleviates the
computational burden associated with volume rendering but also supports direct
3D scene reconstruction, thereby enhancing the geometric understanding of the
backbone network. Our approach follows a progressive two-stage training
strategy. In the first stage, a dual-branch masked autoencoder learns both 2D
and 3D scene representations. In the second stage, we initialize training with
reconstructed point clouds and further supervise learning using the geometric
locations of Gaussian primitives and rendered RGB images. This process
reinforces both geometric and cross-modal learning. We demonstrate the
effectiveness of Gaussian2Scene across several downstream 3D object detection
tasks, showing consistent improvements over existing pre-training methods.

</details>


### [62] [Landsat-Bench: Datasets and Benchmarks for Landsat Foundation Models](https://arxiv.org/abs/2506.08780)
*Isaac Corley,Lakshay Sharma,Ruth Crasto*

Main category: cs.CV

TL;DR: 提出Landsat-Bench基准套件，验证SSL预训练模型在遥感任务上的优越性


<details>
  <summary>Details</summary>
Motivation: 解决Landsat数据缺乏标准化基准的问题，推动地理空间基础模型发展

Method: 构建EuroSAT-L/BigEarthNet-L/LC100-L三基准，建立SSL4EO-L预训练模型与常规架构的标准化评估体系

Result: SSL预训练模型相比ImageNet在分类任务提升4%OA和5.1%mAP

Conclusion: 验证了SSL4EO-L预训练可有效提升下游任务性能，为Landsat基础模型发展提供基准支持

Abstract: The Landsat program offers over 50 years of globally consistent Earth
imagery. However, the lack of benchmarks for this data constrains progress
towards Landsat-based Geospatial Foundation Models (GFM). In this paper, we
introduce Landsat-Bench, a suite of three benchmarks with Landsat imagery that
adapt from existing remote sensing datasets -- EuroSAT-L, BigEarthNet-L, and
LC100-L. We establish baseline and standardized evaluation methods across both
common architectures and Landsat foundation models pretrained on the SSL4EO-L
dataset. Notably, we provide evidence that SSL4EO-L pretrained GFMs extract
better representations for downstream tasks in comparison to ImageNet,
including performance gains of +4% OA and +5.1% mAP on EuroSAT-L and
BigEarthNet-L.

</details>


### [63] [HomographyAD: Deep Anomaly Detection Using Self Homography Learning](https://arxiv.org/abs/2506.08784)
*Jongyub Seok,Chanjin Kang*

Main category: cs.CV

TL;DR: 提出基于深度单应性估计的HomographyAD方法，解决工业异常检测中数据未对齐问题


<details>
  <summary>Details</summary>
Motivation: 现有工业异常检测方法仅适用于完全对齐数据集，无法满足现实工业环境中未对齐数据的检测需求

Method: 通过深度单应性估计实现输入前景对齐，结合自单应性学习微调模型，利用特征距离度量进行异常检测

Result: 在多种现有AD方法上验证，通过大量实验显示性能显著提升

Conclusion: 该方法有效解决了实际工业场景中的数据未对齐问题，提升了异常检测的实用性和泛化能力

Abstract: Anomaly detection (AD) is a task that distinguishes normal and abnormal data,
which is important for applying automation technologies of the manufacturing
facilities. For MVTec dataset that is a representative AD dataset for
industrial environment, many recent works have shown remarkable performances.
However, the existing anomaly detection works have a limitation of showing good
performance for fully-aligned datasets only, unlike real-world industrial
environments. To solve this limitation, we propose HomographyAD, a novel deep
anomaly detection methodology based on the ImageNet-pretrained network, which
is specially designed for actual industrial dataset. Specifically, we first
suggest input foreground alignment using the deep homography estimation method.
In addition, we fine-tune the model by self homography learning to learn
additional shape information from normal samples. Finally, we conduct anomaly
detection based on the measure of how far the feature of test sample is from
the distribution of the extracted normal features. By applying our proposed
method to various existing AD approaches, we show performance enhancement
through extensive experiments.

</details>


### [64] [A PDE-Based Image Dehazing Method via Atmospheric Scattering Theory](https://arxiv.org/abs/2506.08793)
*Zhuoran Zheng*

Main category: cs.CV

TL;DR: 提出基于偏微分方程框架的单幅图像去雾方法，整合大气散射模型、非局部正则化和暗通道先验


<details>
  <summary>Details</summary>
Motivation: 解决传统去雾方法在边缘保留和理论完备性方面的不足，建立可推广至深度学习模型的数学框架

Method: 构建改进型PDE模型，包含边缘保持扩散系数、自适应正则化参数，通过Lax-Milgram定理证明解的存在唯一性，基于PyTorch实现GPU加速迭代算法

Result: 实验验证了方法有效性，证明其可作为深度学习范式的基础去雾方案

Conclusion: 将传统物理模型与现代计算技术结合，为图像复原任务提供了理论严谨且高效可扩展的解决方案

Abstract: This paper presents a novel partial differential equation (PDE) framework for
single-image dehazing. By integrating the atmospheric scattering model with
nonlocal regularization and dark channel prior, we propose the improved PDE: \[
-\text{div}\left(D(\nabla u)\nabla u\right) + \lambda(t) G(u) = \Phi(I,t,A) \]
where $D(\nabla u) = (|\nabla u| + \epsilon)^{-1}$ is the edge-preserving
diffusion coefficient, $G(u)$ is the Gaussian convolution operator, and
$\lambda(t)$ is the adaptive regularization parameter based on transmission map
$t$. We prove the existence and uniqueness of weak solutions in $H_0^1(\Omega)$
using Lax-Milgram theorem, and implement an efficient fixed-point iteration
scheme accelerated by PyTorch GPU computation. The experimental results
demonstrate that this method is a promising deghazing solution that can be
generalized to the deep model paradigm.

</details>


### [65] [Flow Diverse and Efficient: Learning Momentum Flow Matching via Stochastic Velocity Field Sampling](https://arxiv.org/abs/2506.08796)
*Zhiyuan Ma,Ruixun Liu,Sixian Liu,Jianjun Li,Bowen Zhou*

Main category: cs.CV

TL;DR: 提出离散化整流流模型Discretized-RF，通过速度场离散化和速度噪声注入改善生成多样性与多尺度建模


<details>
  <summary>Details</summary>
Motivation: 针对现有整流流模型存在的生成多样性不足和多尺度噪声建模缺陷问题，传统直线路径限制了采样空间且仅优化恒定速度场

Method: 将直线路径离散化为可变速度场子路径（动量场），通过在子路径速度上引入噪声改变方向，扩展搜索空间并增强噪声建模能力

Result: 实验表明随机速度场采样能产生多样高效轨迹，在多个数据集上实现高质量多样化生成效果

Conclusion: 速度离散化与噪声注入为扩散模型发展提供新方向，有效平衡了生成效率与质量

Abstract: Recently, the rectified flow (RF) has emerged as the new state-of-the-art
among flow-based diffusion models due to its high efficiency advantage in
straight path sampling, especially with the amazing images generated by a
series of RF models such as Flux 1.0 and SD 3.0. Although a straight-line
connection between the noisy and natural data distributions is intuitive, fast,
and easy to optimize, it still inevitably leads to: 1) Diversity concerns,
which arise since straight-line paths only cover a fairly restricted sampling
space. 2) Multi-scale noise modeling concerns, since the straight line flow
only needs to optimize the constant velocity field $\bm v$ between the two
distributions $\bm\pi_0$ and $\bm\pi_1$. In this work, we present
Discretized-RF, a new family of rectified flow (also called momentum flow
models since they refer to the previous velocity component and the random
velocity component in each diffusion step), which discretizes the straight path
into a series of variable velocity field sub-paths (namely ``momentum fields'')
to expand the search space, especially when close to the distribution
$p_\text{noise}$. Different from the previous case where noise is directly
superimposed on $\bm x$, we introduce noise on the velocity $\bm v$ of the
sub-path to change its direction in order to improve the diversity and
multi-scale noise modeling abilities. Experimental results on several
representative datasets demonstrate that learning momentum flow matching by
sampling random velocity fields will produce trajectories that are both diverse
and efficient, and can consistently generate high-quality and diverse results.
Code is available at https://github.com/liuruixun/momentum-fm.

</details>


### [66] [HunyuanVideo-HOMA: Generic Human-Object Interaction in Multimodal Driven Human Animation](https://arxiv.org/abs/2506.08797)
*Ziyao Huang,Zixiang Zhou,Juan Cao,Yifeng Ma,Yi Chen,Zejing Rao,Zhiyong Xu,Hongmei Wang,Qin Lin,Yuan Zhou,Qinglin Lu,Fan Tang*

Main category: cs.CV

TL;DR: 提出弱条件多模态驱动框架HunyuanVideo-HOMA，通过双模态融合和适配器设计，解决人机交互视频生成的数据依赖和泛化问题


<details>
  <summary>Details</summary>
Motivation: 针对现有HOI视频生成方法对精细运动数据依赖强、新场景泛化能力弱、可访问性差等痛点进行改进

Method: 采用多模态扩散变换器(MMDiT)构建双输入空间，通过参数空间HOI适配器继承预训练知识，结合面部交叉注意力机制实现精确唇同步

Result: 在弱监督条件下取得SOTA的交互自然度和场景泛化能力，支持文本条件生成和交互式物体操作

Conclusion: 建立了高效可控的视频生成框架，为多模态交互应用提供了新解决方案，通过用户界面增强实用性

Abstract: To address key limitations in human-object interaction (HOI) video generation
-- specifically the reliance on curated motion data, limited generalization to
novel objects/scenarios, and restricted accessibility -- we introduce
HunyuanVideo-HOMA, a weakly conditioned multimodal-driven framework.
HunyuanVideo-HOMA enhances controllability and reduces dependency on precise
inputs through sparse, decoupled motion guidance. It encodes appearance and
motion signals into the dual input space of a multimodal diffusion transformer
(MMDiT), fusing them within a shared context space to synthesize temporally
consistent and physically plausible interactions. To optimize training, we
integrate a parameter-space HOI adapter initialized from pretrained MMDiT
weights, preserving prior knowledge while enabling efficient adaptation, and a
facial cross-attention adapter for anatomically accurate audio-driven lip
synchronization. Extensive experiments confirm state-of-the-art performance in
interaction naturalness and generalization under weak supervision. Finally,
HunyuanVideo-HOMA demonstrates versatility in text-conditioned generation and
interactive object manipulation, supported by a user-friendly demo interface.
The project page is at https://anonymous.4open.science/w/homa-page-0FBE/.

</details>


### [67] [HiSin: Efficient High-Resolution Sinogram Inpainting via Resolution-Guided Progressive Inference](https://arxiv.org/abs/2506.08809)
*Jiaze E,Srutarshi Banerjee,Tekin Bicer,Guannan Wang,Yanfu Zhang,Bin Ren*

Main category: cs.CV

TL;DR: 提出HiSin框架，通过分辨率引导的渐进推理实现高效高分辨率正弦图修复


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型难以处理高分辨率正弦图修复的高内存和计算需求问题

Method: 分辨率引导渐进推理（分层提取结构+小块推理）、频率感知跳块、结构自适应步骤分配

Result: 内存峰值降低31.25%，推理时间减少18.15%，保持多场景下的修复精度

Conclusion: 在保证CT重建质量的同时显著提升效率，为医疗成像提供实用解决方案

Abstract: High-resolution sinogram inpainting is essential for computed tomography
reconstruction, as missing high-frequency projections can lead to visible
artifacts and diagnostic errors. Diffusion models are well-suited for this task
due to their robustness and detail-preserving capabilities, but their
application to high-resolution inputs is limited by excessive memory and
computational demands. To address this limitation, we propose HiSin, a novel
diffusion based framework for efficient sinogram inpainting via
resolution-guided progressive inference. It progressively extracts global
structure at low resolution and defers high-resolution inference to small
patches, enabling memory-efficient inpainting. It further incorporates
frequency-aware patch skipping and structure-adaptive step allocation to reduce
redundant computation. Experimental results show that HiSin reduces peak memory
usage by up to 31.25% and inference time by up to 18.15%, and maintains
inpainting accuracy across datasets, resolutions, and mask conditions.

</details>


### [68] [Video-CoT: A Comprehensive Dataset for Spatiotemporal Understanding of Videos Based on Chain-of-Thought](https://arxiv.org/abs/2506.08817)
*Shuyi Zhang,Xiaoshuai Hao,Yingbo Tang,Lingfeng Zhang,Pengwei Wang,Zhongyuan Wang,Hongxuan Ma,Shanghang Zhang*

Main category: cs.CV

TL;DR: 提出Video-CoT数据集与基准测试，增强视频时空理解能力评估


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型在视频时空细节理解上存在不足，阻碍深度视频分析

Method: 构建包含19.2万细粒度问答对和2.3万CoT标注的数据集，并设计多任务评估体系

Result: 实验证实现有模型在时空理解任务上表现显著不足

Conclusion: 数据集为多媒体理解研究开辟新方向，支持智能系统视频分析能力发展

Abstract: Video content comprehension is essential for various applications, ranging
from video analysis to interactive systems. Despite advancements in large-scale
vision-language models (VLMs), these models often struggle to capture the
nuanced, spatiotemporal details essential for thorough video analysis. To
address this gap, we introduce Video-CoT, a groundbreaking dataset designed to
enhance spatiotemporal understanding using Chain-of-Thought (CoT)
methodologies. Video-CoT contains 192,000 fine-grained spa-tiotemporal
question-answer pairs and 23,000 high-quality CoT-annotated samples, providing
a solid foundation for evaluating spatiotemporal understanding in video
comprehension. Additionally, we provide a comprehensive benchmark for assessing
these tasks, with each task featuring 750 images and tailored evaluation
metrics. Our extensive experiments reveal that current VLMs face significant
challenges in achieving satisfactory performance, high-lighting the
difficulties of effective spatiotemporal understanding. Overall, the Video-CoT
dataset and benchmark open new avenues for research in multimedia understanding
and support future innovations in intelligent systems requiring advanced video
analysis capabilities. By making these resources publicly available, we aim to
encourage further exploration in this critical area. Project
website:https://video-cot.github.io/ .

</details>


### [69] [CulturalFrames: Assessing Cultural Expectation Alignment in Text-to-Image Models and Evaluation Metrics](https://arxiv.org/abs/2506.08835)
*Shravan Nayak,Mehar Bhatia,Xiaofeng Zhang,Verena Rieser,Lisa Anne Hendricks,Sjoerd van Steenkiste,Yash Goyal,Karolina Sta艅czak,Aishwarya Agrawal*

Main category: cs.CV

TL;DR: T2I模型在显性和隐性文化表达上存在显著不足，现有评估指标与人类判断脱节


<details>
  <summary>Details</summary>
Motivation: 评估主流文本生成图像模型在多元文化表征上的准确性需求

Method: 构建包含10国5领域983个prompt的CulturalFrames基准，收集3637张图像及万级人工标注

Result: 模型平均44%未达文化期望（显性失误率68%，隐性49%），现有指标与人类评价低相关

Conclusion: 暴露文化表征缺陷，为开发文化敏感的T2I模型和评估体系指明方向

Abstract: The increasing ubiquity of text-to-image (T2I) models as tools for visual
content generation raises concerns about their ability to accurately represent
diverse cultural contexts. In this work, we present the first study to
systematically quantify the alignment of T2I models and evaluation metrics with
respect to both explicit as well as implicit cultural expectations. To this
end, we introduce CulturalFrames, a novel benchmark designed for rigorous human
evaluation of cultural representation in visual generations. Spanning 10
countries and 5 socio-cultural domains, CulturalFrames comprises 983 prompts,
3637 corresponding images generated by 4 state-of-the-art T2I models, and over
10k detailed human annotations. We find that T2I models not only fail to meet
the more challenging implicit expectations but also the less challenging
explicit expectations. Across models and countries, cultural expectations are
missed an average of 44% of the time. Among these failures, explicit
expectations are missed at a surprisingly high average rate of 68%, while
implicit expectation failures are also significant, averaging 49%. Furthermore,
we demonstrate that existing T2I evaluation metrics correlate poorly with human
judgments of cultural alignment, irrespective of their internal reasoning.
Collectively, our findings expose critical gaps, providing actionable
directions for developing more culturally informed T2I models and evaluation
methodologies.

</details>


### [70] [Adapting Vision-Language Foundation Model for Next Generation Medical Ultrasound Image Analysis](https://arxiv.org/abs/2506.08849)
*Jingguo Qu,Xinyang Han,Tonghuan Xiao,Jia Ai,Juan Wu,Tong Zhao,Jing Qin,Ann Dorothy King,Winnie Chiu-Wing Chu,Jing Cai,Michael Tin-Cheung Ying谋nst*

Main category: cs.CV

TL;DR: 提出基于大语言模型的视觉语言基础模型域适应方法，显著提升超声影像分析的分类和分割性能


<details>
  <summary>Details</summary>
Motivation: 解决医学超声影像分析中人工标注效率低、现有视觉语言模型领域适应性差的问题

Method: 使用LLM作为文本细化器，设计领域适应策略和任务驱动头，建立微调流程

Result: 在6个超声数据集上超越现有视觉语言和纯基础模型，分类和分割任务表现最优

Conclusion: 证明了领域适应策略的有效性，为医学影像分析提供了新的解决方案，代码已开源

Abstract: Medical ultrasonography is an essential imaging technique for examining
superficial organs and tissues, including lymph nodes, breast, and thyroid. It
employs high-frequency ultrasound waves to generate detailed images of the
internal structures of the human body. However, manually contouring regions of
interest in these images is a labor-intensive task that demands expertise and
often results in inconsistent interpretations among individuals.
Vision-language foundation models, which have excelled in various computer
vision applications, present new opportunities for enhancing ultrasound image
analysis. Yet, their performance is hindered by the significant differences
between natural and medical imaging domains. This research seeks to overcome
these challenges by developing domain adaptation methods for vision-language
foundation models. In this study, we explore the fine-tuning pipeline for
vision-language foundation models by utilizing large language model as text
refiner with special-designed adaptation strategies and task-driven heads. Our
approach has been extensively evaluated on six ultrasound datasets and two
tasks: segmentation and classification. The experimental results show that our
method can effectively improve the performance of vision-language foundation
models for ultrasound image analysis, and outperform the existing
state-of-the-art vision-language and pure foundation models. The source code of
this study is available at
\href{https://github.com/jinggqu/NextGen-UIA}{GitHub}.

</details>


### [71] [Spatial Transcriptomics Expression Prediction from Histopathology Based on Cross-Modal Mask Reconstruction and Contrastive Learning](https://arxiv.org/abs/2506.08854)
*Junzhuo Liu,Markus Eckstein,Zhixiang Wang,Friedrich Feuerhake,Dorit Merhof*

Main category: cs.CV

TL;DR: 提出基于对比学习的深度学习方法，从全切片图像预测空间转录组基因表达


<details>
  <summary>Details</summary>
Motivation: 针对空间转录组数据获取成本高、大规模数据稀缺的问题，开发预测模型突破数据限制

Method: 使用对比学习框架，通过深度学习从组织病理图像中学习空间基因表达模式

Result: 在六种疾病数据集上，对高表达/高变异/标志基因的预测PCC分别提升6.27%、6.11%和11.26%

Conclusion: 方法有效保持基因关联性，适用于小样本数据，并为基于生物标志物的肿瘤定位提供新思路

Abstract: Spatial transcriptomics is a technology that captures gene expression levels
at different spatial locations, widely used in tumor microenvironment analysis
and molecular profiling of histopathology, providing valuable insights into
resolving gene expression and clinical diagnosis of cancer. Due to the high
cost of data acquisition, large-scale spatial transcriptomics data remain
challenging to obtain. In this study, we develop a contrastive learning-based
deep learning method to predict spatially resolved gene expression from
whole-slide images. Evaluation across six different disease datasets
demonstrates that, compared to existing studies, our method improves Pearson
Correlation Coefficient (PCC) in the prediction of highly expressed genes,
highly variable genes, and marker genes by 6.27%, 6.11%, and 11.26%
respectively. Further analysis indicates that our method preserves gene-gene
correlations and applies to datasets with limited samples. Additionally, our
method exhibits potential in cancer tissue localization based on biomarker
expression.

</details>


### [72] [StreamSplat: Towards Online Dynamic 3D Reconstruction from Uncalibrated Video Streams](https://arxiv.org/abs/2506.08862)
*Zike Wu,Qi Yan,Xuanyu Yi,Lele Wang,Renjie Liao*

Main category: cs.CV

TL;DR: 提出首个前馈框架StreamSplat，实现未校准视频流在线转换为动态3D高斯溅射表示，解决实时重建、动态建模与长期稳定性的联合挑战。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以同时满足未校准视频实时处理、动态场景精确建模及长期稳定性需求，需开发兼顾三者的一体化解决方案。

Method: 结合静态编码器的概率采样机制（优化3DGS位置预测）与动态解码器的双向形变场（高效建模动态变化）。

Result: 在静态/动态基准测试中全面超越现有方法，首次支持任意长度视频流的在线重建，且重建质量和效率显著提升。

Conclusion: 突破了动态3D重建领域的关键技术瓶颈，为AR/VR、机器人等实时应用提供了可扩展的解决方案。

Abstract: Real-time reconstruction of dynamic 3D scenes from uncalibrated video streams
is crucial for numerous real-world applications. However, existing methods
struggle to jointly address three key challenges: 1) processing uncalibrated
inputs in real time, 2) accurately modeling dynamic scene evolution, and 3)
maintaining long-term stability and computational efficiency. To this end, we
introduce StreamSplat, the first fully feed-forward framework that transforms
uncalibrated video streams of arbitrary length into dynamic 3D Gaussian
Splatting (3DGS) representations in an online manner, capable of recovering
scene dynamics from temporally local observations. We propose two key technical
innovations: a probabilistic sampling mechanism in the static encoder for 3DGS
position prediction, and a bidirectional deformation field in the dynamic
decoder that enables robust and efficient dynamic modeling. Extensive
experiments on static and dynamic benchmarks demonstrate that StreamSplat
consistently outperforms prior works in both reconstruction quality and dynamic
scene modeling, while uniquely supporting online reconstruction of arbitrarily
long video streams. Code and models are available at
https://github.com/nickwzk/StreamSplat.

</details>


### [73] [DiscoVLA: Discrepancy Reduction in Vision, Language, and Alignment for Parameter-Efficient Video-Text Retrieval](https://arxiv.org/abs/2506.08887)
*Leqi Shen,Guoqiang Gong,Tianxiang Hao,Tao He,Yifeng Zhang,Pengzhang Liu,Sicheng Zhao,Jungong Han,Guiguang Ding*

Main category: cs.CV

TL;DR: 提出DiscoVLA方法同时解决视频文本检索中的视觉、语言和对齐差异，显著提升CLIP模型的视频检索性能


<details>
  <summary>Details</summary>
Motivation: 现有视频文本检索方法主要关注视觉差异，忽视了语言和对齐层面的差异

Method: 通过图像-视频特征融合整合多级特征、生成伪图像描述学习细粒度对齐、图像-视频对齐蒸馏增强跨模态对齐

Result: 在MSRVTT数据集上以CLIP(ViT-B/16)达到50.5% R@1，优于先前方法1.5%

Conclusion: 同时解决三类差异的框架显著提升了视频文本检索性能，证明了跨层级特征融合和对齐蒸馏的有效性

Abstract: The parameter-efficient adaptation of the image-text pretraining model CLIP
for video-text retrieval is a prominent area of research. While CLIP is focused
on image-level vision-language matching, video-text retrieval demands
comprehensive understanding at the video level. Three key discrepancies emerge
in the transfer from image-level to video-level: vision, language, and
alignment. However, existing methods mainly focus on vision while neglecting
language and alignment. In this paper, we propose Discrepancy Reduction in
Vision, Language, and Alignment (DiscoVLA), which simultaneously mitigates all
three discrepancies. Specifically, we introduce Image-Video Features Fusion to
integrate image-level and video-level features, effectively tackling both
vision and language discrepancies. Additionally, we generate pseudo image
captions to learn fine-grained image-level alignment. To mitigate alignment
discrepancies, we propose Image-to-Video Alignment Distillation, which
leverages image-level alignment knowledge to enhance video-level alignment.
Extensive experiments demonstrate the superiority of our DiscoVLA. In
particular, on MSRVTT with CLIP (ViT-B/16), DiscoVLA outperforms previous
methods by 1.5% in R@1, reaching a final score of 50.5% R@1. The code is
available at https://github.com/LunarShen/DsicoVLA.

</details>


### [74] [Product of Experts for Visual Generation](https://arxiv.org/abs/2506.08894)
*Yunzhi Zhang,Carson Murtuza-Lanier,Zizhang Li,Yilun Du,Jiajun Wu*

Main category: cs.CV

TL;DR: 提出基于产品专家框架的推理时异构知识整合方法，提升视觉生成任务的灵活性与可控性


<details>
  <summary>Details</summary>
Motivation: 现有方法未充分整合视觉生成模型、视觉语言模型及人工知识模型（图形引擎/物理模拟器）等异构知识源

Method: 采用产品专家框架(PoE)，通过退火重要抽样(AIS)实现异构模型的无需训练的知识组合

Result: 在图像/视频合成任务中展现优于单一模型的动态控制能力，提供灵活的用户交互界面

Conclusion: 验证了推理时知识组合的有效性，为多源知识整合的生成系统提供新范式

Abstract: Modern neural models capture rich priors and have complementary knowledge
over shared data domains, e.g., images and videos. Integrating diverse
knowledge from multiple sources -- including visual generative models, visual
language models, and sources with human-crafted knowledge such as graphics
engines and physics simulators -- remains under-explored. We propose a Product
of Experts (PoE) framework that performs inference-time knowledge composition
from heterogeneous models. This training-free approach samples from the product
distribution across experts via Annealed Importance Sampling (AIS). Our
framework shows practical benefits in image and video synthesis tasks, yielding
better controllability than monolithic methods and additionally providing
flexible user interfaces for specifying visual generation goals.

</details>


### [75] [WetCat: Automating Skill Assessment in Wetlab Cataract Surgery Videos](https://arxiv.org/abs/2506.08896)
*Negin Ghamsarian,Raphael Sznitman,Klaus Schoeffmann,Jens Kowal*

Main category: cs.CV

TL;DR: 提出首个眼科湿实验室白内障手术视频数据集WetCat，支持AI驱动的自动化技能评估。


<details>
  <summary>Details</summary>
Motivation: 传统眼科手术培训依赖主观人工评估，现有数据集无法满足湿实验室环境下的全面技能分析需求。

Method: 构建包含人工眼球手术视频、阶段标注和关键解剖结构分割的WetCat数据集，聚焦囊膜撕开和超声乳化关键阶段。

Result: 提供标准化临床指标对齐的标注框架，建立自动化工作流分析和技能评估新基准，数据集已开源。

Conclusion: 该数据集推动了客观可扩展的眼科手术教育，为AI评估工具开发奠定基础，提升培训效率与客观性。

Abstract: To meet the growing demand for systematic surgical training, wetlab
environments have become indispensable platforms for hands-on practice in
ophthalmology. Yet, traditional wetlab training depends heavily on manual
performance evaluations, which are labor-intensive, time-consuming, and often
subject to variability. Recent advances in computer vision offer promising
avenues for automated skill assessment, enhancing both the efficiency and
objectivity of surgical education. Despite notable progress in ophthalmic
surgical datasets, existing resources predominantly focus on real surgeries or
isolated tasks, falling short of supporting comprehensive skill evaluation in
controlled wetlab settings. To address these limitations, we introduce WetCat,
the first dataset of wetlab cataract surgery videos specifically curated for
automated skill assessment. WetCat comprises high-resolution recordings of
surgeries performed by trainees on artificial eyes, featuring comprehensive
phase annotations and semantic segmentations of key anatomical structures.
These annotations are meticulously designed to facilitate skill assessment
during the critical capsulorhexis and phacoemulsification phases, adhering to
standardized surgical skill assessment frameworks. By focusing on these
essential phases, WetCat enables the development of interpretable, AI-driven
evaluation tools aligned with established clinical metrics. This dataset lays a
strong foundation for advancing objective, scalable surgical education and sets
a new benchmark for automated workflow analysis and skill assessment in
ophthalmology training. The dataset and annotations are publicly available in
Synapse https://www.synapse.org/Synapse:syn66401174/files.

</details>


### [76] [MIRAGE: Multimodal foundation model and benchmark for comprehensive retinal OCT image analysis](https://arxiv.org/abs/2506.08900)
*Jos茅 Morano,Botond Fazekas,Emese S眉kei,Ronald Fecso,Taha Emre,Markus Gumpinger,Georg Faustmann,Marzieh Oghbaie,Ursula Schmidt-Erfurth,Hrvoje Bogunovi膰*

Main category: cs.CV

TL;DR: 提出多模态基础模型MIRAGE用于视网膜OCT/SLO图像分析，在分类和分割任务中表现优于现有方法


<details>
  <summary>Details</summary>
Motivation: 现有眼科基础模型缺乏多模态支持且验证不足，需解决AI模型泛化性差和标注依赖问题

Method: 开发基于大规模未标记数据训练的多模态基础模型，设计包含分类和分割任务的新型评估基准

Result: MIRAGE在跨模态任务中全面超越通用/专用基础模型和分割方法，验证其作为视网膜分析基础系统的潜力

Conclusion: 首个支持多模态的视网膜分析基础模型，开源模型和基准测试推动眼科AI系统开发

Abstract: Artificial intelligence (AI) has become a fundamental tool for assisting
clinicians in analyzing ophthalmic images, such as optical coherence tomography
(OCT). However, developing AI models often requires extensive annotation, and
existing models tend to underperform on independent, unseen data. Foundation
models (FMs), large AI models trained on vast unlabeled datasets, have shown
promise in overcoming these challenges. Nonetheless, available FMs for
ophthalmology lack extensive validation, especially for segmentation tasks, and
focus on a single imaging modality. In this context, we propose MIRAGE, a novel
multimodal FM for the analysis of OCT and scanning laser ophthalmoscopy (SLO)
images. Additionally, we propose a new evaluation benchmark with OCT/SLO
classification and segmentation tasks. The comparison with general and
specialized FMs and segmentation methods shows the superiority of MIRAGE in
both types of tasks, highlighting its suitability as a basis for the
development of robust AI systems for retinal OCT image analysis. Both MIRAGE
and the evaluation benchmark are publicly available:
https://github.com/j-morano/MIRAGE.

</details>


### [77] [Hyperbolic Dual Feature Augmentation for Open-Environment](https://arxiv.org/abs/2506.08906)
*Peilin Yu,Yuwei Wu,Zhi Gao,Xiaomeng Fan,Shuo Yang,Yunde Jia*

Main category: cs.CV

TL;DR: 提出双曲空间特征增强方法，通过神经ODE和元学习实现开放环境下可见/不可见类别的特征增强


<details>
  <summary>Details</summary>
Motivation: 现有双曲特征增强局限于封闭环境，无法处理开放环境中的未知类别

Method: 1) 元学习增强的神经ODE估计特征分布 2) 正则化保持层次结构 3) 推导双曲增强损失上界

Result: 在增量学习、小样本开放集识别等5类任务中提升双曲算法性能

Conclusion: 突破封闭环境限制，为双曲模型在开放环境应用提供新思路

Abstract: Feature augmentation generates novel samples in the feature space, providing
an effective way to enhance the generalization ability of learning algorithms
with hyperbolic geometry. Most hyperbolic feature augmentation is confined to
closed-environment, assuming the number of classes is fixed (\emph{i.e.}, seen
classes) and generating features only for these classes. In this paper, we
propose a hyperbolic dual feature augmentation method for open-environment,
which augments features for both seen and unseen classes in the hyperbolic
space. To obtain a more precise approximation of the real data distribution for
efficient training, (1) we adopt a neural ordinary differential equation
module, enhanced by meta-learning, estimating the feature distributions of both
seen and unseen classes; (2) we then introduce a regularizer to preserve the
latent hierarchical structures of data in the hyperbolic space; (3) we also
derive an upper bound for the hyperbolic dual augmentation loss, allowing us to
train a hyperbolic model using infinite augmentations for seen and unseen
classes. Extensive experiments on five open-environment tasks:
class-incremental learning, few-shot open-set recognition, few-shot learning,
zero-shot learning, and general image classification, demonstrate that our
method effectively enhances the performance of hyperbolic algorithms in
open-environment.

</details>


### [78] [SkipVAR: Accelerating Visual Autoregressive Modeling via Adaptive Frequency-Aware Skipping](https://arxiv.org/abs/2506.08908)
*Jiajun Li,Yue Ma,Xinyu Zhang,Qingyan Wei,Songhua Liu,Linfeng Zhang*

Main category: cs.CV

TL;DR: 提出SkipVAR框架通过自适应步骤跳过和分支替换加速视觉自回归模型，保持生成质量


<details>
  <summary>Details</summary>
Motivation: 现有VAR模型高频成分导致推理延迟，但计算冗余未被充分研究

Method: 自动步骤跳过策略+无条件分支替换技术，构建基于频率信息自适应选择加速策略的框架

Result: 1.81倍整体加速，GenEval基准测试2.62倍提速，SSIM保持0.88

Conclusion: 验证了无需训练的频域感知自适应加速在自回归图像生成中的有效性

Abstract: Recent studies on Visual Autoregressive (VAR) models have highlighted that
high-frequency components, or later steps, in the generation process contribute
disproportionately to inference latency. However, the underlying computational
redundancy involved in these steps has yet to be thoroughly investigated. In
this paper, we conduct an in-depth analysis of the VAR inference process and
identify two primary sources of inefficiency: step redundancy and unconditional
branch redundancy. To address step redundancy, we propose an automatic
step-skipping strategy that selectively omits unnecessary generation steps to
improve efficiency. For unconditional branch redundancy, we observe that the
information gap between the conditional and unconditional branches is minimal.
Leveraging this insight, we introduce unconditional branch replacement, a
technique that bypasses the unconditional branch to reduce computational cost.
Notably, we observe that the effectiveness of acceleration strategies varies
significantly across different samples. Motivated by this, we propose SkipVAR,
a sample-adaptive framework that leverages frequency information to dynamically
select the most suitable acceleration strategy for each instance. To evaluate
the role of high-frequency information, we introduce high-variation benchmark
datasets that test model sensitivity to fine details. Extensive experiments
show SkipVAR achieves over 0.88 average SSIM with up to 1.81x overall
acceleration and 2.62x speedup on the GenEval benchmark, maintaining model
quality. These results confirm the effectiveness of frequency-aware,
training-free adaptive acceleration for scalable autoregressive image
generation. Our code is available at https://github.com/fakerone-li/SkipVAR and
has been publicly released.

</details>


### [79] [Inherently Faithful Attention Maps for Vision Transformers](https://arxiv.org/abs/2506.08915)
*Ananthu Aniraj,Cassio F. Dantas,Dino Ienco,Diego Marcos*

Main category: cs.CV

TL;DR: 提出两阶段注意力框架解决上下文干扰导致的物体感知偏差问题


<details>
  <summary>Details</summary>
Motivation: 图像中的上下文信息可能引起物体表征偏差，尤其在分布外背景场景下更为明显，需平衡全局信息利用与局部聚焦的矛盾

Method: 采用联合训练的两阶段架构：第一阶段识别任务相关区域，第二阶段通过二值注意力掩码聚焦目标区域进行特征分析

Result: 在多种基准测试中显著提升对虚假关联和分布外背景的鲁棒性

Conclusion: 通过注意力掩码机制实现了上下文信息的可控利用，为物体中心任务提供了新的解决方案

Abstract: We introduce an attention-based method that uses learned binary attention
masks to ensure that only attended image regions influence the prediction.
Context can strongly affect object perception, sometimes leading to biased
representations, particularly when objects appear in out-of-distribution
backgrounds. At the same time, many image-level object-centric tasks require
identifying relevant regions, often requiring context. To address this
conundrum, we propose a two-stage framework: stage 1 processes the full image
to discover object parts and identify task-relevant regions, while stage 2
leverages input attention masking to restrict its receptive field to these
regions, enabling a focused analysis while filtering out potentially spurious
information. Both stages are trained jointly, allowing stage 2 to refine stage
1. Extensive experiments across diverse benchmarks demonstrate that our
approach significantly improves robustness against spurious correlations and
out-of-distribution backgrounds.

</details>


### [80] [Socratic-MCTS: Test-Time Visual Reasoning by Asking the Right Questions](https://arxiv.org/abs/2506.08927)
*David Acuna,Ximing Lu,Jaehun Jung,Hyunwoo Kim,Amlan Kar,Sanja Fidler,Yejin Choi*

Main category: cs.CV

TL;DR: 提出基于蒙特卡洛树搜索的推理框架，无需额外训练即可增强现有视觉语言模型的长链推理能力


<details>
  <summary>Details</summary>
Motivation: 针对已部署的非推理型视觉语言模型无法进行长链思考的问题，探索无需重新训练的知识激发机制

Method: 采用蒙特卡洛树搜索算法，通过注入子问题-子答案对构建推理轨迹，将知识碎片进行关联

Result: 在MMMU-PRO基准上整体提升2%，其中文科领域显著提升9%，三个测试集均取得持续改进

Conclusion: 论证了搜索机制替代训练方案的有效性，为优化现有模型资源利用提供新思路

Abstract: Recent research in vision-language models (VLMs) has centered around the
possibility of equipping them with implicit long-form chain-of-thought
reasoning -- akin to the success observed in language models -- via
distillation and reinforcement learning. But what about the non-reasoning
models already trained and deployed across the internet? Should we simply
abandon them, or is there hope for a search mechanism that can elicit hidden
knowledge and induce long reasoning traces -- without any additional training
or supervision? In this paper, we explore this possibility using a Monte Carlo
Tree Search (MCTS)-inspired algorithm, which injects subquestion-subanswer
pairs into the model's output stream. We show that framing reasoning as a
search process -- where subquestions act as latent decisions within a broader
inference trajectory -- helps the model "connect the dots" between fragmented
knowledge and produce extended reasoning traces in non-reasoning models. We
evaluate our method across three benchmarks and observe consistent
improvements. Notably, our approach yields a 2% overall improvement on
MMMU-PRO, including a significant 9% gain in Liberal Arts.

</details>


### [81] [What Limits Virtual Agent Application? OmniBench: A Scalable Multi-Dimensional Benchmark for Essential Virtual Agent Capabilities](https://arxiv.org/abs/2506.08933)
*Wendong Bu,Yang Wu,Qifan Yu,Minghe Gao,Bingchen Miao,Zhenkui Zhang,Kaihang Pan,Yunfei Li,Mengze Li,Wei Ji,Juncheng Li,Siliang Tang,Yueting Zhuang*

Main category: cs.CV

TL;DR: 提出OmniBench自生成图结构基准与OmniEval多维评估框架，解决现有多模态代理评测的复杂性控制和评估维度不足问题。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大模型代理评测存在任务复杂度不可控、人工标注成本高、场景有限且缺乏多维评估体系的问题。

Method: 通过子任务组合自动合成可控复杂度的图结构任务(OmniBench)，并提出包含子任务评估、图指标和10项能力的多维评估框架(OmniEval)。

Result: 构建含36k图结构任务的数据集(人工接受率91%)，实验证明其训练效率优于人工标注数据，并揭示主流模型多维能力差异。

Conclusion: 该框架为多模态代理提供系统化评测方案，数据集生成与评估方法对模型迭代具有指导意义。

Abstract: As multimodal large language models (MLLMs) advance, MLLM-based virtual
agents have demonstrated remarkable performance. However, existing benchmarks
face significant limitations, including uncontrollable task complexity,
extensive manual annotation with limited scenarios, and a lack of
multidimensional evaluation. In response to these challenges, we introduce
OmniBench, a self-generating, cross-platform, graph-based benchmark with an
automated pipeline for synthesizing tasks of controllable complexity through
subtask composition. To evaluate the diverse capabilities of virtual agents on
the graph, we further present OmniEval, a multidimensional evaluation framework
that includes subtask-level evaluation, graph-based metrics, and comprehensive
tests across 10 capabilities. Our synthesized dataset contains 36k
graph-structured tasks across 20 scenarios, achieving a 91\% human acceptance
rate. Training on our graph-structured data shows that it can more efficiently
guide agents compared to manually annotated data. We conduct multidimensional
evaluations for various open-source and closed-source models, revealing their
performance across various capabilities and paving the way for future
advancements. Our project is available at https://omni-bench.github.io/.

</details>


### [82] [SSS: Semi-Supervised SAM-2 with Efficient Prompting for Medical Imaging Segmentation](https://arxiv.org/abs/2506.08949)
*Hongjie Zhu,Xiwei Liu,Rundong Xue,Zeyu Zhang,Yong Xu,Daji Ergu,Ying Cai,Yang Zhao*

Main category: cs.CV

TL;DR: 结合SAM-2视觉基模型与判别特征增强机制，提出新型半监督医学图像分割方法SSS


<details>
  <summary>Details</summary>
Motivation: 解决医学图像分割对标注数据过度依赖的问题，通过半监督学习有效利用未标注数据提升模型性能

Method: 基于弱强一致性框架，集成判别特征增强(DFE)进行多尺度特征建模，开发物理约束滑动窗口(PCSW)提示生成器适配SAM-2模型

Result: 在BHSD数据集上达到53.15 Dice分数，较SOTA提升3.65个点

Conclusion: 通过融合基础模型先验知识与传统半监督范式，显著提升医学图像分割性能，为降低标注成本提供新思路

Abstract: In the era of information explosion, efficiently leveraging large-scale
unlabeled data while minimizing the reliance on high-quality pixel-level
annotations remains a critical challenge in the field of medical imaging.
Semi-supervised learning (SSL) enhances the utilization of unlabeled data by
facilitating knowledge transfer, significantly improving the performance of
fully supervised models and emerging as a highly promising research direction
in medical image analysis. Inspired by the ability of Vision Foundation Models
(e.g., SAM-2) to provide rich prior knowledge, we propose SSS (Semi-Supervised
SAM-2), a novel approach that leverages SAM-2's robust feature extraction
capabilities to uncover latent knowledge in unlabeled medical images, thus
effectively enhancing feature support for fully supervised medical image
segmentation. Specifically, building upon the single-stream "weak-to-strong"
consistency regularization framework, this paper introduces a Discriminative
Feature Enhancement (DFE) mechanism to further explore the feature
discrepancies introduced by various data augmentation strategies across
multiple views. By leveraging feature similarity and dissimilarity across
multi-scale augmentation techniques, the method reconstructs and models the
features, thereby effectively optimizing the salient regions. Furthermore, a
prompt generator is developed that integrates Physical Constraints with a
Sliding Window (PCSW) mechanism to generate input prompts for unlabeled data,
fulfilling SAM-2's requirement for additional prompts. Extensive experiments
demonstrate the superiority of the proposed method for semi-supervised medical
image segmentation on two multi-label datasets, i.e., ACDC and BHSD. Notably,
SSS achieves an average Dice score of 53.15 on BHSD, surpassing the previous
state-of-the-art method by +3.65 Dice. Code will be available at
https://github.com/AIGeeksGroup/SSS.

</details>


### [83] [Cross-Spectral Body Recognition with Side Information Embedding: Benchmarks on LLCM and Analyzing Range-Induced Occlusions on IJB-MDF](https://arxiv.org/abs/2506.08953)
*Anirudh Nanduri,Siyuan Huang,Rama Chellappa*

Main category: cs.CV

TL;DR: 通过整合相机信息嵌入改进视觉Transformer，在跨光谱人体识别中实现SOTA，并首次系统评估可见-红外场景下的遮挡问题


<details>
  <summary>Details</summary>
Motivation: 解决现有可见-红外跨光谱人体识别存在的领域差异和遮挡评估空白，现有数据集缺乏真实遮挡场景

Method: 在ViT中引入侧信息嵌入(SIE)，特别探索相机信息编码；使用IJB-MDF数据集进行跨距离/跨光谱遮挡分析

Result: 仅编码相机信息即在LLCM数据集达到最优性能，首次建立可见-红外场景的跨范围遮挡评估基准

Conclusion: 相机元数据对跨光谱匹配具有重要价值，提出的方法为复杂场景下的生物识别提供了新解决方案和研究方向

Abstract: Vision Transformers (ViTs) have demonstrated impressive performance across a
wide range of biometric tasks, including face and body recognition. In this
work, we adapt a ViT model pretrained on visible (VIS) imagery to the
challenging problem of cross-spectral body recognition, which involves matching
images captured in the visible and infrared (IR) domains. Recent ViT
architectures have explored incorporating additional embeddings beyond
traditional positional embeddings. Building on this idea, we integrate Side
Information Embedding (SIE) and examine the impact of encoding domain and
camera information to enhance cross-spectral matching. Surprisingly, our
results show that encoding only camera information - without explicitly
incorporating domain information - achieves state-of-the-art performance on the
LLCM dataset. While occlusion handling has been extensively studied in
visible-spectrum person re-identification (Re-ID), occlusions in
visible-infrared (VI) Re-ID remain largely underexplored - primarily because
existing VI-ReID datasets, such as LLCM, SYSU-MM01, and RegDB, predominantly
feature full-body, unoccluded images. To address this gap, we analyze the
impact of range-induced occlusions using the IARPA Janus Benchmark Multi-Domain
Face (IJB-MDF) dataset, which provides a diverse set of visible and infrared
images captured at various distances, enabling cross-range, cross-spectral
evaluations.

</details>


### [84] [Segment Concealed Objects with Incomplete Supervision](https://arxiv.org/abs/2506.08955)
*Chunming He,Kai Li,Yachao Zhang,Ziyun Yang,Youwei Pang,Longxiang Tang,Chengyu Fang,Yulun Zhang,Linghe Kong,Xiu Li,Sina Farsiu*

Main category: cs.CV

TL;DR: 提出首个ISCOS统一方法SEE，结合SAM生成伪标签和混合粒度特征分组，解决不完全监督与隐蔽目标分割难题


<details>
  <summary>Details</summary>
Motivation: 隐蔽目标分割面临标注数据不足和目标-背景高度相似的双重挑战，需要开发有效的不完全监督学习方法

Method: 基于mean-teacher框架集成SAM生成伪标签，设计伪标签优化策略，并构建混合粒度特征分组模块增强特征表达

Result: 在多个ISCOS任务中取得SOTA性能，且可作为即插即用模块提升现有模型效果

Conclusion: SEE通过创新的伪标签优化和特征聚类机制，为不完全监督下的复杂隐蔽目标分割提供了有效解决方案

Abstract: Incompletely-Supervised Concealed Object Segmentation (ISCOS) involves
segmenting objects that seamlessly blend into their surrounding environments,
utilizing incompletely annotated data, such as weak and semi-annotations, for
model training. This task remains highly challenging due to (1) the limited
supervision provided by the incompletely annotated training data, and (2) the
difficulty of distinguishing concealed objects from the background, which
arises from the intrinsic similarities in concealed scenarios. In this paper,
we introduce the first unified method for ISCOS to address these challenges. To
tackle the issue of incomplete supervision, we propose a unified mean-teacher
framework, SEE, that leverages the vision foundation model, ``\emph{Segment
Anything Model (SAM)}'', to generate pseudo-labels using coarse masks produced
by the teacher model as prompts. To mitigate the effect of low-quality
segmentation masks, we introduce a series of strategies for pseudo-label
generation, storage, and supervision. These strategies aim to produce
informative pseudo-labels, store the best pseudo-labels generated, and select
the most reliable components to guide the student model, thereby ensuring
robust network training. Additionally, to tackle the issue of intrinsic
similarity, we design a hybrid-granularity feature grouping module that groups
features at different granularities and aggregates these results. By clustering
similar features, this module promotes segmentation coherence, facilitating
more complete segmentation for both single-object and multiple-object images.
We validate the effectiveness of our approach across multiple ISCOS tasks, and
experimental results demonstrate that our method achieves state-of-the-art
performance. Furthermore, SEE can serve as a plug-and-play solution, enhancing
the performance of existing models.

</details>


### [85] [Data Augmentation For Small Object using Fast AutoAugment](https://arxiv.org/abs/2506.08956)
*DaeEun Yoon,Semin Kim,SangWook Yoo,Jongha Lee*

Main category: cs.CV

TL;DR: 提出基于快速自动增强的小物体检测数据优化方法，性能提升20%


<details>
  <summary>Details</summary>
Motivation: 当前目标检测技术虽进步显著，但小物体检测性能仍远低于大物体，成为计算机视觉领域重要挑战

Method: 采用Fast AutoAugment快速搜索最优数据增强策略以克服小物体检测性能退化

Result: 在DOTA数据集上实现20%的性能提升

Conclusion: 该方法有效解决小目标检测难题，对遥感影像分析等实际应用具有重要价值

Abstract: In recent years, there has been tremendous progress in object detection
performance. However, despite these advances, the detection performance for
small objects is significantly inferior to that of large objects. Detecting
small objects is one of the most challenging and important problems in computer
vision. To improve the detection performance for small objects, we propose an
optimal data augmentation method using Fast AutoAugment. Through our proposed
method, we can quickly find optimal augmentation policies that can overcome
degradation when detecting small objects, and we achieve a 20% performance
improvement on the DOTA dataset.

</details>


### [86] [ORIDa: Object-centric Real-world Image Composition Dataset](https://arxiv.org/abs/2506.08964)
*Jinwoo Kim,Sangmin Han,Jinho Jeong,Jiwoo Choi,Dongyoung Kim,Seon Joo Kim*

Main category: cs.CV

TL;DR: 提出首个大规模真实场景物体合成数据集ORIDa，支持多样化对象位置与场景组合研究。


<details>
  <summary>Details</summary>
Motivation: 现有数据集因缺乏多样性和规模，难以覆盖真实场景的复杂性。

Method: 构建包含3万+图像的数据集ORIDa，采用事实-反事实组合和纯事实场景两种数据模式。

Result: 数据集涵盖200个物体在多种场景中的位置变化，验证其对合成任务的研究价值。

Conclusion: ORIDa填补了真实世界图像合成数据缺口，为生成模型研究提供关键资源。

Abstract: Object compositing, the task of placing and harmonizing objects in images of
diverse visual scenes, has become an important task in computer vision with the
rise of generative models. However, existing datasets lack the diversity and
scale required to comprehensively explore real-world scenarios. We introduce
ORIDa (Object-centric Real-world Image Composition Dataset), a large-scale,
real-captured dataset containing over 30,000 images featuring 200 unique
objects, each of which is presented across varied positions and scenes. ORIDa
has two types of data: factual-counterfactual sets and factual-only scenes. The
factual-counterfactual sets consist of four factual images showing an object in
different positions within a scene and a single counterfactual (or background)
image of the scene without the object, resulting in five images per scene. The
factual-only scenes include a single image containing an object in a specific
context, expanding the variety of environments. To our knowledge, ORIDa is the
first publicly available dataset with its scale and complexity for real-world
image composition. Extensive analysis and experiments highlight the value of
ORIDa as a resource for advancing further research in object compositing.

</details>


### [87] [ADAM: Autonomous Discovery and Annotation Model using LLMs for Context-Aware Annotations](https://arxiv.org/abs/2506.08968)
*Amirreza Rouhi,Solmaz Arezoomandan,Knut Peterson,Joseph T. Woods,David K. Han*

Main category: cs.CV

TL;DR: 提出无需训练的ADAM框架，通过LLM生成上下文标签和视觉嵌入实现开放世界物体标注


<details>
  <summary>Details</summary>
Motivation: 传统目标检测依赖预定义类别，无法识别开放场景中的新物体

Method: 融合LLM上下文标签生成与CLIP视觉嵌入构建ELR库，通过检索投票和自优化循环实现标签标注

Result: 在COCO/PASCAL实现无监督新类别标注，无需微调即超越基线方法

Conclusion: 首次实现视觉与语言模型协同的开放世界自主标注框架，推动通用检测系统发展

Abstract: Object detection models typically rely on predefined categories, limiting
their ability to identify novel objects in open-world scenarios. To overcome
this constraint, we introduce ADAM: Autonomous Discovery and Annotation Model,
a training-free, self-refining framework for open-world object labeling. ADAM
leverages large language models (LLMs) to generate candidate labels for unknown
objects based on contextual information from known entities within a scene.
These labels are paired with visual embeddings from CLIP to construct an
Embedding-Label Repository (ELR) that enables inference without category
supervision. For a newly encountered unknown object, ADAM retrieves visually
similar instances from the ELR and applies frequency-based voting and
cross-modal re-ranking to assign a robust label. To further enhance
consistency, we introduce a self-refinement loop that re-evaluates repository
labels using visual cohesion analysis and k-nearest-neighbor-based majority
re-labeling. Experimental results on the COCO and PASCAL datasets demonstrate
that ADAM effectively annotates novel categories using only visual and
contextual signals, without requiring any fine-tuning or retraining.

</details>


### [88] [Rethinking Range-View LiDAR Segmentation in Adverse Weather](https://arxiv.org/abs/2506.08979)
*Longyu Yang,Ping Hu,Lu Zhang,Jun Liu,Yap-Peng Tan,Heng Tao Shen,Xiaofeng Zhu*

Main category: cs.CV

TL;DR: 提出模块化框架提升激光雷达分割在恶劣天气下的泛化性能


<details>
  <summary>Details</summary>
Motivation: 现有范围视图方法在极端天气下可靠性不足，限制实际应用场景

Method: 双分支架构分别处理几何属性和反射强度，包含噪声抑制模块(GAS)和反射校准模块(RDC)

Result: 多基准测试显示显著提升恶劣天气适应性，推理开销仅微量增加

Conclusion: 提供即插即用解决方案，有效增强现有模型的现实环境适用性

Abstract: LiDAR segmentation has emerged as an important task to enrich multimedia
experiences and analysis. Range-view-based methods have gained popularity due
to their high computational efficiency and compatibility with real-time
deployment. However, their generalized performance under adverse weather
conditions remains underexplored, limiting their reliability in real-world
environments. In this work, we identify and analyze the unique challenges that
affect the generalization of range-view LiDAR segmentation in severe weather.
To address these challenges, we propose a modular and lightweight framework
that enhances robustness without altering the core architecture of existing
models. Our method reformulates the initial stem block of standard range-view
networks into two branches to process geometric attributes and reflectance
intensity separately. Specifically, a Geometric Abnormality Suppression (GAS)
module reduces the influence of weather-induced spatial noise, and a
Reflectance Distortion Calibration (RDC) module corrects reflectance
distortions through memory-guided adaptive instance normalization. The
processed features are then fused and passed to the original segmentation
pipeline. Extensive experiments on different benchmarks and baseline models
demonstrate that our approach significantly improves generalization to adverse
weather with minimal inference overhead, offering a practical and effective
solution for real-world LiDAR segmentation.

</details>


### [89] [Efficient Medical Vision-Language Alignment Through Adapting Masked Vision Models](https://arxiv.org/abs/2506.08990)
*Chenyu Lian,Hong-Yu Zhou,Dongyun Liang,Jing Qin,Liansheng Wang*

Main category: cs.CV

TL;DR: 提出高效医学视觉语言对齐方法ALTA，通过适配预训练视觉模型提升跨模态匹配性能


<details>
  <summary>Details</summary>
Motivation: 传统跨模态对比学习方法视觉表示能力受限，而多模态掩码建模模型不擅长直接跨模态匹配

Method: 通过适配掩码建模预训练视觉模型(仅需8%参数)，整合时间多视图输入增强模态一致性

Result: 文本-图像检索准确率提升4%，图像-文本检索提升6%，计算资源消耗减少80%

Conclusion: ALTA实现了高效跨模态对齐，同时促进视觉与语言理解，为医学多模态学习提供新方案

Abstract: Medical vision-language alignment through cross-modal contrastive learning
shows promising performance in image-text matching tasks, such as retrieval and
zero-shot classification. However, conventional cross-modal contrastive
learning (CLIP-based) methods suffer from suboptimal visual representation
capabilities, which also limits their effectiveness in vision-language
alignment. In contrast, although the models pretrained via multimodal masked
modeling struggle with direct cross-modal matching, they excel in visual
representation. To address this contradiction, we propose ALTA (ALign Through
Adapting), an efficient medical vision-language alignment method that utilizes
only about 8% of the trainable parameters and less than 1/5 of the
computational consumption required for masked record modeling. ALTA achieves
superior performance in vision-language matching tasks like retrieval and
zero-shot classification by adapting the pretrained vision model from masked
record modeling. Additionally, we integrate temporal-multiview radiograph
inputs to enhance the information consistency between radiographs and their
corresponding descriptions in reports, further improving the vision-language
alignment. Experimental evaluations show that ALTA outperforms the
best-performing counterpart by over 4% absolute points in text-to-image
accuracy and approximately 6% absolute points in image-to-text retrieval
accuracy. The adaptation of vision-language models during efficient alignment
also promotes better vision and language understanding. Code is publicly
available at https://github.com/DopamineLcy/ALTA.

</details>


### [90] [Do Concept Replacement Techniques Really Erase Unacceptable Concepts?](https://arxiv.org/abs/2506.08991)
*Anudeep Das,Gurjot Singh,Prach Chantasantitam,N. Asokan*

Main category: cs.CV

TL;DR: 现有概念替换技术在I2I场景中效果不足，提出AntiMirror方法兼顾有效性与保真度。


<details>
  <summary>Details</summary>
Motivation: 生成模型需避免输出不可接受内容，但现有概念擦除技术CRTs在I2I场景中失效，且忽视输出保真度问题。

Method: 提出基于针对性图像编辑的技术AntiMirror，确保替换不可接受概念的同时保持输入内容的完整性。

Result: 实验证明AntiMirror在I2I场景中有效解决了CRTs的局限性，并实现了概念替换的高保真度。

Conclusion: 强调概念替换需兼顾效果与保真度，AntiMirror为生成模型内容安全提供了新方向。

Abstract: Generative models, particularly diffusion-based text-to-image (T2I) models,
have demonstrated astounding success. However, aligning them to avoid
generating content with unacceptable concepts (e.g., offensive or copyrighted
content, or celebrity likenesses) remains a significant challenge. Concept
replacement techniques (CRTs) aim to address this challenge, often by trying to
"erase" unacceptable concepts from models. Recently, model providers have
started offering image editing services which accept an image and a text prompt
as input, to produce an image altered as specified by the prompt. These are
known as image-to-image (I2I) models. In this paper, we first use an I2I model
to empirically demonstrate that today's state-of-the-art CRTs do not in fact
erase unacceptable concepts. Existing CRTs are thus likely to be ineffective in
emerging I2I scenarios, despite their proven ability to remove unwanted
concepts in T2I pipelines, highlighting the need to understand this discrepancy
between T2I and I2I settings. Next, we argue that a good CRT, while replacing
unacceptable concepts, should preserve other concepts specified in the inputs
to generative models. We call this fidelity. Prior work on CRTs have neglected
fidelity in the case of unacceptable concepts. Finally, we propose the use of
targeted image-editing techniques to achieve both effectiveness and fidelity.
We present such a technique, AntiMirror, and demonstrate its viability.

</details>


### [91] [SDTagNet: Leveraging Text-Annotated Navigation Maps for Online HD Map Construction](https://arxiv.org/abs/2506.08997)
*Fabian Immel,Jan-Hendrik Pauls,Richard Fehler,Frank Bieder,Jonas Merkert,Christoph Stiller*

Main category: cs.CV

TL;DR: 提出SDTagNet方法，利用标准地图（SD）的文本和语义信息增强在线高精地图构建，提升远距离检测精度。


<details>
  <summary>Details</summary>
Motivation: 在线高精地图构建受限于车载传感器感知范围，维护成本高；现有SD地图先验方法信息利用不足。

Method: 融合SD地图折线数据与文本语义信息，结合NLP特征；提出点级SD地图编码器与正交元素标识符统一整合多类地图元素。

Result: 在Argoverse2和nuScenes上，较无先验方法提升45%（+5.9mAP），较现有SD先验方法提升20%（+3.2mAP）。

Conclusion: SDTagNet通过充分挖掘SD地图信息突破了传感器局限，降低维护成本，推动自动驾驶高精地图规模化应用。

Abstract: Autonomous vehicles rely on detailed and accurate environmental information
to operate safely. High definition (HD) maps offer a promising solution, but
their high maintenance cost poses a significant barrier to scalable deployment.
This challenge is addressed by online HD map construction methods, which
generate local HD maps from live sensor data. However, these methods are
inherently limited by the short perception range of onboard sensors. To
overcome this limitation and improve general performance, recent approaches
have explored the use of standard definition (SD) maps as prior, which are
significantly easier to maintain. We propose SDTagNet, the first online HD map
construction method that fully utilizes the information of widely available SD
maps, like OpenStreetMap, to enhance far range detection accuracy. Our approach
introduces two key innovations. First, in contrast to previous work, we
incorporate not only polyline SD map data with manually selected classes, but
additional semantic information in the form of textual annotations. In this
way, we enrich SD vector map tokens with NLP-derived features, eliminating the
dependency on predefined specifications or exhaustive class taxonomies. Second,
we introduce a point-level SD map encoder together with orthogonal element
identifiers to uniformly integrate all types of map elements. Experiments on
Argoverse 2 and nuScenes show that this boosts map perception performance by up
to +5.9 mAP (+45%) w.r.t. map construction without priors and up to +3.2 mAP
(+20%) w.r.t. previous approaches that already use SD map priors. Code is
available at https://github.com/immel-f/SDTagNet

</details>


### [92] [Do MIL Models Transfer?](https://arxiv.org/abs/2506.09022)
*Daniel Shao,Richard J. Chen,Andrew H. Song,Joel Runevic,Ming Y. Lu,Tong Ding,Faisal Mahmood*

Main category: cs.CV

TL;DR: 多示例学习模型通过迁移学习在计算病理学中展现强大跨器官和跨任务泛化能力


<details>
  <summary>Details</summary>
Motivation: 针对多示例学习在计算病理学中面临的小样本和弱监督挑战，探索其迁移学习潜力以弥补与NLP/CV领域的应用差距

Method: 系统评估11个MIL模型在21个预训练任务中的表现，涵盖形态学和分子亚型预测，采用泛癌数据集进行预训练对比

Result: 预训练模型跨器官表现优异，泛癌预训练模型仅需少量数据即超越基础模型，最高提升15%的预测性能

Conclusion: 验证了MIL模型迁移学习的有效性，提供标准化实现框架促进计算病理学研究，为小样本医疗AI应用提供新思路

Abstract: Multiple Instance Learning (MIL) is a cornerstone approach in computational
pathology (CPath) for generating clinically meaningful slide-level embeddings
from gigapixel tissue images. However, MIL often struggles with small, weakly
supervised clinical datasets. In contrast to fields such as NLP and
conventional computer vision, where transfer learning is widely used to address
data scarcity, the transferability of MIL models remains poorly understood. In
this study, we systematically evaluate the transfer learning capabilities of
pretrained MIL models by assessing 11 models across 21 pretraining tasks for
morphological and molecular subtype prediction. Our results show that
pretrained MIL models, even when trained on different organs than the target
task, consistently outperform models trained from scratch. Moreover,
pretraining on pancancer datasets enables strong generalization across organs
and tasks, outperforming slide foundation models while using substantially less
pretraining data. These findings highlight the robust adaptability of MIL
models and demonstrate the benefits of leveraging transfer learning to boost
performance in CPath. Lastly, we provide a resource which standardizes the
implementation of MIL models and collection of pretrained model weights on
popular CPath tasks, available at https://github.com/mahmoodlab/MIL-Lab

</details>


### [93] [DIsoN: Decentralized Isolation Networks for Out-of-Distribution Detection in Medical Imaging](https://arxiv.org/abs/2506.09024)
*Felix Wagner,Pramit Saha,Harry Anthony,J. Alison Noble,Konstantinos Kamnitsas*

Main category: cs.CV

TL;DR: 提出去中心化隔离网络DIsoN，在保护隐私条件下通过参数交换实现医学影像的分布外检测


<details>
  <summary>Details</summary>
Motivation: 现有OOD检测方法需要共享训练数据或中心化存储，但现实中因隐私和数据规模限制难以实现

Method: 通过隔离网络量化样本分离难度，设计参数交换机制实现去中心化比较，引入类条件约束提升检测精度

Result: 在12个医疗影像检测任务中优于现有方法，保持隐私保护的同时实现有效OOD检测

Conclusion: 开创了去中心化OOD检测新范式，为模型部署提供安全的数据服务支持

Abstract: Safe deployment of machine learning (ML) models in safety-critical domains
such as medical imaging requires detecting inputs with characteristics not seen
during training, known as out-of-distribution (OOD) detection, to prevent
unreliable predictions. Effective OOD detection after deployment could benefit
from access to the training data, enabling direct comparison between test
samples and the training data distribution to identify differences.
State-of-the-art OOD detection methods, however, either discard training data
after deployment or assume that test samples and training data are centrally
stored together, an assumption that rarely holds in real-world settings. This
is because shipping training data with the deployed model is usually impossible
due to the size of training databases, as well as proprietary or privacy
constraints. We introduce the Isolation Network, an OOD detection framework
that quantifies the difficulty of separating a target test sample from the
training data by solving a binary classification task. We then propose
Decentralized Isolation Networks (DIsoN), which enables the comparison of
training and test data when data-sharing is impossible, by exchanging only
model parameters between the remote computational nodes of training and
deployment. We further extend DIsoN with class-conditioning, comparing a target
sample solely with training data of its predicted class. We evaluate DIsoN on
four medical imaging datasets (dermatology, chest X-ray, breast ultrasound,
histopathology) across 12 OOD detection tasks. DIsoN performs favorably against
existing methods while respecting data-privacy. This decentralized OOD
detection framework opens the way for a new type of service that ML developers
could provide along with their models: providing remote, secure utilization of
their training data for OOD detection services. Code will be available upon
acceptance at: *****

</details>


### [94] [Diffuse and Disperse: Image Generation with Representation Regularization](https://arxiv.org/abs/2506.09027)
*Runqian Wang,Kaiming He*

Main category: cs.CV

TL;DR: 提出无需正样本的Dispersive Loss正则化方法，提升扩散模型的隐空间表示分散性


<details>
  <summary>Details</summary>
Motivation: 针对扩散模型缺乏显式正则化且与表示学习脱节的问题，旨在通过隐空间正则化提升模型表现

Method: 基于对比学习思想设计分散损失，无需正样本对/预训练/额外参数，可即插即用于现有模型

Result: 在ImageNet多模型测试中持续优于基线方法，验证了方法的普适有效性

Conclusion: 为生成模型注入表示学习思想，建立了二者间的理论联系，推动领域融合发展

Abstract: The development of diffusion-based generative models over the past decade has
largely proceeded independently of progress in representation learning. These
diffusion models typically rely on regression-based objectives and generally
lack explicit regularization. In this work, we propose \textit{Dispersive
Loss}, a simple plug-and-play regularizer that effectively improves
diffusion-based generative models. Our loss function encourages internal
representations to disperse in the hidden space, analogous to contrastive
self-supervised learning, with the key distinction that it requires no positive
sample pairs and therefore does not interfere with the sampling process used
for regression. Compared to the recent method of representation alignment
(REPA), our approach is self-contained and minimalist, requiring no
pre-training, no additional parameters, and no external data. We evaluate
Dispersive Loss on the ImageNet dataset across a range of models and report
consistent improvements over widely used and strong baselines. We hope our work
will help bridge the gap between generative modeling and representation
learning.

</details>


### [95] [Princeton365: A Diverse Dataset with Accurate Camera Pose](https://arxiv.org/abs/2506.09035)
*Karhan Kayan,Stamatis Alexandropoulos,Rishabh Jain,Yiming Zuo,Erich Liang,Jia Deng*

Main category: cs.CV

TL;DR: 提出Princeton365多场景数据集与光流评估指标，突破现有SLAM基准在精度与多样性上的局限


<details>
  <summary>Details</summary>
Motivation: 现有SLAM基准存在数据多样性不足与精度难以兼顾的问题，需建立覆盖多场景的标准化评估体系

Method: 使用标定板+360相机构建真值采集框架，收集室内/室外/物体扫描多模态数据；提出基于姿态误差光流的场景尺度感知评估指标

Result: 创建365个视频数据集含相机轨迹真值，新指标支持跨场景SLAM性能对比，NVS基准覆盖非朗伯表面/360轨迹等挑战场景

Conclusion: 数据集与评估体系为SLAM算法提供全面测试平台，新指标揭示了传统ATE无法捕捉的算法失效模式，推动领域研究进步

Abstract: We introduce Princeton365, a large-scale diverse dataset of 365 videos with
accurate camera pose. Our dataset bridges the gap between accuracy and data
diversity in current SLAM benchmarks by introducing a novel ground truth
collection framework that leverages calibration boards and a 360-camera. We
collect indoor, outdoor, and object scanning videos with synchronized monocular
and stereo RGB video outputs as well as IMU. We further propose a new scene
scale-aware evaluation metric for SLAM based on the the optical flow induced by
the camera pose estimation error. In contrast to the current metrics, our new
metric allows for comparison between the performance of SLAM methods across
scenes as opposed to existing metrics such as Average Trajectory Error (ATE),
allowing researchers to analyze the failure modes of their methods. We also
propose a challenging Novel View Synthesis benchmark that covers cases not
covered by current NVS benchmarks, such as fully non-Lambertian scenes with
360-degree camera trajectories. Please visit
https://princeton365.cs.princeton.edu for the dataset, code, videos, and
submission.

</details>


### [96] [Autoregressive Semantic Visual Reconstruction Helps VLMs Understand Better](https://arxiv.org/abs/2506.09040)
*Dianyi Wang,Wei Song,Yikun Wang,Siyuan Wang,Kaicheng Yu,Zhongyu Wei,Jiaqi Wang*

Main category: cs.CV

TL;DR: 提出自回归语义视觉重建框架ASVR，通过联合学习视觉语义和文本模态，显著提升多模态理解性能


<details>
  <summary>Details</summary>
Motivation: 传统视觉语言模型仅监督文本序列，导致视觉信息利用不充分，存在图像依赖标注、细节丢失和视觉内容表达受限三个核心问题

Method: 构建统一自回归框架，通过重建图像语义表征而非原始像素，结合离散语义标记实现跨模态联合学习

Result: 在多种数据规模(556k-2M)和LLM架构下稳定提升，LLaVA-1.5在14个多模态基准上平均提升5%

Conclusion: 验证了语义重构对多模态理解的有效性，开创了自回归框架下视觉-语言协同学习新范式

Abstract: Typical large vision-language models (LVLMs) apply autoregressive supervision
solely to textual sequences, without fully incorporating the visual modality
into the learning process. This results in three key limitations: (1) an
inability to utilize images without accompanying captions, (2) the risk that
captions omit critical visual details, and (3) the challenge that certain
vision-centric content cannot be adequately conveyed through text. As a result,
current LVLMs often prioritize vision-to-language alignment while potentially
overlooking fine-grained visual information. While some prior works have
explored autoregressive image generation, effectively leveraging autoregressive
visual supervision to enhance image understanding remains an open challenge. In
this paper, we introduce Autoregressive Semantic Visual Reconstruction (ASVR),
which enables joint learning of visual and textual modalities within a unified
autoregressive framework. We show that autoregressively reconstructing the raw
visual appearance of images does not enhance and may even impair multimodal
understanding. In contrast, autoregressively reconstructing the semantic
representation of images consistently improves comprehension. Notably, we find
that even when models are given continuous image features as input, they can
effectively reconstruct discrete semantic tokens, resulting in stable and
consistent improvements across a wide range of multimodal understanding
benchmarks. Our approach delivers significant performance gains across varying
data scales (556k-2M) and types of LLM bacbones. Specifically, ASVR improves
LLaVA-1.5 by 5% in average scores across 14 multimodal benchmarks. The code is
available at https://github.com/AlenjandroWang/ASVR.

</details>


### [97] [Cosmos-Drive-Dreams: Scalable Synthetic Driving Data Generation with World Foundation Models](https://arxiv.org/abs/2506.09042)
*Xuanchi Ren,Yifan Lu,Tianshi Cao,Ruiyuan Gao,Shengyu Huang,Amirmojtaba Sabour,Tianchang Shen,Tobias Pfaff,Jay Zhangjie Wu,Runjian Chen,Seung Wook Kim,Jun Gao,Laura Leal-Taixe,Mike Chen,Sanja Fidler,Huan Ling*

Main category: cs.CV

TL;DR: 提出基于合成数据生成的Cosmos-Drive-Dreams管道，通过高保真驾驶场景生成解决自动驾驶数据长尾问题。


<details>
  <summary>Details</summary>
Motivation: 采集真实驾驶场景的边角案例数据成本高且困难，这些数据对训练安全关键型AI系统至关重要。

Method: 基于NVIDIA Cosmos构建可控、多视角、时空一致的驾驶视频生成模型(Cosmos-Drive)，通过SDG流程扩展数据集

Result: 生成数据有效缓解长尾分布问题，在3D车道检测、物体检测和驾驶策略学习等任务中提升模型泛化能力

Conclusion: 证明了合成数据对自动驾驶系统开发的价值，开源工具链为研究社区提供支持

Abstract: Collecting and annotating real-world data for safety-critical physical AI
systems, such as Autonomous Vehicle (AV), is time-consuming and costly. It is
especially challenging to capture rare edge cases, which play a critical role
in training and testing of an AV system. To address this challenge, we
introduce the Cosmos-Drive-Dreams - a synthetic data generation (SDG) pipeline
that aims to generate challenging scenarios to facilitate downstream tasks such
as perception and driving policy training. Powering this pipeline is
Cosmos-Drive, a suite of models specialized from NVIDIA Cosmos world foundation
model for the driving domain and are capable of controllable, high-fidelity,
multi-view, and spatiotemporally consistent driving video generation. We
showcase the utility of these models by applying Cosmos-Drive-Dreams to scale
the quantity and diversity of driving datasets with high-fidelity and
challenging scenarios. Experimentally, we demonstrate that our generated data
helps in mitigating long-tail distribution problems and enhances generalization
in downstream tasks such as 3D lane detection, 3D object detection and driving
policy learning. We open source our pipeline toolkit, dataset and model weights
through the NVIDIA's Cosmos platform.
  Project page: https://research.nvidia.com/labs/toronto-ai/cosmos_drive_dreams

</details>


### [98] [MagCache: Fast Video Generation with Magnitude-Aware Cache](https://arxiv.org/abs/2506.09045)
*Zehong Ma,Longhui Wei,Feng Wang,Shiliang Zhang,Qi Tian*

Main category: cs.CV

TL;DR: 提出基于幅度规律的自适应缓存MagCache，实现视频扩散模型高效加速并保持视觉质量


<details>
  <summary>Details</summary>
Motivation: 现有加速方法依赖启发式规则，需大量校准且存在提示词过拟合导致的输出不一致问题

Method: 通过发现残差输出幅度比单调下降规律，设计误差建模机制和自适应缓存策略自动跳过非关键时间步

Result: 在Open-Sora和Wan 2.1分别实现2.1x和2.68x加速，LPIPS/SSIM/PSNR指标显著优于同类方法

Conclusion: MagCache仅需单样本校准即实现高效加速，为视频生成模型优化提供可靠解决方案

Abstract: Existing acceleration techniques for video diffusion models often rely on
uniform heuristics or time-embedding variants to skip timesteps and reuse
cached features. These approaches typically require extensive calibration with
curated prompts and risk inconsistent outputs due to prompt-specific
overfitting. In this paper, we introduce a novel and robust discovery: a
unified magnitude law observed across different models and prompts.
Specifically, the magnitude ratio of successive residual outputs decreases
monotonically and steadily in most timesteps while rapidly in the last several
steps. Leveraging this insight, we introduce a Magnitude-aware Cache (MagCache)
that adaptively skips unimportant timesteps using an error modeling mechanism
and adaptive caching strategy. Unlike existing methods requiring dozens of
curated samples for calibration, MagCache only requires a single sample for
calibration. Experimental results show that MagCache achieves 2.1x and 2.68x
speedups on Open-Sora and Wan 2.1, respectively, while preserving superior
visual fidelity. It significantly outperforms existing methods in LPIPS, SSIM,
and PSNR, under comparable computational budgets.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [99] [Re-Thinking the Automatic Evaluation of Image-Text Alignment in Text-to-Image Models](https://arxiv.org/abs/2506.08480)
*Huixuan Zhang,Xiaojun Wan*

Main category: cs.CL

TL;DR: 研究揭示现行文本-图像对齐评估框架的局限性，并提出改进建议


<details>
  <summary>Details</summary>
Motivation: 现有评估方法过度依赖人类判断，忽视可信评估框架需具备的其他关键特性

Method: 通过系统分析确定可靠评估的关键属性，实证检验主流框架缺陷，并提出改进方案

Result: 实证表明现有评估框架在多样化指标和模型上无法全面满足可信评估要求

Conclusion: 提出了完善图像-文本对齐评估的指导性原则，推动评估体系向更可信方向发展

Abstract: Text-to-image models often struggle to generate images that precisely match
textual prompts. Prior research has extensively studied the evaluation of
image-text alignment in text-to-image generation. However, existing evaluations
primarily focus on agreement with human assessments, neglecting other critical
properties of a trustworthy evaluation framework. In this work, we first
identify two key aspects that a reliable evaluation should address. We then
empirically demonstrate that current mainstream evaluation frameworks fail to
fully satisfy these properties across a diverse range of metrics and models.
Finally, we propose recommendations for improving image-text alignment
evaluation.

</details>


### [100] [ClimateViz: A Benchmark for Statistical Reasoning and Fact Verification on Scientific Charts](https://arxiv.org/abs/2506.08700)
*Ruiran Su,Jiasheng Si,Zhijiang Guo,Janet B. Pierrehumbert*

Main category: cs.CL

TL;DR: 提出首个科学图表事实验证基准ClimateViz，揭示主流多模态模型在图表推理上的不足


<details>
  <summary>Details</summary>
Motivation: 现有科学事实核查主要关注文本表格，忽视科学图表作为定量证据的核心载体

Method: 构建含49,862个声明和2,896图表的数据集，引入知识图谱解释，评估多种多模态模型的零样本/少样本表现

Result: 最佳模型准确率仅76.2-77.8%，远低于人类水平（89.3-92.7%），解释增强输出对部分模型有效

Conclusion: 建立首个科学图表验证基准，揭示当前模型图表推理缺陷，推动可解释科学验证研究，开源数据集促进社区发展

Abstract: Scientific fact-checking has mostly focused on text and tables, overlooking
scientific charts, which are key for presenting quantitative evidence and
statistical reasoning. We introduce ClimateViz, the first large-scale benchmark
for scientific fact-checking using expert-curated scientific charts. ClimateViz
contains 49,862 claims linked to 2,896 visualizations, each labeled as support,
refute, or not enough information. To improve interpretability, each example
includes structured knowledge graph explanations covering trends, comparisons,
and causal relations. We evaluate state-of-the-art multimodal language models,
including both proprietary and open-source systems, in zero-shot and few-shot
settings. Results show that current models struggle with chart-based reasoning:
even the best systems, such as Gemini 2.5 and InternVL 2.5, reach only 76.2 to
77.8 percent accuracy in label-only settings, far below human performance (89.3
and 92.7 percent). Explanation-augmented outputs improve performance in some
models. We released our dataset and code alongside the paper.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [101] [FEDTAIL: Federated Long-Tailed Domain Generalization with Sharpness-Guided Gradient Matching](https://arxiv.org/abs/2506.08518)
*Sunny Gupta,Nikita Jangid,Shounak Das,Amit Sethi*

Main category: cs.AI

TL;DR: 提出联邦域泛化框架FedTAIL，通过锐度感知梯度对齐优化解决长尾分布和优化冲突问题


<details>
  <summary>Details</summary>
Motivation: 现有域泛化方法在长尾类别分布和优化目标冲突场景下表现不佳，需提升模型在域偏移下的鲁棒性

Method: 融合梯度一致性正则化缓解目标冲突，使用类感知锐度最小化和曲率动态加权平衡类别，整合锐度感知扰动增强条件分布对齐

Result: 在标准基准测试中达到SOTA，在域偏移和标签不平衡场景下表现优异，验证了集中式和联邦场景的有效性

Conclusion: 通过优化协调、类正则化和条件对齐的整体框架，显著提升模型在异构数据下的泛化能力和鲁棒性，推动联邦域泛化发展

Abstract: Domain Generalization (DG) seeks to train models that perform reliably on
unseen target domains without access to target data during training. While
recent progress in smoothing the loss landscape has improved generalization,
existing methods often falter under long-tailed class distributions and
conflicting optimization objectives. We introduce FedTAIL, a federated domain
generalization framework that explicitly addresses these challenges through
sharpness-guided, gradient-aligned optimization. Our method incorporates a
gradient coherence regularizer to mitigate conflicts between classification and
adversarial objectives, leading to more stable convergence. To combat class
imbalance, we perform class-wise sharpness minimization and propose a
curvature-aware dynamic weighting scheme that adaptively emphasizes
underrepresented tail classes. Furthermore, we enhance conditional distribution
alignment by integrating sharpness-aware perturbations into entropy
regularization, improving robustness under domain shift. FedTAIL unifies
optimization harmonization, class-aware regularization, and conditional
alignment into a scalable, federated-compatible framework. Extensive
evaluations across standard domain generalization benchmarks demonstrate that
FedTAIL achieves state-of-the-art performance, particularly in the presence of
domain shifts and label imbalance, validating its effectiveness in both
centralized and federated settings. Code: https://github.com/sunnyinAI/FedTail

</details>


### [102] [VIKI-R: Coordinating Embodied Multi-Agent Cooperation via Reinforcement Learning](https://arxiv.org/abs/2506.09049)
*Li Kang,Xiufeng Song,Heng Zhou,Yiran Qin,Jie Yang,Xiaohong Liu,Philip Torr,Lei Bai,Zhenfei Yin*

Main category: cs.AI

TL;DR: 提出首个具身多智能体协作分层基准VIKI-Bench和两阶段训练框架VIKI-R，显著提升视觉驱动协作能力。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型在多智能体协作中存在具身类型支持不足的问题，需开发新基准和方法提升视觉驱动协作。

Method: 构建包含三层次结构的VIKI-Bench基准，提出VIKI-R框架（思维链微调+多级强化学习），支持异构智能体协作。

Result: VIKI-R全任务层级超越基线，强化学习促使异构智能体涌现组合协作模式，技术指标显著提升。

Conclusion: 为具身AI系统提供了统一的多智能体视觉协作测试平台与方法，推动人机协作技术发展。

Abstract: Coordinating multiple embodied agents in dynamic environments remains a core
challenge in artificial intelligence, requiring both perception-driven
reasoning and scalable cooperation strategies. While recent works have
leveraged large language models (LLMs) for multi-agent planning, a few have
begun to explore vision-language models (VLMs) for visual reasoning. However,
these VLM-based approaches remain limited in their support for diverse
embodiment types. In this work, we introduce VIKI-Bench, the first hierarchical
benchmark tailored for embodied multi-agent cooperation, featuring three
structured levels: agent activation, task planning, and trajectory perception.
VIKI-Bench includes diverse robot embodiments, multi-view visual observations,
and structured supervision signals to evaluate reasoning grounded in visual
inputs. To demonstrate the utility of VIKI-Bench, we propose VIKI-R, a
two-stage framework that fine-tunes a pretrained vision-language model (VLM)
using Chain-of-Thought annotated demonstrations, followed by reinforcement
learning under multi-level reward signals. Our extensive experiments show that
VIKI-R significantly outperforms baselines method across all task levels.
Furthermore, we show that reinforcement learning enables the emergence of
compositional cooperation patterns among heterogeneous agents. Together,
VIKI-Bench and VIKI-R offer a unified testbed and method for advancing
multi-agent, visual-driven cooperation in embodied AI systems.

</details>


<div id='q-bio.BM'></div>

# q-bio.BM [[Back]](#toc)

### [103] [Aligning Proteins and Language: A Foundation Model for Protein Retrieval](https://arxiv.org/abs/2506.08023)
*Qifeng Wu,Zhengzhe Liu,Han Zhu,Yizhou Zhao,Daisuke Kihara,Min Xu*

Main category: q-bio.BM

TL;DR: 提出CLIP风格多模态模型，通过对比学习实现蛋白质3D结构与功能注释的跨模态检索


<details>
  <summary>Details</summary>
Motivation: 利用视觉语言模型的最新进展解决冷冻电镜等结构生物学方法产生的蛋白质结构功能注释问题

Method: 构建20万蛋白质-文本对数据集，开发基于对比学习的3D结构-文本对齐框架

Result: 在PDB和EMDB数据集上实现优异的零样本跨库检索性能

Conclusion: 证明了多模态基础模型在蛋白质结构功能理解中的应用潜力，推动结构生物学研究

Abstract: This paper aims to retrieve proteins with similar structures and semantics
from large-scale protein dataset, facilitating the functional interpretation of
protein structures derived by structural determination methods like
cryo-Electron Microscopy (cryo-EM). Motivated by the recent progress of
vision-language models (VLMs), we propose a CLIP-style framework for aligning
3D protein structures with functional annotations using contrastive learning.
For model training, we propose a large-scale dataset of approximately 200,000
protein-caption pairs with rich functional descriptors. We evaluate our model
in both in-domain and more challenging cross-database retrieval on Protein Data
Bank (PDB) and Electron Microscopy Data Bank (EMDB) dataset, respectively. In
both cases, our approach demonstrates promising zero-shot retrieval
performance, highlighting the potential of multimodal foundation models for
structure-function understanding in protein biology.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [104] [Surgeons Awareness, Expectations, and Involvement with Artificial Intelligence: a Survey Pre and Post the GPT Era](https://arxiv.org/abs/2506.08258)
*Lorenzo Arboit,Dennis N. Schneider,Toby Collins,Daniel A. Hashimoto,Silvana Perretta,Bernard Dallemagne,Jacques Marescaux,EAES Working Group,Nicolas Padoy,Pietro Mascagni*

Main category: cs.CY

TL;DR: 外科医生对AI的认知和接受度显著提升，但知识差距和基础设施挑战持续存在


<details>
  <summary>Details</summary>
Motivation: 探究生成式AI（如ChatGPT）和外科数据科学发展对外科医生AI认知的影响演变

Method: 2021年和2024年两次全球横断面调查，比较AI认知度、期望值、参与程度及伦理观念变化

Result: AI课程知晓率提升3倍（14.5%→44.6%），伦理关注度显著增强（87.2%强调问责制），79.9%相信AI对手术有积极影响，96.6%愿意临床整合

Conclusion: 需通过教育体系、伦理框架和基础设施升级来弥合知识鸿沟，实现AI在手术领域的有效整合

Abstract: Artificial Intelligence (AI) is transforming medicine, with generative AI
models like ChatGPT reshaping perceptions of its potential. This study examines
surgeons' awareness, expectations, and involvement with AI in surgery through
comparative surveys conducted in 2021 and 2024. Two cross-sectional surveys
were distributed globally in 2021 and 2024, the first before an IRCAD webinar
and the second during the annual EAES meeting. The surveys assessed
demographics, AI awareness, expectations, involvement, and ethics (2024 only).
The surveys collected a total of 671 responses from 98 countries, 522 in 2021
and 149 in 2024. Awareness of AI courses rose from 14.5% in 2021 to 44.6% in
2024, while course attendance increased from 12.9% to 23%. Despite this,
familiarity with foundational AI concepts remained limited. Expectations for
AI's role shifted in 2024, with hospital management gaining relevance. Ethical
concerns gained prominence, with 87.2% of 2024 participants emphasizing
accountability and transparency. Infrastructure limitations remained the
primary obstacle to implementation. Interdisciplinary collaboration and
structured training were identified as critical for successful AI adoption.
Optimism about AI's transformative potential remained high, with 79.9% of
respondents believing AI would positively impact surgery and 96.6% willing to
integrate AI into their clinical practice. Surgeons' perceptions of AI are
evolving, driven by the rise of generative AI and advancements in surgical data
science. While enthusiasm for integration is strong, knowledge gaps and
infrastructural challenges persist. Addressing these through education, ethical
frameworks, and infrastructure development is essential.

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [105] [Normalized Radon Cumulative Distribution Transforms for Invariance and Robustness in Optimal Transport Based Image Classification](https://arxiv.org/abs/2506.08761)
*Matthias Beckmann,Robert Beinert,Jonas Bresch*

Main category: math.NA

TL;DR: 提出两种归一化R-CDT变换增强图像分类对抗非仿射形变及脉冲噪声的鲁棒性


<details>
  <summary>Details</summary>
Motivation: 现有R-CDT虽能处理仿射变换，但实际应用中存在非仿射形变和噪声干扰问题

Method: 开发max-normalized R-CDT（基于Wasserstein-∞距离）和mean-normalized R-CDT（基于Wasserstein-2距离）

Result: 理论证明和实验验证显示新方法对局部形变和脉冲噪声具有鲁棒性

Conclusion: 归一化R-CDT扩展了图像特征提取器的适用场景，为复杂形变环境提供有效解决方案

Abstract: The Radon cumulative distribution transform (R-CDT), is an easy-to-compute
feature extractor that facilitates image classification tasks especially in the
small data regime. It is closely related to the sliced Wasserstein distance and
provably guaranties the linear separability of image classes that emerge from
translations or scalings. In many real-world applications, like the recognition
of watermarks in filigranology, however, the data is subject to general affine
transformations originating from the measurement process. To overcome this
issue, we recently introduced the so-called max-normalized R-CDT that only
requires elementary operations and guaranties the separability under arbitrary
affine transformations. The aim of this paper is to continue our study of the
max-normalized R-CDT especially with respect to its robustness against
non-affine image deformations. Our sensitivity analysis shows that its
separability properties are stable provided the Wasserstein-infinity distance
between the samples can be controlled. Since the Wasserstein-infinity distance
only allows small local image deformations, we moreover introduce a
mean-normalized version of the R-CDT. In this case, robustness relates to the
Wasserstein-2 distance and also covers image deformations caused by impulsive
noise for instance. Our theoretical results are supported by numerical
experiments showing the effectiveness of our novel feature extractors as well
as their robustness against local non-affine deformations and impulsive noise.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [106] [SakugaFlow: A Stagewise Illustration Framework Emulating the Human Drawing Process and Providing Interactive Tutoring for Novice Drawing Skills](https://arxiv.org/abs/2506.08443)
*Kazuki Kawamura,Jun Rekimoto*

Main category: cs.HC

TL;DR: 提出结合扩散生成与教学对话的四阶段流程SakugaFlow，支持非线性创作与美术技能学习


<details>
  <summary>Details</summary>
Motivation: 现有AI绘图工具缺乏人类艺术家的分步创作过程揭示，难以支持技能学习

Method: 四阶段流程融合扩散模型与语言模型，通过实时反馈、非线性修改和分支版本实现创作引导

Result: 中间输出可视化与教学对话机制将黑盒生成器转化为支持创意探索与技能习得的学习环境

Conclusion: 通过结构化流程与教学互动设计，为AI创作工具赋予教育价值，促进艺术创作与学习融合

Abstract: While current AI illustration tools can generate high-quality images from
text prompts, they rarely reveal the step-by-step procedure that human artists
follow. We present SakugaFlow, a four-stage pipeline that pairs diffusion-based
image generation with a large-language-model tutor. At each stage, novices
receive real-time feedback on anatomy, perspective, and composition, revise any
step non-linearly, and branch alternative versions. By exposing intermediate
outputs and embedding pedagogical dialogue, SakugaFlow turns a black-box
generator into a scaffolded learning environment that supports both creative
exploration and skills acquisition.

</details>


### [107] [MOSAIC-F: A Framework for Enhancing Students' Oral Presentation Skills through Personalized Feedback](https://arxiv.org/abs/2506.08634)
*Alvaro Becerra,Daniel Andres,Pablo Villegas,Roberto Daza,Ruth Cobos*

Main category: cs.HC

TL;DR: 提出多模态反馈框架MOSAIC-F，结合人工与数据评估生成个性化学习反馈


<details>
  <summary>Details</summary>
Motivation: 为提高学生学习反馈的准确性与个性化程度，特别是在提升口头表达能力场景中

Method: 四步框架：标准化人工评估→多模态数据收集→AI整合生成反馈→视频自评与可视化对比

Result: 通过结合人类评估与生理信号、行为数据等多模态分析，实现更精准可操作的反馈机制

Conclusion: 人机协同评估模式为教育反馈提供新范式，融合主观评价与客观数据分析增强教学效果

Abstract: In this article, we present a novel multimodal feedback framework called
MOSAIC-F, an acronym for a data-driven Framework that integrates Multimodal
Learning Analytics (MMLA), Observations, Sensors, Artificial Intelligence (AI),
and Collaborative assessments for generating personalized feedback on student
learning activities. This framework consists of four key steps. First, peers
and professors' assessments are conducted through standardized rubrics (that
include both quantitative and qualitative evaluations). Second, multimodal data
are collected during learning activities, including video recordings, audio
capture, gaze tracking, physiological signals (heart rate, motion data), and
behavioral interactions. Third, personalized feedback is generated using AI,
synthesizing human-based evaluations and data-based multimodal insights such as
posture, speech patterns, stress levels, and cognitive load, among others.
Finally, students review their own performance through video recordings and
engage in self-assessment and feedback visualization, comparing their own
evaluations with peers and professors' assessments, class averages, and
AI-generated recommendations. By combining human-based and data-based
evaluation techniques, this framework enables more accurate, personalized and
actionable feedback. We tested MOSAIC-F in the context of improving oral
presentation skills.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [108] [Gridding Forced Displacement using Semi-Supervised Learning](https://arxiv.org/abs/2506.08019)
*Andrew Wells,Geraldine Henningsen,Brice Bolane Tchinde Kengne*

Main category: cs.LG

TL;DR: 提出半监督方法将非洲难民数据网格化定位，精度达92.9%


<details>
  <summary>Details</summary>
Motivation: 传统区域级难民统计掩盖局部迁移模式，需更细粒度分析

Method: 结合UNHCR登记数据、卫星建筑足迹和开放地图坐标，采用标签传播算法

Result: 成功定位超千万难民数据，首次揭示被遮蔽的微观迁移格局

Conclusion: 高分辨率难民数据集为研究流离失所驱动因素提供新方法支撑

Abstract: We present a semi-supervised approach that disaggregates refugee statistics
from administrative boundaries to 0.5-degree grid cells across 25 Sub-Saharan
African countries. By integrating UNHCR's ProGres registration data with
satellite-derived building footprints from Google Open Buildings and location
coordinates from OpenStreetMap Populated Places, our label spreading algorithm
creates spatially explicit refugee statistics at high granularity.This
methodology achieves 92.9% average accuracy in placing over 10 million refugee
observations into appropriate grid cells, enabling the identification of
localized displacement patterns previously obscured in broader regional and
national statistics. The resulting high-resolution dataset provides a
foundation for a deeper understanding of displacement drivers.

</details>


### [109] [Bi-level Unbalanced Optimal Transport for Partial Domain Adaptation](https://arxiv.org/abs/2506.08020)
*Zi-Ying Chen,Chuan-Xian Ren,Hong Yan*

Main category: cs.LG

TL;DR: 提出双层次非平衡最优传输模型BUOT，在统一框架中同时处理样本级和类级关系，提升部分领域自适应效果


<details>
  <summary>Details</summary>
Motivation: 现有加权框架仅通过样本权重调整标签分布，易受预测误差干扰且忽略聚类结构特征

Method: 建立样本级与类级传输的协作机制，通过双层次最优传输联合建模，引入标签感知传输成本保证局部结构

Result: 在基准数据集上验证了模型的有效性，展现出竞争力

Conclusion: 通过层次化传输框架实现了对异常类的稳健识别，提升了迁移效率与精度

Abstract: Partial domain adaptation (PDA) problem requires aligning cross-domain
samples while distinguishing the outlier classes for accurate knowledge
transfer. The widely used weighting framework tries to address the outlier
classes by introducing the reweighed source domain with a similar label
distribution to the target domain. However, the empirical modeling of weights
can only characterize the sample-wise relations, which leads to insufficient
exploration of cluster structures, and the weights could be sensitive to the
inaccurate prediction and cause confusion on the outlier classes. To tackle
these issues, we propose a Bi-level Unbalanced Optimal Transport (BUOT) model
to simultaneously characterize the sample-wise and class-wise relations in a
unified transport framework. Specifically, a cooperation mechanism between
sample-level and class-level transport is introduced, where the sample-level
transport provides essential structure information for the class-level
knowledge transfer, while the class-level transport supplies discriminative
information for the outlier identification. The bi-level transport plan
provides guidance for the alignment process. By incorporating the label-aware
transport cost, the local transport structure is ensured and a fast computation
formulation is derived to improve the efficiency. Extensive experiments on
benchmark datasets validate the competitiveness of BUOT.

</details>


### [110] [Modality-Balancing Preference Optimization of Large Multimodal Models by Adversarial Negative Mining](https://arxiv.org/abs/2506.08022)
*Chenxi Liu,Tianyi Xiong,Ruibo Chen,Yihan Wu,Junfeng Guo,Tianyi Zhou,Heng Huang*

Main category: cs.LG

TL;DR: 提出模态平衡偏好优化框架MBPO，通过生成对抗性困难负样本和在线验证奖励解决多模态模型的模态失衡问题


<details>
  <summary>Details</summary>
Motivation: 现有大型多模态模型存在语言先验偏差压倒视觉输入的模态失衡问题，导致泛化能力受限和幻觉现象

Method: 结合对抗图像扰动生成困难负样本，利用封闭任务验证奖励生成在线响应，使用GRPO进行离线-在线混合数据训练

Result: 实验表明MBPO在视觉语言任务中提升性能并有效减少幻觉

Conclusion: MBPO为解决多模态模型模态失衡提供了新方向，增强了模型对下游任务的适应能力

Abstract: The task adaptation and alignment of Large Multimodal Models (LMMs) have been
significantly advanced by instruction tuning and further strengthened by recent
preference optimization. Yet, most LMMs still suffer from severe modality
imbalance during reasoning, i.e., outweighing language prior biases over visual
inputs, which bottlenecks their generalization to downstream tasks and causes
hallucinations. However, existing preference optimization approaches for LMMs
do not focus on restraining the internal biases of their Large Language Model
(LLM) backbones when curating the training data. Moreover, they heavily rely on
offline data and lack the capacity to explore diverse responses adaptive to
dynamic distributional shifts during training. Meanwhile, Group Relative Policy
Optimization (GRPO), a recent method using online-generated data and verified
rewards to improve reasoning capabilities, remains largely underexplored in LMM
alignment. In this paper, we propose a novel preference learning framework,
Modality-Balancing Preference Optimization (MBPO), to address the modality
imbalance in LMMs. MBPO constructs a more effective offline preference dataset
by generating hard negatives, i.e., rejected responses misled by LLM biases due
to limited usage of visual information, through adversarial perturbation of
input images. Moreover, MBPO leverages the easy-to-verify nature of close-ended
tasks to generate online responses with verified rewards. GRPO is then employed
to train the model with offline-online hybrid data. Extensive experiments
demonstrate that MBPO can enhance LMM performance on challenging
vision-language tasks and effectively reduce hallucinations.

</details>


### [111] [UniVarFL: Uniformity and Variance Regularized Federated Learning for Heterogeneous Data](https://arxiv.org/abs/2506.08167)
*Sunny Gupta,Nikita Jangid,Amit Sethi*

Main category: cs.LG

TL;DR: 提出UniVarFL框架通过双重正则化策略解决联邦学习中非IID数据导致的分类器偏差问题


<details>
  <summary>Details</summary>
Motivation: 针对联邦学习在非IID数据下因本地分类器偏差导致性能下降的问题，传统方法存在高计算成本或适应力不足的缺陷

Method: 采用分类器方差正则化对齐IID预期的类概率分布，结合超球面均匀正则化优化特征表示分布

Result: 在多个基准测试中准确率超越现有方法，具有高可扩展性和计算效率

Conclusion: 为资源受限场景提供了有效的联邦学习解决方案，通过模拟IID训练机制显著提升模型泛化能力

Abstract: Federated Learning (FL) often suffers from severe performance degradation
when faced with non-IID data, largely due to local classifier bias. Traditional
remedies such as global model regularization or layer freezing either incur
high computational costs or struggle to adapt to feature shifts. In this work,
we propose UniVarFL, a novel FL framework that emulates IID-like training
dynamics directly at the client level, eliminating the need for global model
dependency. UniVarFL leverages two complementary regularization strategies
during local training: Classifier Variance Regularization, which aligns
class-wise probability distributions with those expected under IID conditions,
effectively mitigating local classifier bias; and Hyperspherical Uniformity
Regularization, which encourages a uniform distribution of feature
representations across the hypersphere, thereby enhancing the model's ability
to generalize under diverse data distributions. Extensive experiments on
multiple benchmark datasets demonstrate that UniVarFL outperforms existing
methods in accuracy, highlighting its potential as a highly scalable and
efficient solution for real-world FL deployments, especially in
resource-constrained settings. Code: https://github.com/sunnyinAI/UniVarFL

</details>


### [112] [An Adaptive Method Stabilizing Activations for Enhanced Generalization](https://arxiv.org/abs/2506.08353)
*Hyunseok Seung,Jaewoo Lee,Hyunsuk Ko*

Main category: cs.LG

TL;DR: 提出基于激活方差调整学习率的AdaAct优化算法，平衡Adam的收敛速度与SGD的泛化能力


<details>
  <summary>Details</summary>
Motivation: 解决传统优化器在收敛速度和泛化能力之间的权衡困境，传统激活正则化方法存在局限性

Method: 通过神经元层面的自适应机制动态调整学习率，利用激活方差稳定神经元输出分布

Result: 在CIFAR和ImageNet基准测试中表现优异，缩短Adam与SGD性能差距，保持训练效率

Conclusion: 通过激活动态调节机制开创了优化算法新范式，为平衡收敛速度与泛化能力提供有效解决方案

Abstract: We introduce AdaAct, a novel optimization algorithm that adjusts learning
rates according to activation variance. Our method enhances the stability of
neuron outputs by incorporating neuron-wise adaptivity during the training
process, which subsequently leads to better generalization -- a complementary
approach to conventional activation regularization methods. Experimental
results demonstrate AdaAct's competitive performance across standard image
classification benchmarks. We evaluate AdaAct on CIFAR and ImageNet, comparing
it with other state-of-the-art methods. Importantly, AdaAct effectively bridges
the gap between the convergence speed of Adam and the strong generalization
capabilities of SGD, all while maintaining competitive execution times. Code is
available at https://github.com/hseung88/adaact.

</details>


### [113] [Boosting Gradient Leakage Attacks: Data Reconstruction in Realistic FL Settings](https://arxiv.org/abs/2506.08435)
*Mingyuan Fan,Fuyi Wang,Cen Chen,Jianying Zhou*

Main category: cs.LG

TL;DR: 提出FedLeak系统破解联邦学习隐私防护，证实现实场景中仍存在重大数据泄露风险


<details>
  <summary>Details</summary>
Motivation: 针对现有研究低估联邦学习实际应用中的隐私风险，通过实证研究揭示潜在漏洞

Method: 开发FedLeak系统，创新引入部分梯度匹配和梯度正则化技术解决传统梯度泄漏攻击的性能瓶颈

Result: 在符合现实FL环境设定的评估协议下成功实现高保真数据重建，攻击准确率显著提升

Conclusion: 揭示了当前联邦学习系统的根本性安全漏洞，为开发更有效的防御机制提供了紧迫性依据

Abstract: Federated learning (FL) enables collaborative model training among multiple
clients without the need to expose raw data. Its ability to safeguard privacy,
at the heart of FL, has recently been a hot-button debate topic. To elaborate,
several studies have introduced a type of attacks known as gradient leakage
attacks (GLAs), which exploit the gradients shared during training to
reconstruct clients' raw data. On the flip side, some literature, however,
contends no substantial privacy risk in practical FL environments due to the
effectiveness of such GLAs being limited to overly relaxed conditions, such as
small batch sizes and knowledge of clients' data distributions.
  This paper bridges this critical gap by empirically demonstrating that
clients' data can still be effectively reconstructed, even within realistic FL
environments. Upon revisiting GLAs, we recognize that their performance
failures stem from their inability to handle the gradient matching problem. To
alleviate the performance bottlenecks identified above, we develop FedLeak,
which introduces two novel techniques, partial gradient matching and gradient
regularization. Moreover, to evaluate the performance of FedLeak in real-world
FL environments, we formulate a practical evaluation protocol grounded in a
thorough review of extensive FL literature and industry practices. Under this
protocol, FedLeak can still achieve high-fidelity data reconstruction, thereby
underscoring the significant vulnerability in FL systems and the urgent need
for more effective defense methods.

</details>


### [114] [HSG-12M: A Large-Scale Spatial Multigraph Dataset](https://arxiv.org/abs/2506.08618)
*Xianquan Yan,Hakan Akg眉n,Kenji Kawaguchi,N. Duane Loh,Ching Hua Lee*

Main category: cs.LG

TL;DR: 提出首个大规模空间多图数据集HSG-12M，推动几何感知图学习发展


<details>
  <summary>Details</summary>
Motivation: 传统图基准无法处理物理系统中的多路径几何特征，需要建立能保留空间多边关系的表征体系

Method: 基于177TB光谱势数据构建包含1160万静态/510万动态哈密顿谱图的多图数据集，开发Poly2Graph开源转换工具

Result: 发现GNN在大规模多边几何学习中存在新挑战，验证谱图可作为数学对象的拓扑指纹

Conclusion: 建立了代数与图结构的新联系，为凝聚态物理等领域的科学发现提供几何感知的数据基础

Abstract: Existing graph benchmarks assume non-spatial, simple edges, collapsing
physically distinct paths into a single link. We introduce HSG-12M, the first
large-scale dataset of $\textbf{spatial multigraphs}-$graphs embedded in a
metric space where multiple geometrically distinct trajectories between two
nodes are retained as separate edges. HSG-12M contains 11.6 million static and
5.1 million dynamic $\textit{Hamiltonian spectral graphs}$ across 1401
characteristic-polynomial classes, derived from 177 TB of spectral potential
data. Each graph encodes the full geometry of a 1-D crystal's energy spectrum
on the complex plane, producing diverse, physics-grounded topologies that
transcend conventional node-coordinate datasets. To enable future extensions,
we release $\texttt{Poly2Graph}$: a high-performance, open-source pipeline that
maps arbitrary 1-D crystal Hamiltonians to spectral graphs. Benchmarks with
popular GNNs expose new challenges in learning from multi-edge geometry at
scale. Beyond its practical utility, we show that spectral graphs serve as
universal topological fingerprints of polynomials, vectors, and matrices,
forging a new algebra-to-graph link. HSG-12M lays the groundwork for
geometry-aware graph learning and new opportunities of data-driven scientific
discovery in condensed matter physics and beyond.

</details>


### [115] [Time Series Representations for Classification Lie Hidden in Pretrained Vision Transformers](https://arxiv.org/abs/2506.08641)
*Simon Roschmann,Quentin Bouniot,Vasilii Feofanov,Ievgen Redko,Zeynep Akata*

Main category: cs.LG

TL;DR: 提出通过时间序列图像化方法TiViT，利用预训练视觉Transformer实现高效时间序列分类


<details>
  <summary>Details</summary>
Motivation: 时间序列基础模型发展受限于公开数据集匮乏，尝试复用大规模图像预训练模型的表征能力

Method: 将时间序列转化为图像，引入冻结权重的OpenCLIP视觉Transformer进行特征提取

Result: 在标准基准达到SOTA，发现中间层高维度特征最有效，与TSFM表征空间具有互补性

Conclusion: 证明了视觉表征在非视觉领域的复用潜力，开辟了跨模态迁移学习新方向

Abstract: Time series classification is a fundamental task in healthcare and industry,
yet the development of time series foundation models (TSFMs) remains limited by
the scarcity of publicly available time series datasets. In this work, we
propose Time Vision Transformer (TiViT), a framework that converts time series
into images to leverage the representational power of frozen Vision
Transformers (ViTs) pretrained on large-scale image datasets. First, we
theoretically motivate our approach by analyzing the 2D patching of ViTs for
time series, showing that it can increase the number of label-relevant tokens
and reduce the sample complexity. Second, we empirically demonstrate that TiViT
achieves state-of-the-art performance on standard time series classification
benchmarks by utilizing the hidden representations of large OpenCLIP models. We
explore the structure of TiViT representations and find that intermediate
layers with high intrinsic dimension are the most effective for time series
classification. Finally, we assess the alignment between TiViT and TSFM
representation spaces and identify a strong complementarity, with further
performance gains achieved by combining their features. Our findings reveal yet
another direction for reusing vision representations in a non-visual domain.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [116] [Instruction-Tuned Video-Audio Models Elucidate Functional Specialization in the Brain](https://arxiv.org/abs/2506.08277)
*Subba Reddy Oota,Khushbu Pahwa,Prachi Jindal,Satya Sai Srinath Namburi,Maneesh Singh,Tanmoy Chakraborty,Bapi S. Raju,Manish Gupta*

Main category: q-bio.NC

TL;DR: 指令调优的多模态大语言模型在自然电影刺激下展现出显著优于非指令模型的大脑神经活动预测能力


<details>
  <summary>Details</summary>
Motivation: 填补现有研究空白，验证指令调优MLLMs在多模态刺激下与大脑活动的对齐效果

Method: 使用6个视频和2个音频指令调优MLLM，通过13种视频任务指令对比非指令多模态/单模态模型

Result: 视频MLLMs预测性能超越非指令模型15-20%，模型层次与大脑皮层呈现分层对齐特性（早期层-感觉区，中后期层-高级认知区）

Conclusion: 任务指令显著增强脑机对齐效果，为双向系统联合信息处理映射开辟新途径，代码已开源

Abstract: Recent voxel-wise multimodal brain encoding studies have shown that
multimodal large language models (MLLMs) exhibit a higher degree of brain
alignment compared to unimodal models in both unimodal and multimodal stimulus
settings. More recently, instruction-tuned multimodal models have shown to
generate task-specific representations that align strongly with brain activity.
However, prior work evaluating the brain alignment of MLLMs has primarily
focused on unimodal settings or relied on non-instruction-tuned multimodal
models for multimodal stimuli. To address this gap, we investigated brain
alignment, that is, measuring the degree of predictivity of neural activity
recorded while participants were watching naturalistic movies (video along with
audio) with representations derived from MLLMs. We utilized
instruction-specific embeddings from six video and two audio instruction-tuned
MLLMs. Experiments with 13 video task-specific instructions show that
instruction-tuned video MLLMs significantly outperform non-instruction-tuned
multimodal (by 15%) and unimodal models (by 20%). Our evaluation of MLLMs for
both video and audio tasks using language-guided instructions shows clear
disentanglement in task-specific representations from MLLMs, leading to precise
differentiation of multimodal functional processing in the brain. We also find
that MLLM layers align hierarchically with the brain, with early sensory areas
showing strong alignment with early layers, while higher-level visual and
language regions align more with middle to late layers. These findings provide
clear evidence for the role of task-specific instructions in improving the
alignment between brain activity and MLLMs, and open new avenues for mapping
joint information processing in both the systems. We make the code publicly
available [https://github.com/subbareddy248/mllm_videos].

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [117] [Neural-Augmented Kelvinlet: Real-Time Soft Tissue Deformation with Multiple Graspers](https://arxiv.org/abs/2506.08043)
*Ashkan Shahbazi,Kyvia Pereira,Jon S. Heiselman,Elaheh Akbari,Annie C. Benson,Sepehr Seifi,Xinyuan Liu,Garrison L. Johnston,Erwin Terpstra,Anne Draaisma,Jan-Jaap Severes,Jie Ying Wu,Nabil Simaan,Michael L. Miga,Soheil Kolouri*

Main category: cs.GR

TL;DR: 提出结合Kelvinlet先验的物理神经模拟器，实现高精度实时软组织形变仿真


<details>
  <summary>Details</summary>
Motivation: 手术机器人和医学训练需要实时精准的软组织形变模拟技术

Method: 将Kelvinlet物理先验融入神经网络，结合线性和非线性FEM数据进行残差学习与正则化

Result: 提升各类架构神经网络的预测精度和物理一致性，在腹腔镜操作仿真中实现高保真低延时

Conclusion: Kelvinlet增强学习为手术仿真提供了高效物理感知解决方案

Abstract: Fast and accurate simulation of soft tissue deformation is a critical factor
for surgical robotics and medical training. In this paper, we introduce a novel
physics-informed neural simulator that approximates soft tissue deformations in
a realistic and real-time manner. Our framework integrates Kelvinlet-based
priors into neural simulators, making it the first approach to leverage
Kelvinlets for residual learning and regularization in data-driven soft tissue
modeling. By incorporating large-scale Finite Element Method (FEM) simulations
of both linear and nonlinear soft tissue responses, our method improves neural
network predictions across diverse architectures, enhancing accuracy and
physical consistency while maintaining low latency for real-time performance.
We demonstrate the effectiveness of our approach by performing accurate
surgical maneuvers that simulate the use of standard laparoscopic tissue
grasping tools with high fidelity. These results establish Kelvinlet-augmented
learning as a powerful and efficient strategy for real-time, physics-aware soft
tissue simulation in surgical applications.

</details>


### [118] [A Real-time 3D Desktop Display](https://arxiv.org/abs/2506.08064)
*Livio Tenze,Enrique Canessa*

Main category: cs.GR

TL;DR: 扩展altiro3D库以实时处理2D图像/视频生成光场3D效果，集成AI提升性能并提供跨平台界面。


<details>
  <summary>Details</summary>
Motivation: 将原有玻璃全息显示库升级，解决实时处理3D视频流（2D摄像机输入或平面文件）及提升用户体验需求。

Method: 采用MiDaS CNN提取单图深度图，结合AI优化算法，开发GUI实现屏幕区域选择及光场设备输出。

Result: 支持实时转换2D图像/视频流为Native光场格式，兼容桌面多任务场景，适配Looking Glass等3D设备。

Conclusion: 拓展了3D内容生成方式，降低实时立体显示技术门槛，推进消费级光场显示应用。

Abstract: A new extended version of the altiro3D C++ Library -- initially developed to
get glass-free holographic displays starting from 2D images -- is here
introduced aiming to deal with 3D video streams from either 2D webcam images or
flat video files. These streams are processed in real-time to synthesize
light-fields (in Native format) and feed realistic 3D experiences. The core
function needed to recreate multiviews consists on the use of MiDaS
Convolutional Neural Network (CNN), which allows to extract a depth map from a
single 2D image. Artificial Intelligence (AI) computing techniques are applied
to improve the overall performance of the extended altiro3D Library. Thus,
altiro3D can now treat standard images, video streams or screen portions of a
Desktop where other apps may be also running (like web browsers, video chats,
etc) and render them into 3D. To achieve the latter, a screen region need to be
selected in order to feed the output directly into a light-field 3D device such
as Looking Glass (LG) Portrait. In order to simplify the acquisition of a
Desktop screen area by the user, a multi-platform Graphical User Interface has
been also implemented. Sources available at:
https://github.com/canessae/altiro3D/releases/tag/2.0.0

</details>


### [119] [Generalizable Articulated Object Reconstruction from Casually Captured RGBD Videos](https://arxiv.org/abs/2506.08334)
*Weikun Peng,Jun Lv,Cewu Lu,Manolis Savva*

Main category: cs.GR

TL;DR: 提出从动态RGBD视频重建铰接物体的新方法，显著优于现有技术


<details>
  <summary>Details</summary>
Motivation: 解决现有方法依赖精细采集数据的问题，实现手机拍摄视频的实用化物体重建

Method: 开发基于粗到精框架的联合参数推断和部件分割算法，处理动态场景中的运动遮挡

Result: 构建784个视频数据集，在合成和真实场景中均展现优越的重建精度

Conclusion: 为机器人交互提供可扩展的铰接物体理解方案，推动具身智能应用发展

Abstract: Articulated objects are prevalent in daily life. Understanding their
kinematic structure and reconstructing them have numerous applications in
embodied AI and robotics. However, current methods require carefully captured
data for training or inference, preventing practical, scalable, and
generalizable reconstruction of articulated objects. We focus on reconstruction
of an articulated object from a casually captured RGBD video shot with a
hand-held camera. A casually captured video of an interaction with an
articulated object is easy to acquire at scale using smartphones. However, this
setting is quite challenging, as the object and camera move simultaneously and
there are significant occlusions as the person interacts with the object. To
tackle these challenges, we introduce a coarse-to-fine framework that infers
joint parameters and segments movable parts of the object from a dynamic RGBD
video. To evaluate our method under this new setting, we build a 20$\times$
larger synthetic dataset of 784 videos containing 284 objects across 11
categories. We compare our approach with existing methods that also take video
as input. Experiments show that our method can reconstruct synthetic and real
articulated objects across different categories from dynamic RGBD videos,
outperforming existing methods significantly.

</details>


### [120] [Complex-Valued Holographic Radiance Fields](https://arxiv.org/abs/2506.08350)
*Yicheng Zhan,Dong-Ha Shin,Seung-Hwan Baek,Kaan Ak艧it*

Main category: cs.GR

TL;DR: 提出复值高斯原语优化方法，实现高效3D全息场景表示


<details>
  <summary>Details</summary>
Motivation: 解决现有全息显示中光场振幅与相位联合建模难题，提升物理真实感渲染效果

Method: 通过复值高斯原语重构3D高斯泼溅算法，利用RGBD多视角图像直接优化全息场景表示

Result: 实现30-10000倍速度提升，保持图像质量，无需重复全息图优化

Conclusion: 首次实现几何对齐的物理真实全息场景表示，为下一代显示技术奠定基础

Abstract: Modeling the full properties of light, including both amplitude and phase, in
3D representations is crucial for advancing physically plausible rendering,
particularly in holographic displays. To support these features, we propose a
novel representation that optimizes 3D scenes without relying on
intensity-based intermediaries. We reformulate 3D Gaussian splatting with
complex-valued Gaussian primitives, expanding support for rendering with light
waves. By leveraging RGBD multi-view images, our method directly optimizes
complex-valued Gaussians as a 3D holographic scene representation. This
eliminates the need for computationally expensive hologram re-optimization.
Compared with state-of-the-art methods, our method achieves 30x-10,000x speed
improvements while maintaining on-par image quality, representing a first step
towards geometrically aligned, physically plausible holographic scene
representations.

</details>


### [121] [Fine-Grained Spatially Varying Material Selection in Images](https://arxiv.org/abs/2506.09023)
*Julia Guerrero-Viu,Michael Fischer,Iliyan Georgiev,Elena Garces,Diego Gutierrez,Belen Masia,Valentin Deschaintre*

Main category: cs.GR

TL;DR: 提出基于视觉Transformer的多分辨率材料选择方法，支持纹理/子纹理双层次编辑并构建大规模标注数据集


<details>
  <summary>Details</summary>
Motivation: 解决现有材质选择方法对光照和反射变化敏感的问题，提升图像编辑效率

Method: 利用ViT特征提取，结合多分辨率处理策略，构建包含80万标注样本的DuMaS数据集实现双层次选择

Result: 相比现有方法获得更精细稳定的选择效果，支持纹理和子纹理层级的材质编辑

Conclusion: 为复杂材质编辑提供可靠工具，开源数据集推动相关领域研究发展

Abstract: Selection is the first step in many image editing processes, enabling faster
and simpler modifications of all pixels sharing a common modality. In this
work, we present a method for material selection in images, robust to lighting
and reflectance variations, which can be used for downstream editing tasks. We
rely on vision transformer (ViT) models and leverage their features for
selection, proposing a multi-resolution processing strategy that yields finer
and more stable selection results than prior methods. Furthermore, we enable
selection at two levels: texture and subtexture, leveraging a new two-level
material selection (DuMaS) dataset which includes dense annotations for over
800,000 synthetic images, both on the texture and subtexture levels.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [122] [A System for Accurate Tracking and Video Recordings of Rodent Eye Movements using Convolutional Neural Networks for Biomedical Image Segmentation](https://arxiv.org/abs/2506.08183)
*Isha Puri,David Cox*

Main category: eess.IV

TL;DR: 开发首个基于卷积神经网络的啮齿动物瞳孔与角膜反射识别系统，提升眼动追踪精度


<details>
  <summary>Details</summary>
Motivation: 现有眼动追踪技术主要针对人类眼睛设计，无法有效处理啮齿动物眼睛的小尺寸、毛发干扰和个体差异等独特挑战

Method: 提出可增量训练的卷积神经网络架构，结合自动化红外视频系统，实现生物医学图像分割

Result: 开发出高精度的眼动追踪技术，在啮齿动物实验中达到当前最先进水平

Conclusion: 为神经科学和视觉科学研究提供了首个实用化的高精度啮齿动物眼动追踪解决方案

Abstract: Research in neuroscience and vision science relies heavily on careful
measurements of animal subject's gaze direction. Rodents are the most widely
studied animal subjects for such research because of their economic advantage
and hardiness. Recently, video based eye trackers that use image processing
techniques have become a popular option for gaze tracking because they are easy
to use and are completely noninvasive. Although significant progress has been
made in improving the accuracy and robustness of eye tracking algorithms,
unfortunately, almost all of the techniques have focused on human eyes, which
does not account for the unique characteristics of the rodent eye images, e.g.,
variability in eye parameters, abundance of surrounding hair, and their small
size. To overcome these unique challenges, this work presents a flexible,
robust, and highly accurate model for pupil and corneal reflection
identification in rodent gaze determination that can be incrementally trained
to account for variability in eye parameters encountered in the field. To the
best of our knowledge, this is the first paper that demonstrates a highly
accurate and practical biomedical image segmentation based convolutional neural
network architecture for pupil and corneal reflection identification in eye
images. This new method, in conjunction with our automated infrared videobased
eye recording system, offers the state of the art technology in eye tracking
for neuroscience and vision science research for rodents.

</details>


### [123] [Snap-and-tune: combining deep learning and test-time optimization for high-fidelity cardiovascular volumetric meshing](https://arxiv.org/abs/2506.08280)
*Daniel H. Pak,Shubh Thaker,Kyle Baylous,Xiaoran Zhang,Danny Bluestein,James S. Duncan*

Main category: eess.IV

TL;DR: 提出结合深度学习与测试时优化的snap-and-tune策略，显著提升医学体积网格生成质量


<details>
  <summary>Details</summary>
Motivation: 现有基于深度学习的模板变形方法在复杂医学结构建模中存在高曲率区域灵活性不足和部件间距失真问题

Method: 采用两阶段snap-and-tune策略：先通过深度学习快速拟合初始形状，再通过测试时优化进行样本特异性网格修正

Result: 在空间精度和网格质量上实现显著提升，无需额外训练标签且全自动化完成

Conclusion: 该方法生成的网格在多个物理模拟平台验证有效，为个性化医疗的物理仿真提供可靠基础

Abstract: High-quality volumetric meshing from medical images is a key bottleneck for
physics-based simulations in personalized medicine. For volumetric meshing of
complex medical structures, recent studies have often utilized deep learning
(DL)-based template deformation approaches to enable fast test-time generation
with high spatial accuracy. However, these approaches still exhibit
limitations, such as limited flexibility at high-curvature areas and
unrealistic inter-part distances. In this study, we introduce a simple yet
effective snap-and-tune strategy that sequentially applies DL and test-time
optimization, which combines fast initial shape fitting with more detailed
sample-specific mesh corrections. Our method provides significant improvements
in both spatial accuracy and mesh quality, while being fully automated and
requiring no additional training labels. Finally, we demonstrate the
versatility and usefulness of our newly generated meshes via solid mechanics
simulations in two different software platforms. Our code is available at
https://github.com/danpak94/Deep-Cardiac-Volumetric-Mesh.

</details>


### [124] [Plug-and-Play Linear Attention for Pre-trained Image and Video Restoration Models](https://arxiv.org/abs/2506.08520)
*Srinivasan Kidambi,Pravin Nair*

Main category: eess.IV

TL;DR: 提出基于Nyström方法的PnP-Nystra线性注意力机制，实现无需训练的MHSA加速替代方案


<details>
  <summary>Details</summary>
Motivation: 多头自注意力(MHSA)的二次计算复杂度在实时视觉任务中形成性能瓶颈

Method: 利用Nyström近似构建即插即用的线性注意力模块，兼容SwinIR等主流架构

Result: 在4090 GPU和CPU上分别实现2-5倍加速，PSNR最大损失仅1.5dB

Conclusion: 首个证明线性注意力可作为MHSA免训练替代的方案，平衡效率与性能

Abstract: Multi-head self-attention (MHSA) has become a core component in modern
computer vision models. However, its quadratic complexity with respect to input
length poses a significant computational bottleneck in real-time and resource
constrained environments. We propose PnP-Nystra, a Nystr\"om based linear
approximation of self-attention, developed as a plug-and-play (PnP) module that
can be integrated into the pre-trained image and video restoration models
without retraining. As a drop-in replacement for MHSA, PnP-Nystra enables
efficient acceleration in various window-based transformer architectures,
including SwinIR, Uformer, and RVRT. Our experiments across diverse image and
video restoration tasks, including denoising, deblurring, and super-resolution,
demonstrate that PnP-Nystra achieves a 2-4x speed-up on an NVIDIA RTX 4090 GPU
and a 2-5x speed-up on CPU inference. Despite these significant gains, the
method incurs a maximum PSNR drop of only 1.5 dB across all evaluated tasks. To
the best of our knowledge, we are the first to demonstrate a linear attention
functioning as a training-free substitute for MHSA in restoration models.

</details>


### [125] [DCD: A Semantic Segmentation Model for Fetal Ultrasound Four-Chamber View](https://arxiv.org/abs/2506.08534)
*Donglian Li,Hui Guo,Minglang Chen,Huizhen Chen,Jialing Chen,Bocheng Liang,Pengchen Liang,Ying Tan*

Main category: eess.IV

TL;DR: 提出DCD深度学习模型，通过Dense ASPP和CBAM模块实现胎儿四腔心超声图像精准分割


<details>
  <summary>Details</summary>
Motivation: 胎儿心脏结构分割受超声伪影、噪声和孕周变化影响，需提高产前先天性心脏病诊断精度

Method: 结合密集空洞空间金字塔池化(Dense ASPP)实现多尺度特征提取，集成卷积注意力模块(CBAM)增强特征表示

Result: 模型有效捕获局部/全局上下文信息，实现精确稳定的心脏结构分割

Conclusion: DCD模型降低超声医师工作量，提升产前心脏评估准确性，促进先心病早期诊断

Abstract: Accurate segmentation of anatomical structures in the apical four-chamber
(A4C) view of fetal echocardiography is essential for early diagnosis and
prenatal evaluation of congenital heart disease (CHD). However, precise
segmentation remains challenging due to ultrasound artifacts, speckle noise,
anatomical variability, and boundary ambiguity across different gestational
stages. To reduce the workload of sonographers and enhance segmentation
accuracy, we propose DCD, an advanced deep learning-based model for automatic
segmentation of key anatomical structures in the fetal A4C view. Our model
incorporates a Dense Atrous Spatial Pyramid Pooling (Dense ASPP) module,
enabling superior multi-scale feature extraction, and a Convolutional Block
Attention Module (CBAM) to enhance adaptive feature representation. By
effectively capturing both local and global contextual information, DCD
achieves precise and robust segmentation, contributing to improved prenatal
cardiac assessment.

</details>


### [126] [Biologically Inspired Deep Learning Approaches for Fetal Ultrasound Image Classification](https://arxiv.org/abs/2506.08623)
*Rinat Prochii,Elizaveta Dakhova,Pavel Birulin,Maxim Sharaev*

Main category: eess.IV

TL;DR: 提出生物启发的双路径集成模型，实现在复杂临床超声图像中同时识别16个胎儿解剖结构


<details>
  <summary>Details</summary>
Motivation: 解决中期胎儿超声图像分类中存在的低质量、类内差异大和类别不平衡问题，扩展传统仅针对少数解剖结构的识别能力

Method: 模拟生物视觉系统的层次化模块结构，结合浅层路径（低分辨率粗粒度特征）和细节路径（高分辨率细粒度特征）的双分支集成框架

Result: 在5,298张真实临床图像上达到90%器官识别准确率>0.75，75%器官>0.85，性能与复杂模型在小类别集上相当

Conclusion: 生物启发式模块化架构为临床复杂场景下的胎儿解剖识别提供了高效可扩展的解决方案

Abstract: Accurate classification of second-trimester fetal ultrasound images remains
challenging due to low image quality, high intra-class variability, and
significant class imbalance. In this work, we introduce a simple yet powerful,
biologically inspired deep learning ensemble framework that-unlike prior
studies focused on only a handful of anatomical targets-simultaneously
distinguishes 16 fetal structures. Drawing on the hierarchical, modular
organization of biological vision systems, our model stacks two complementary
branches (a "shallow" path for coarse, low-resolution cues and a "detailed"
path for fine, high-resolution features), concatenating their outputs for final
prediction. To our knowledge, no existing method has addressed such a large
number of classes with a comparably lightweight architecture. We trained and
evaluated on 5,298 routinely acquired clinical images (annotated by three
experts and reconciled via Dawid-Skene), reflecting real-world noise and
variability rather than a "cleaned" dataset. Despite this complexity, our
ensemble (EfficientNet-B0 + EfficientNet-B6 with LDAM-Focal loss) identifies
90% of organs with accuracy > 0.75 and 75% of organs with accuracy >
0.85-performance competitive with more elaborate models applied to far fewer
categories. These results demonstrate that biologically inspired modular
stacking can yield robust, scalable fetal anatomy recognition in challenging
clinical settings.

</details>


### [127] [MAMBO: High-Resolution Generative Approach for Mammography Images](https://arxiv.org/abs/2506.08677)
*Milica 艩kipina,Nikola Jovi拧i膰,Nicola Dall'Asen,Vanja 艩venda,Anil Osman Tur,Slobodan Ili膰,Elisa Ricci,Dubravko 膯ulibrk*

Main category: eess.IV

TL;DR: 提出基于扩散模型的MAMBO方法生成3840x3840像素高分辨率乳腺X光图像，突破数据限制并提升AI辅助诊断能力


<details>
  <summary>Details</summary>
Motivation: 解决AI训练所需大规模乳腺X光数据因隐私/伦理限制难以获取的问题

Method: 集成局部/全局扩散模型，通过上下文信息辅助去噪的patch-based架构，实现全分辨率生成

Result: 生成3840x3840分辨率图像，经放射科验证具备临床可用性，在分类模型训练和异常检测中表现提升

Conclusion: 该框架突破高分辨率医学影像生成瓶颈，为精准诊断和早期病变检测提供有效数据增强方案

Abstract: Mammography is the gold standard for the detection and diagnosis of breast
cancer. This procedure can be significantly enhanced with Artificial
Intelligence (AI)-based software, which assists radiologists in identifying
abnormalities. However, training AI systems requires large and diverse
datasets, which are often difficult to obtain due to privacy and ethical
constraints. To address this issue, the paper introduces MAMmography ensemBle
mOdel (MAMBO), a novel patch-based diffusion approach designed to generate
full-resolution mammograms. Diffusion models have shown breakthrough results in
realistic image generation, yet few studies have focused on mammograms, and
none have successfully generated high-resolution outputs required to capture
fine-grained features of small lesions. To achieve this, MAMBO integrates
separate diffusion models to capture both local and global (image-level)
contexts. The contextual information is then fed into the final patch-based
model, significantly aiding the noise removal process. This thoughtful design
enables MAMBO to generate highly realistic mammograms of up to 3840x3840
pixels. Importantly, this approach can be used to enhance the training of
classification models and extended to anomaly detection. Experiments, both
numerical and radiologist validation, assess MAMBO's capabilities in image
generation, super-resolution, and anomaly detection, highlighting its potential
to enhance mammography analysis for more accurate diagnoses and earlier lesion
detection.

</details>


### [128] [Enhancing Synthetic CT from CBCT via Multimodal Fusion: A Study on the Impact of CBCT Quality and Alignment](https://arxiv.org/abs/2506.08716)
*Maximilian Tschuchnig,Lukas Lamminger,Philipp Steininger,Michael Gadermayr*

Main category: eess.IV

TL;DR: 通过多模态学习整合术中CBCT与术前CT，提升了合成CT生成质量


<details>
  <summary>Details</summary>
Motivation: CBCT存在严重伪影问题，需通过合成CT技术提升影像质量

Method: 采用多模态学习方法，结合术前CT与术中CBCT数据，利用合成和真实临床数据集验证

Result: 多模态方法显著优于单模态基线，在配准良好的低质量CBCT-CT案例中提升最明显

Conclusion: 多模态学习方法有效提升合成CT质量，研究结果具有临床可重复性

Abstract: Cone-Beam Computed Tomography (CBCT) is widely used for real-time
intraoperative imaging due to its low radiation dose and high acquisition
speed. However, despite its high resolution, CBCT suffers from significant
artifacts and thereby lower visual quality, compared to conventional Computed
Tomography (CT). A recent approach to mitigate these artifacts is synthetic CT
(sCT) generation, translating CBCT volumes into the CT domain. In this work, we
enhance sCT generation through multimodal learning, integrating intraoperative
CBCT with preoperative CT. Beyond validation on two real-world datasets, we use
a versatile synthetic dataset, to analyze how CBCT-CT alignment and CBCT
quality affect sCT quality. The results demonstrate that multimodal sCT
consistently outperform unimodal baselines, with the most significant gains
observed in well-aligned, low-quality CBCT-CT cases. Finally, we demonstrate
that these findings are highly reproducible in real-world clinical datasets.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [129] [PhyBlock: A Progressive Benchmark for Physical Understanding and Planning via 3D Block Assembly](https://arxiv.org/abs/2506.08708)
*Liang Ma,Jiajun Wen,Min Lin,Rongtao Xu,Xiwen Liang,Bingqian Lin,Jun Ma,Yongxin Wang,Ziming Wei,Haokun Lin,Mingfei Han,Meng Cao,Bokui Chen,Ivan Laptev,Xiaodan Liang*

Main category: cs.RO

TL;DR: 提出PhyBlock基准测试框架，揭示视觉语言模型在3D物理推理和规划任务中的显著局限性


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型在理解结构化3D环境物理现象方面存在严重缺陷，阻碍具身智能发展

Method: 结合四层认知层次组装任务和定向VQA样本，构建包含2600个任务的测评体系，从三个维度评估21个前沿模型

Result: 模型在高级规划任务中性能随复杂度骤降，空间定位错误率高达显著水平，思维链提示改进微弱（仅提升<2%）

Conclusion: PhyBench填补物理推理评估空白，为连接视觉语言理解与现实物理问题解决提供标准化测试平台

Abstract: While vision-language models (VLMs) have demonstrated promising capabilities
in reasoning and planning for embodied agents, their ability to comprehend
physical phenomena, particularly within structured 3D environments, remains
severely limited. To close this gap, we introduce PhyBlock, a progressive
benchmark designed to assess VLMs on physical understanding and planning
through robotic 3D block assembly tasks. PhyBlock integrates a novel four-level
cognitive hierarchy assembly task alongside targeted Visual Question Answering
(VQA) samples, collectively aimed at evaluating progressive spatial reasoning
and fundamental physical comprehension, including object properties, spatial
relationships, and holistic scene understanding. PhyBlock includes 2600 block
tasks (400 assembly tasks, 2200 VQA tasks) and evaluates models across three
key dimensions: partial completion, failure diagnosis, and planning robustness.
We benchmark 21 state-of-the-art VLMs, highlighting their strengths and
limitations in physically grounded, multi-step planning. Our empirical findings
indicate that the performance of VLMs exhibits pronounced limitations in
high-level planning and reasoning capabilities, leading to a notable decline in
performance for the growing complexity of the tasks. Error analysis reveals
persistent difficulties in spatial orientation and dependency reasoning.
Surprisingly, chain-of-thought prompting offers minimal improvements,
suggesting spatial tasks heavily rely on intuitive model comprehension. We
position PhyBlock as a unified testbed to advance embodied reasoning, bridging
vision-language understanding and real-world physical problem-solving.

</details>
