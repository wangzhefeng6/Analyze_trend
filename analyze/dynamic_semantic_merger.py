#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
åŠ¨æ€è¯­ä¹‰ç›¸ä¼¼æ€§åˆå¹¶æ¨¡å—
åŸºäºè¯å‘é‡ç›¸ä¼¼åº¦è‡ªåŠ¨è¯†åˆ«å’Œåˆå¹¶è¯­ä¹‰ç›¸è¿‘çš„èŠ‚ç‚¹
"""

import os
import logging
from typing import Dict, List, Tuple, Set
from collections import defaultdict
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.cluster import AgglomerativeClustering
import re
import jieba
import openai
from dataclasses import dataclass

@dataclass
class SimilarityCluster:
    """ç›¸ä¼¼æ€§èšç±»ç»“æœ"""
    main_term: str
    similar_terms: List[str]
    similarity_scores: List[float]
    merged_papers: int
    merged_leaves: int

class DynamicSemanticMerger:
    """åŠ¨æ€è¯­ä¹‰ç›¸ä¼¼æ€§åˆå¹¶å™¨"""
    
    def __init__(self, api_key: str = None, similarity_threshold: float = 0.85):
        """
        åˆå§‹åŒ–è¯­ä¹‰åˆå¹¶å™¨
        
        Args:
            api_key: OpenAI APIå¯†é’¥ï¼ˆç”¨äºembeddingï¼‰
            similarity_threshold: ç›¸ä¼¼åº¦é˜ˆå€¼ï¼Œè¶…è¿‡æ­¤å€¼çš„èŠ‚ç‚¹ä¼šè¢«åˆå¹¶
        """
        self.api_key = api_key or os.getenv('OPENAI_API_KEY')
        self.similarity_threshold = similarity_threshold
        self.logger = logging.getLogger(__name__)
        
        # åˆå§‹åŒ–OpenAIå®¢æˆ·ç«¯
        if self.api_key:
            openai.api_key = self.api_key
        
        # ç¼“å­˜å‘é‡ä»¥é¿å…é‡å¤è®¡ç®—
        self.embedding_cache = {}
        
        # é¢„å®šä¹‰çš„åŒä¹‰è¯è§„åˆ™ï¼ˆä½œä¸ºè¡¥å……ï¼‰
        self.synonym_rules = {
            # NLPç›¸å…³ - æ–°å¢å¤§é‡NLPæœ¯è¯­
            'large_language_models': [
                'large language models', 'large language model', 'llm', 'llms',
                'å¤§è¯­è¨€æ¨¡å‹', 'å¤§å‹è¯­è¨€æ¨¡å‹', 'è¯­è¨€å¤§æ¨¡å‹', 'è¯­è¨€æ¨¡å‹'
            ],
            'text_generation': [
                'text generation', 'natural language generation',
                'nlg', 'æ–‡æœ¬ç”Ÿæˆ', 'è‡ªç„¶è¯­è¨€ç”Ÿæˆ', 'è‡ªåŠ¨æ–‡æœ¬ç†è§£ä¸ç”Ÿæˆ'
            ],
            'dialogue_systems': [
                'dialogue systems', 'dialog systems', 'conversational ai',
                'chatbot', 'conversation', 'å¯¹è¯ç³»ç»Ÿ', 'ä¼šè¯ç³»ç»Ÿ', 'å¯¹è¯ç³»ç»Ÿä¸å¯¹è¯å»ºæ¨¡'
            ],
            'machine_translation': [
                'machine translation', 'neural machine translation', 'nmt',
                'æœºå™¨ç¿»è¯‘', 'ç¥ç»æœºå™¨ç¿»è¯‘'
            ],
            'question_answering': [
                'question answering', 'qa', 'question-answering',
                'é—®ç­”ç³»ç»Ÿ', 'é—®ç­”', 'æ™ºèƒ½é—®ç­”'
            ],
            'information_extraction': [
                'information extraction', 'ie', 'named entity recognition',
                'ner', 'relation extraction', 'ä¿¡æ¯æŠ½å–', 'å‘½åå®ä½“è¯†åˆ«'
            ],
            'sentiment_analysis': [
                'sentiment analysis', 'emotion analysis', 'opinion mining',
                'æƒ…æ„Ÿåˆ†æ', 'æƒ…ç»ªåˆ†æ', 'æ„è§æŒ–æ˜'
            ],
            'natural_language_processing': [
                'natural language processing', 'nlp', 'computational linguistics',
                'è‡ªç„¶è¯­è¨€å¤„ç†', 'è®¡ç®—è¯­è¨€å­¦'
            ],
            'text_classification': [
                'text classification', 'document classification', 'text categorization',
                'æ–‡æœ¬åˆ†ç±»', 'æ–‡æ¡£åˆ†ç±»'
            ],
            'multimodal_learning': [
                'multimodal learning', 'multi-modal learning', 'vision-language',
                'å¤šæ¨¡æ€å­¦ä¹ ', 'å¤šæ¨¡æ€æ¨ç†ä¸ç†è§£', 'è§†è§‰é—®ç­”ä¸å¤šæ¨¡æ€ç†è§£', 'è§†è§‰ä¸è¯­è¨€'
            ],
            
            # CVç›¸å…³ï¼ˆä¿æŒåŸæœ‰é€»è¾‘ï¼‰
            'three_d_vision': [
                'ä¸‰ç»´è§†è§‰ä¸é‡å»º', 'ä¸‰ç»´è§†è§‰ä¸åœºæ™¯é‡å»º', 'ä¸‰ç»´è§†è§‰ä¸å‡ ä½•å»ºæ¨¡',
                'ä¸‰ç»´é‡å»ºä¸è¿åŠ¨æ•æ‰', 'ä¸‰ç»´é‡å»ºä¸åœºæ™¯ç†è§£', 'ä¸‰ç»´è§†è§‰ä¸åœºæ™¯ç†è§£',
                'ç«‹ä½“è§†è§‰ä¸ä¸‰ç»´é‡å»º', 'ä¸‰ç»´é‡å»º', '3d reconstruction', '3d vision'
            ],
            'medical_imaging': [
                'åŒ»å­¦å›¾åƒåˆ†æ', 'åŒ»å­¦å½±åƒåˆ†æ', 'åŒ»å­¦å›¾åƒå¤„ç†',
                'medical imaging', 'medical image analysis'
            ],
            'object_detection': [
                'ç›®æ ‡æ£€æµ‹', 'ç›®æ ‡æ£€æµ‹ä¸è¯†åˆ«', 'ç›®æ ‡æ£€æµ‹ä¸å®šä½',
                'object detection', 'object recognition', 'ç›®æ ‡æ£€æµ‹ä¸è·Ÿè¸ª'
            ],
            'image_generation': [
                'å›¾åƒç”Ÿæˆ', 'å›¾åƒç”Ÿæˆä¸ç¼–è¾‘', 'å›¾åƒåˆæˆ', 'å›¾åƒç”Ÿæˆä¸ç†è§£',
                'image generation', 'image synthesis'
            ],
            'video_analysis': [
                'è§†é¢‘å¤„ç†', 'è§†é¢‘ç†è§£', 'è§†é¢‘åˆ†æä¸ç†è§£', 'è§†é¢‘ç†è§£ä¸ç”Ÿæˆ',
                'video processing', 'video understanding'
            ],
            
            # æœºå™¨å­¦ä¹ ç›¸å…³
            'deep_learning': [
                'deep learning', 'æ·±åº¦å­¦ä¹ ', 'neural networks', 'ç¥ç»ç½‘ç»œ', 'æ·±åº¦å­¦ä¹ å¯è§£é‡Šæ€§'
            ],
            'machine_learning': [
                'machine learning', 'æœºå™¨å­¦ä¹ ', 'ml', 'artificial intelligence', 'ai'
            ],
            'reinforcement_learning': [
                'reinforcement learning', 'rl', 'å¼ºåŒ–å­¦ä¹ ', 'å¢å¼ºå­¦ä¹ '
            ],
            
            # æœºå™¨äººç›¸å…³
            'robotics_perception': [
                'æœºå™¨äººæ„ŸçŸ¥ä¸åœºæ™¯ç†è§£', 'æœºå™¨äººæ„ŸçŸ¥ä¸è®¤çŸ¥', 'æœºå™¨äººæ„ŸçŸ¥ä¸è§†è§‰', 
                'æœºå™¨äººæ„ŸçŸ¥ä¸æ§åˆ¶', 'robotics perception', 'robot vision'
            ]
        }
    
    def get_text_embedding(self, text: str) -> np.ndarray:
        """
        è·å–æ–‡æœ¬çš„å‘é‡è¡¨ç¤º
        
        Args:
            text: è¾“å…¥æ–‡æœ¬
            
        Returns:
            æ–‡æœ¬å‘é‡
        """
        # æ£€æŸ¥ç¼“å­˜
        if text in self.embedding_cache:
            return self.embedding_cache[text]
        
        try:
            # ä½¿ç”¨OpenAI Embedding API
            response = openai.Embedding.create(
                model="text-embedding-ada-002",
                input=text
            )
            embedding = np.array(response['data'][0]['embedding'])
            self.embedding_cache[text] = embedding
            return embedding
            
        except Exception as e:
            self.logger.warning(f"è·å–embeddingå¤±è´¥ï¼Œä½¿ç”¨ç®€å•æ–¹æ³•: {e}")
            # é™çº§åˆ°ç®€å•çš„å­—ç¬¦çº§ç›¸ä¼¼åº¦
            return self._simple_text_vector(text)
    
    def _simple_text_vector(self, text: str) -> np.ndarray:
        """
        ç®€å•çš„æ–‡æœ¬å‘é‡åŒ–æ–¹æ³•ï¼ˆé™çº§æ–¹æ¡ˆï¼‰
        
        Args:
            text: è¾“å…¥æ–‡æœ¬
            
        Returns:
            ç®€å•å‘é‡è¡¨ç¤º
        """
        # æ¸…ç†æ–‡æœ¬
        text = re.sub(r'[^\w\s]', '', text.lower())
        
        # åˆ†è¯ï¼ˆä¸­è‹±æ–‡ï¼‰
        words = []
        # ä¸­æ–‡åˆ†è¯
        if re.search(r'[\u4e00-\u9fff]', text):
            words.extend(jieba.cut(text))
        else:
            words.extend(text.split())
        
        # åˆ›å»ºè¯é¢‘å‘é‡ï¼ˆç®€å•å®ç°ï¼‰
        vocab = set()
        for word in words:
            vocab.add(word.strip())
        
        vocab = sorted(list(vocab))
        vector = np.zeros(max(len(vocab), 10))  # æœ€å°‘10ç»´
        
        for i, word in enumerate(vocab):
            if i < len(vector):
                vector[i] = text.count(word)
        
        # å½’ä¸€åŒ–
        norm = np.linalg.norm(vector)
        if norm > 0:
            vector = vector / norm
            
        return vector
    
    def check_synonym_rules(self, terms: List[str]) -> Dict[str, List[str]]:
        """
        åŸºäºé¢„å®šä¹‰è§„åˆ™æ£€æŸ¥åŒä¹‰è¯
        
        Args:
            terms: å¾…æ£€æŸ¥çš„æœ¯è¯­åˆ—è¡¨
            
        Returns:
            åŒä¹‰è¯åˆ†ç»„ç»“æœ
        """
        groups = {}
        used_terms = set()
        
        for rule_name, synonyms in self.synonym_rules.items():
            matched_terms = []
            
            for term in terms:
                if term in used_terms:
                    continue
                    
                # æ£€æŸ¥æ˜¯å¦åŒ¹é…åŒä¹‰è¯è§„åˆ™
                term_lower = term.lower().strip()
                for synonym in synonyms:
                    synonym_lower = synonym.lower().strip()
                    # æ›´å®½æ¾çš„åŒ¹é…è§„åˆ™
                    if (term_lower == synonym_lower or 
                        synonym_lower in term_lower or
                        term_lower in synonym_lower or
                        self._fuzzy_match(term_lower, synonym_lower)):
                        matched_terms.append(term)
                        used_terms.add(term)
                        break
            
            if len(matched_terms) > 1:
                # é€‰æ‹©æœ€å¸¸è§æˆ–æœ€çŸ­çš„ä½œä¸ºä¸»æœ¯è¯­
                main_term = min(matched_terms, key=len)
                groups[main_term] = matched_terms
        
        return groups
    
    def _fuzzy_match(self, term1: str, term2: str) -> bool:
        """
        æ¨¡ç³ŠåŒ¹é…ä¸¤ä¸ªæœ¯è¯­
        """
        # å»é™¤å¸¸è§è¯æ±‡
        stop_words = {'and', 'or', 'the', 'a', 'an', 'with', 'for', 'to', 'of', 'in'}
        
        words1 = set(term1.split()) - stop_words
        words2 = set(term2.split()) - stop_words
        
        if not words1 or not words2:
            return False
        
        # è®¡ç®—è¯æ±‡é‡å åº¦
        intersection = words1.intersection(words2)
        union = words1.union(words2)
        
        overlap_ratio = len(intersection) / len(union)
        return overlap_ratio > 0.6  # 60%ä»¥ä¸Šé‡å è®¤ä¸ºæ˜¯ç›¸ä¼¼çš„
    
    def compute_similarity_matrix(self, terms: List[str]) -> np.ndarray:
        """
        è®¡ç®—æœ¯è¯­ç›¸ä¼¼åº¦çŸ©é˜µ
        
        Args:
            terms: æœ¯è¯­åˆ—è¡¨
            
        Returns:
            ç›¸ä¼¼åº¦çŸ©é˜µ
        """
        embeddings = []
        for term in terms:
            embedding = self.get_text_embedding(term)
            embeddings.append(embedding)
        
        embeddings = np.array(embeddings)
        similarity_matrix = cosine_similarity(embeddings)
        
        return similarity_matrix
    
    def cluster_similar_terms(self, terms: List[str], node_data: Dict[str, Dict]) -> List[SimilarityCluster]:
        """
        èšç±»ç›¸ä¼¼æœ¯è¯­
        
        Args:
            terms: æœ¯è¯­åˆ—è¡¨
            node_data: èŠ‚ç‚¹æ•°æ®å­—å…¸
            
        Returns:
            ç›¸ä¼¼æ€§èšç±»ç»“æœ
        """
        if len(terms) < 2:
            return []
        
        # 1. é¦–å…ˆä½¿ç”¨é¢„å®šä¹‰è§„åˆ™
        rule_groups = self.check_synonym_rules(terms)
        clusters = []
        used_terms = set()
        
        for main_term, similar_terms in rule_groups.items():
            total_papers = sum(node_data.get(term, {}).get('node_info', {}).get('total_papers', 0) for term in similar_terms)
            total_leaves = sum(node_data.get(term, {}).get('node_info', {}).get('leaf_count', 0) for term in similar_terms)
            
            clusters.append(SimilarityCluster(
                main_term=main_term,
                similar_terms=similar_terms,
                similarity_scores=[1.0] * len(similar_terms),  # è§„åˆ™åŒ¹é…ç»™æœ€é«˜åˆ†
                merged_papers=total_papers,
                merged_leaves=total_leaves
            ))
            used_terms.update(similar_terms)
        
        # 2. å¯¹æœªä½¿ç”¨è§„åˆ™åŒ¹é…çš„æœ¯è¯­è¿›è¡Œå‘é‡èšç±»
        remaining_terms = [term for term in terms if term not in used_terms]
        
        if len(remaining_terms) >= 2:
            try:
                similarity_matrix = self.compute_similarity_matrix(remaining_terms)
                
                # ä½¿ç”¨å±‚æ¬¡èšç±»
                distance_matrix = 1 - similarity_matrix
                clustering = AgglomerativeClustering(
                    n_clusters=None,
                    distance_threshold=1 - self.similarity_threshold,
                    affinity='precomputed',
                    linkage='average'
                )
                cluster_labels = clustering.fit_predict(distance_matrix)
                
                # ç»„ç»‡èšç±»ç»“æœ
                cluster_groups = defaultdict(list)
                for i, label in enumerate(cluster_labels):
                    cluster_groups[label].append(remaining_terms[i])
                
                # è½¬æ¢ä¸ºSimilarityClusterå¯¹è±¡
                for cluster_id, cluster_terms in cluster_groups.items():
                    if len(cluster_terms) > 1:
                        # é€‰æ‹©è®ºæ–‡æ•°æœ€å¤šçš„ä½œä¸ºä¸»æœ¯è¯­
                        main_term = max(cluster_terms, 
                                      key=lambda t: node_data.get(t, {}).get('node_info', {}).get('total_papers', 0))
                        
                        # è®¡ç®—ç›¸ä¼¼åº¦åˆ†æ•°
                        main_idx = remaining_terms.index(main_term)
                        similarity_scores = []
                        for term in cluster_terms:
                            term_idx = remaining_terms.index(term)
                            similarity_scores.append(similarity_matrix[main_idx][term_idx])
                        
                        total_papers = sum(node_data.get(term, {}).get('node_info', {}).get('total_papers', 0) for term in cluster_terms)
                        total_leaves = sum(node_data.get(term, {}).get('node_info', {}).get('leaf_count', 0) for term in cluster_terms)
                        
                        clusters.append(SimilarityCluster(
                            main_term=main_term,
                            similar_terms=cluster_terms,
                            similarity_scores=similarity_scores,
                            merged_papers=total_papers,
                            merged_leaves=total_leaves
                        ))
                        
            except Exception as e:
                self.logger.warning(f"å‘é‡èšç±»å¤±è´¥: {e}")
        
        return clusters
    
    def merge_similar_nodes(self, parent_aggregation: Dict) -> Dict:
        """
        åŠ¨æ€åˆå¹¶è¯­ä¹‰ç›¸ä¼¼çš„èŠ‚ç‚¹
        
        Args:
            parent_aggregation: çˆ¶èŠ‚ç‚¹èšåˆç»“æœ
            
        Returns:
            åˆå¹¶åçš„ç»“æœ
        """
        # æå–æ‰€æœ‰èŠ‚ç‚¹çš„æ˜¾ç¤ºåç§°
        display_names = []
        name_to_key = {}
        
        for key, data in parent_aggregation.items():
            display_name = data['node_info'].get('display_name', key)
            display_names.append(display_name)
            name_to_key[display_name] = key
        
        print(f"ğŸ” å¼€å§‹åŠ¨æ€è¯­ä¹‰ç›¸ä¼¼æ€§åˆ†æ...")
        print(f"   å¾…åˆ†æèŠ‚ç‚¹æ•°: {len(display_names)}")
        print(f"   ç›¸ä¼¼åº¦é˜ˆå€¼: {self.similarity_threshold}")
        
        # ä½¿ç”¨é¢„å®šä¹‰è§„åˆ™è¿›è¡Œåˆ†ç»„
        rule_groups = self.check_synonym_rules(display_names)
        
        merged_result = {}
        used_keys = set()
        
        print(f"ğŸ”— å‘ç° {len(rule_groups)} ä¸ªç›¸ä¼¼æ€§èšç±»:")
        
        # å¤„ç†è§„åˆ™åŒ¹é…çš„åˆ†ç»„
        for i, (main_term, similar_terms) in enumerate(rule_groups.items(), 1):
            print(f"   {i}. {main_term} (åˆå¹¶ {len(similar_terms)} ä¸ªèŠ‚ç‚¹)")
            print(f"      â””â”€ åŒ…å«: {', '.join(similar_terms)}")
            
            # åˆå¹¶èŠ‚ç‚¹æ•°æ®
            all_keywords = []
            all_methods = []
            all_problems = []
            all_leaf_nodes = []
            total_papers = 0
            total_leaf_count = 0
            
            for term in similar_terms:
                key = name_to_key[term]
                node_data = parent_aggregation[key]
                
                total_papers += node_data['node_info']['total_papers']
                total_leaf_count += node_data['node_info']['leaf_count']
                all_leaf_nodes.extend(node_data['node_info']['leaf_nodes'])
                all_keywords.extend(node_data['aggregated_analysis']['keywords'])
                all_methods.extend(node_data['aggregated_analysis']['methods'])
                all_problems.extend(node_data['aggregated_analysis']['problems'])
                
                used_keys.add(key)
            
            print(f"      â””â”€ è®ºæ–‡æ•°: {total_papers}, å­é¢†åŸŸ: {total_leaf_count}")
            
            # é‡æ–°èšåˆç»Ÿè®¡æ•°æ®
            merged_keywords = self._merge_frequency_items(all_keywords, 'term')
            merged_methods = self._merge_frequency_items(all_methods, 'method')
            merged_problems = self._merge_frequency_items(all_problems, 'problem')
            
            # ç”Ÿæˆæ‘˜è¦
            summary = f"è¯¥é¢†åŸŸåŒ…å«{total_leaf_count}ä¸ªå­é¢†åŸŸï¼Œå…±{total_papers}ç¯‡è®ºæ–‡ã€‚"
            if merged_keywords:
                top_keywords = [item['term'] for item in merged_keywords[:3]]
                summary += f" ä¸»è¦å…³é”®è¯ï¼š{', '.join(top_keywords)}ã€‚"
            
            merged_result[main_term] = {
                'node_info': {
                    'name': main_term,
                    'display_name': main_term,
                    'total_papers': total_papers,
                    'leaf_count': total_leaf_count,
                    'leaf_nodes': list(set(all_leaf_nodes)),
                    'merged_from': similar_terms
                },
                'aggregated_analysis': {
                    'keywords': merged_keywords,
                    'methods': merged_methods,
                    'problems': merged_problems,
                    'summary': summary
                }
            }
        
        # æ·»åŠ æœªè¢«åˆå¹¶çš„èŠ‚ç‚¹
        for key, data in parent_aggregation.items():
            if key not in used_keys:
                display_name = data['node_info'].get('display_name', key)
                merged_result[display_name] = {
                    'node_info': {
                        'name': display_name,
                        'display_name': display_name,
                        'total_papers': data['node_info']['total_papers'],
                        'leaf_count': data['node_info']['leaf_count'],
                        'leaf_nodes': data['node_info']['leaf_nodes']
                    },
                    'aggregated_analysis': data['aggregated_analysis']
                }
        
        print(f"\nâœ… åˆå¹¶å®Œæˆ: {len(parent_aggregation)} -> {len(merged_result)} ä¸ªèŠ‚ç‚¹")
        return merged_result
    
    def _merge_frequency_items(self, items_list: List[Dict], key_field: str) -> List[Dict]:
        """
        åˆå¹¶é¢‘æ¬¡ç»Ÿè®¡é¡¹ç›®
        
        Args:
            items_list: é¡¹ç›®åˆ—è¡¨
            key_field: å…³é”®å­—æ®µå ('term', 'method', 'problem')
            
        Returns:
            åˆå¹¶åçš„TOP10é¡¹ç›®
        """
        term_counts = defaultdict(int)
        term_descriptions = {}
        
        for item in items_list:
            if isinstance(item, dict):
                term = item.get(key_field, '')
                frequency = item.get('frequency', 1)
                description = item.get('description', '')
                
                if term:
                    term_counts[term] += frequency
                    if description and term not in term_descriptions:
                        term_descriptions[term] = description
        
        # æ’åºå¹¶å–TOP10
        sorted_terms = sorted(term_counts.items(), key=lambda x: x[1], reverse=True)[:10]
        
        result = []
        for term, count in sorted_terms:
            result.append({
                key_field: term,
                'frequency': count,
                'description': term_descriptions.get(term, '')
            })
        
        return result

def test_dynamic_merger():
    """æµ‹è¯•åŠ¨æ€è¯­ä¹‰åˆå¹¶å™¨"""
    
    # æ¨¡æ‹ŸåŒ…å«NLPå’ŒCVèŠ‚ç‚¹çš„æ•°æ®
    mock_data = {
        "text_generation": {
            'node_info': {
                'display_name': "æ–‡æœ¬ç”Ÿæˆ",
                'total_papers': 74,
                'leaf_count': 20,
                'leaf_nodes': ["GPTç”Ÿæˆ", "æ–‡æœ¬æ‘˜è¦"]
            },
            'aggregated_analysis': {
                'keywords': [{'term': 'Large Language Models', 'frequency': 30}],
                'methods': [{'method': 'Transformer', 'frequency': 25}],
                'problems': [{'problem': 'hallucination', 'frequency': 15}],
                'summary': 'æ–‡æœ¬ç”Ÿæˆç›¸å…³ç ”ç©¶'
            }
        },
        "large_language_models": {
            'node_info': {
                'display_name': "å¤§è¯­è¨€æ¨¡å‹",
                'total_papers': 43,
                'leaf_count': 15,
                'leaf_nodes': ["LLMè®­ç»ƒ", "æ¨¡å‹å¯¹é½"]
            },
            'aggregated_analysis': {
                'keywords': [{'term': 'LLM', 'frequency': 40}],
                'methods': [{'method': 'fine-tuning', 'frequency': 30}],
                'problems': [{'problem': 'bias', 'frequency': 20}],
                'summary': 'å¤§è¯­è¨€æ¨¡å‹ç ”ç©¶'
            }
        },
        "natural_language_processing": {
            'node_info': {
                'display_name': "è‡ªç„¶è¯­è¨€å¤„ç†",
                'total_papers': 25,
                'leaf_count': 10,
                'leaf_nodes': ["NER", "æƒ…æ„Ÿåˆ†æ"]
            },
            'aggregated_analysis': {
                'keywords': [{'term': 'NLP', 'frequency': 25}],
                'methods': [{'method': 'BERT', 'frequency': 20}],
                'problems': [{'problem': 'low resource', 'frequency': 10}],
                'summary': 'NLPç›¸å…³ç ”ç©¶'
            }
        },
        "medical_image_analysis": {
            'node_info': {
                'display_name': "åŒ»å­¦å›¾åƒåˆ†æ",
                'total_papers': 35,
                'leaf_count': 8,
                'leaf_nodes': ["CTåˆ†æ", "MRIå¤„ç†"]
            },
            'aggregated_analysis': {
                'keywords': [{'term': 'medical imaging', 'frequency': 30}],
                'methods': [{'method': 'CNN', 'frequency': 25}],
                'problems': [{'problem': 'data scarcity', 'frequency': 15}],
                'summary': 'åŒ»å­¦å›¾åƒåˆ†æ'
            }
        },
        "medical_imaging": {
            'node_info': {
                'display_name': "åŒ»å­¦å½±åƒåˆ†æ",
                'total_papers': 12,
                'leaf_count': 3,
                'leaf_nodes': ["å½±åƒè¯Šæ–­"]
            },
            'aggregated_analysis': {
                'keywords': [{'term': 'medical image', 'frequency': 12}],
                'methods': [{'method': 'deep learning', 'frequency': 10}],
                'problems': [{'problem': 'annotation cost', 'frequency': 8}],
                'summary': 'åŒ»å­¦å½±åƒç›¸å…³'
            }
        }
    }
    
    # æµ‹è¯•åŠ¨æ€åˆå¹¶
    merger = DynamicSemanticMerger(similarity_threshold=0.8)
    result = merger.merge_similar_nodes(mock_data)
    
    print(f"\nğŸ“Š æµ‹è¯•ç»“æœ:")
    for name, data in result.items():
        info = data['node_info']
        print(f"  âœ“ {name}: {info['total_papers']} ç¯‡è®ºæ–‡ ({info['leaf_count']} å­é¢†åŸŸ)")
        if 'merged_from' in info:
            print(f"    â””â”€ åˆå¹¶è‡ª: {', '.join(info['merged_from'])}")

if __name__ == "__main__":
    test_dynamic_merger() 