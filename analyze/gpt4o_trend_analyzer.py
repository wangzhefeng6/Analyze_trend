#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è®ºæ–‡è¶‹åŠ¿åˆ†æç³»ç»Ÿ
ç”¨äºå¤„ç†dataç›®å½•ä¸‹çš„jsonlæ–‡ä»¶ï¼Œä½¿ç”¨OpenAI APIå¤§æ¨¡å‹åˆ†æè®ºæ–‡æ‘˜è¦å¹¶ç»Ÿè®¡è¶‹åŠ¿
"""

import json
import os
import sys
import logging
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from collections import defaultdict, Counter
from typing import Dict, List, Any, Tuple
from openai import OpenAI
import time
from concurrent.futures import ThreadPoolExecutor, as_completed
import pickle
from pathlib import Path
from dotenv import load_dotenv

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('openai_analysis.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class GPT4oTrendAnalyzer:
    """ä½¿ç”¨OpenAI APIè¿›è¡Œè®ºæ–‡è¶‹åŠ¿åˆ†æçš„ç±»"""
    
    def __init__(self, api_key=None, proxy_url=None, data_dir="data"):
        """
        åˆå§‹åŒ–OpenAIåˆ†æå™¨
        
        Args:
            api_key: OpenAI APIå¯†é’¥
            proxy_url: ä»£ç†URLï¼ˆåºŸå¼ƒï¼Œä¿ç•™ç”¨äºå…¼å®¹æ€§ï¼‰
            data_dir: æ•°æ®ç›®å½•è·¯å¾„
        """
        # åŠ è½½ç¯å¢ƒå˜é‡
        env_file = '.env'
        if os.path.exists(env_file):
            load_dotenv(env_file)
            logger.info(f"å·²åŠ è½½ç¯å¢ƒå˜é‡æ–‡ä»¶: {env_file}")
        
        # APIé…ç½®
        self.api_key = api_key or os.getenv('OPENAI_API_KEY')
        if not self.api_key:
            raise ValueError("æœªæ‰¾åˆ°APIå¯†é’¥ï¼Œè¯·è®¾ç½®OPENAI_API_KEYç¯å¢ƒå˜é‡")
        
        # OpenAIå®¢æˆ·ç«¯é…ç½®
        self.base_url = os.getenv('OPENAI_BASE_URL', 'https://api.dou.chat/v1')
        self.model_name = "openai/gpt-4.1"
        
        # åˆå§‹åŒ–OpenAIå®¢æˆ·ç«¯
        self.client = OpenAI(
            api_key=self.api_key,
            base_url=self.base_url
        )
        
        logger.info(f"ä½¿ç”¨APIåŸºç¡€URL: {self.base_url}")
        logger.info(f"ä½¿ç”¨æ¨¡å‹: {self.model_name}")
        
        # ç¼“å­˜æ–‡ä»¶è·¯å¾„
        self.cache_file = Path("cache/openai_analysis_cache.pkl")
        self.cache_file.parent.mkdir(exist_ok=True)
        
        # æ•°æ®ç›®å½•
        self.data_dir = data_dir
        
        # åŠ è½½ç¼“å­˜
        self.analysis_cache = self._load_cache()
        
        logger.info(f"âœ… OpenAI {self.model_name} APIå®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ")
        
        # åˆ†ææç¤ºè¯æ¨¡æ¿
        self.analysis_prompt = """
ä½ æ˜¯ä¸€åèµ„æ·±çš„äººå·¥æ™ºèƒ½ä¸è®¡ç®—æœºç§‘å­¦é¢†åŸŸçš„å­¦æœ¯åˆ†æä¸“å®¶ï¼Œæ‹¥æœ‰æ·±åº¦çš„æŠ€æœ¯ç†è§£èƒ½åŠ›ã€‚
è¯·å¯¹ä»¥ä¸‹å­¦æœ¯è®ºæ–‡æ‘˜è¦è¿›è¡Œå…¨é¢è€Œç²¾ç¡®çš„æŠ€æœ¯åˆ†æã€‚

**è®ºæ–‡æ‘˜è¦**ï¼š
{abstract}

**åˆ†æè¦æ±‚**ï¼š
è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹ç»´åº¦è¿›è¡Œæ·±åº¦åˆ†æï¼Œç¡®ä¿è¾“å‡ºå†…å®¹çš„å‡†ç¡®æ€§å’Œä¸“ä¸šæ€§ï¼š

1. **æ ¸å¿ƒé—®é¢˜è¯†åˆ«** (problems)ï¼š
   - è®ºæ–‡è¯•å›¾è§£å†³çš„ä¸»è¦æŠ€æœ¯é—®é¢˜æˆ–æŒ‘æˆ˜
   - ç°æœ‰æ–¹æ³•çš„å±€é™æ€§æˆ–ä¸è¶³
   - ç ”ç©¶åŠ¨æœºå’Œé—®é¢˜èƒŒæ™¯
   
2. **æŠ€æœ¯æ–¹æ³•** (methods)ï¼š
   - ä½¿ç”¨çš„æ ¸å¿ƒç®—æ³•ã€æ¨¡å‹æˆ–æŠ€æœ¯æ¡†æ¶
   - åˆ›æ–°çš„æŠ€æœ¯æ‰‹æ®µæˆ–æ”¹è¿›æ–¹æ³•
   - å®éªŒè®¾è®¡å’Œè¯„ä¼°æ–¹æ³•
   
3. **åº”ç”¨é¢†åŸŸ** (domains)ï¼š
   - ä¸»è¦åº”ç”¨åœºæ™¯å’Œç›®æ ‡é¢†åŸŸ
   - æ½œåœ¨çš„åº”ç”¨æ‹“å±•æ–¹å‘
   - å®é™…åº”ç”¨ä»·å€¼
   
4. **æŠ€æœ¯å…³é”®è¯** (keywords)ï¼š
   - æå–5-8ä¸ªæœ€é‡è¦çš„æŠ€æœ¯æœ¯è¯­
   - åŒ…æ‹¬ç®—æ³•åç§°ã€æŠ€æœ¯æ¦‚å¿µã€è¯„ä¼°æŒ‡æ ‡ç­‰
   - ä¼˜å…ˆé€‰æ‹©å…·æœ‰ä»£è¡¨æ€§çš„ä¸“ä¸šæœ¯è¯­
   
5. **åˆ›æ–°è¯„åˆ†** (score)ï¼š
   - è¯„åˆ†æ ‡å‡†ï¼š1åˆ†(å¢é‡æ”¹è¿›) 2åˆ†(ä¸€èˆ¬æ”¹è¿›) 3åˆ†(æ˜¾è‘—æ”¹è¿›) 4åˆ†(é‡è¦çªç ´) 5åˆ†(å¼€åˆ›æ€§è´¡çŒ®)
   - åŸºäºæŠ€æœ¯æ–°é¢–æ€§ã€æ–¹æ³•åˆ›æ–°æ€§ã€æ€§èƒ½æå‡ã€å®ç”¨ä»·å€¼ç­‰ç»´åº¦ç»¼åˆè¯„ä¼°
   
6. **åˆ›æ–°ç‚¹æè¿°** (innovation)ï¼š
   - æ€»ç»“ä¸»è¦æŠ€æœ¯è´¡çŒ®å’Œåˆ›æ–°äº®ç‚¹
   - ä¸ç°æœ‰å·¥ä½œçš„å…³é”®åŒºåˆ«
   - å¯¹é¢†åŸŸå‘å±•çš„æ½œåœ¨å½±å“

**è¾“å‡ºæ ¼å¼**ï¼š
è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¿”å›ç»“æœï¼Œç¡®ä¿å­—æ®µåç§°å’Œç»“æ„å®Œå…¨åŒ¹é…ï¼š

{{
  "problems": ["å…·ä½“é—®é¢˜æè¿°1", "å…·ä½“é—®é¢˜æè¿°2"],
  "methods": ["æŠ€æœ¯æ–¹æ³•1", "æŠ€æœ¯æ–¹æ³•2", "æŠ€æœ¯æ–¹æ³•3"],
  "domains": ["åº”ç”¨é¢†åŸŸ1", "åº”ç”¨é¢†åŸŸ2"],
  "keywords": ["å…³é”®è¯1", "å…³é”®è¯2", "å…³é”®è¯3", "å…³é”®è¯4", "å…³é”®è¯5"],
  "score": æ•°å€¼,
  "innovation": "è¯¦ç»†çš„åˆ›æ–°ç‚¹æè¿°ï¼ŒåŒ…æ‹¬ä¸»è¦è´¡çŒ®å’ŒæŠ€æœ¯äº®ç‚¹"
}}

**æ³¨æ„äº‹é¡¹**ï¼š
- è¯·ç¡®ä¿åˆ†æå†…å®¹åŸºäºæ‘˜è¦çš„å®é™…å†…å®¹ï¼Œé¿å…è¿‡åº¦æ¨æµ‹
- æŠ€æœ¯æœ¯è¯­è¯·ä½¿ç”¨å‡†ç¡®çš„ä¸“ä¸šè¡¨è¿°
- åˆ›æ–°è¯„åˆ†éœ€è¦å®¢è§‚å…¬æ­£ï¼Œæœ‰åˆç†ä¾æ®
- å¦‚æœæ‘˜è¦ä¿¡æ¯ä¸è¶³ï¼Œè¯·åœ¨ç›¸åº”å­—æ®µä¸­æ ‡æ³¨"ä¿¡æ¯ä¸è¶³"
"""
        
        # arXivåˆ†ç±»æ˜ å°„
        self.category_mapping = {
            'cs.CV': 'è®¡ç®—æœºè§†è§‰',
            'cs.AI': 'äººå·¥æ™ºèƒ½',
            'cs.LG': 'æœºå™¨å­¦ä¹ ',
            'cs.CL': 'è®¡ç®—è¯­è¨€å­¦',
            'cs.RO': 'æœºå™¨äººå­¦',
            'cs.GR': 'è®¡ç®—æœºå›¾å½¢å­¦',
            'cs.IR': 'ä¿¡æ¯æ£€ç´¢',
            'cs.HC': 'äººæœºäº¤äº’',
            'cs.NE': 'ç¥ç»ä¸è¿›åŒ–è®¡ç®—',
            'cs.MA': 'å¤šæ™ºèƒ½ä½“ç³»ç»Ÿ',
            'cs.SE': 'è½¯ä»¶å·¥ç¨‹',
            'eess.IV': 'å›¾åƒä¸è§†é¢‘å¤„ç†',
            'eess.SP': 'ä¿¡å·å¤„ç†',
            'stat.ML': 'ç»Ÿè®¡æœºå™¨å­¦ä¹ ',
            'math.OC': 'ä¼˜åŒ–ä¸æ§åˆ¶'
        }
    
    def _load_cache(self) -> Dict:
        """
        åŠ è½½åˆ†æç¼“å­˜
        
        Returns:
            ç¼“å­˜å­—å…¸
        """
        try:
            if self.cache_file.exists():
                with open(self.cache_file, 'rb') as f:
                    cache = pickle.load(f)
                logger.info(f"å·²åŠ è½½ç¼“å­˜æ–‡ä»¶: {self.cache_file}")
                return cache
        except Exception as e:
            logger.warning(f"åŠ è½½ç¼“å­˜å¤±è´¥: {e}")
        
        return {}
    
    def _save_cache(self):
        """ä¿å­˜åˆ†æç¼“å­˜"""
        try:
            with open(self.cache_file, 'wb') as f:
                pickle.dump(self.analysis_cache, f)
            logger.debug("ç¼“å­˜å·²ä¿å­˜")
        except Exception as e:
            logger.warning(f"ä¿å­˜ç¼“å­˜å¤±è´¥: {e}")
        
    def load_jsonl_files(self, start_date: str = None, end_date: str = None, categories: List[str] = None) -> List[Dict]:
        """
        åŠ è½½æŒ‡å®šæ—¶é—´èŒƒå›´å’Œç±»åˆ«çš„jsonlæ–‡ä»¶
        
        Args:
            start_date: å¼€å§‹æ—¥æœŸ (YYYY-MM-DD)
            end_date: ç»“æŸæ—¥æœŸ (YYYY-MM-DD)
            categories: è¦ç­›é€‰çš„arXivç±»åˆ«åˆ—è¡¨ (å¦‚ ['cs.CV', 'cs.AI'])
            
        Returns:
            è®ºæ–‡æ•°æ®åˆ—è¡¨
        """
        papers = []
        
        # æ£€æŸ¥æ•°æ®ç›®å½•æ˜¯å¦å­˜åœ¨
        if not os.path.exists(self.data_dir):
            # å°è¯•ç›¸å¯¹äºå½“å‰è„šæœ¬çš„è·¯å¾„
            script_dir = os.path.dirname(os.path.abspath(__file__))
            parent_dir = os.path.dirname(script_dir)
            alt_data_dir = os.path.join(parent_dir, 'data')
            
            if os.path.exists(alt_data_dir):
                self.data_dir = alt_data_dir
                logger.info(f"ä½¿ç”¨æ›¿ä»£æ•°æ®ç›®å½•: {self.data_dir}")
            else:
                # æä¾›è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯
                current_dir = os.getcwd()
                possible_paths = [
                    os.path.abspath(self.data_dir),
                    alt_data_dir,
                    os.path.join(current_dir, 'data'),
                    os.path.join(current_dir, '..', 'data')
                ]
                
                error_msg = f"""
æ•°æ®ç›®å½•æœªæ‰¾åˆ°ï¼è¯·æ£€æŸ¥ä»¥ä¸‹å¯èƒ½çš„è·¯å¾„ï¼š
å½“å‰å·¥ä½œç›®å½•: {current_dir}
å°è¯•çš„è·¯å¾„:
{chr(10).join(f'  - {path} {"âœ“" if os.path.exists(path) else "âœ—"}' for path in possible_paths)}

è¯·ç¡®ä¿dataç›®å½•å­˜åœ¨å¹¶åŒ…å«JSONLæ–‡ä»¶ï¼Œæˆ–è€…ï¼š
1. åœ¨å½“å‰ç›®å½•åˆ›å»ºdataç›®å½•
2. è®¾ç½®æ­£ç¡®çš„data_dirå‚æ•°
3. ç¡®ä¿å·¥ä½œç›®å½•æ­£ç¡®
"""
                logger.error(error_msg)
                raise FileNotFoundError(error_msg)
        
        # è·å–æ‰€æœ‰jsonlæ–‡ä»¶
        try:
            jsonl_files = [f for f in os.listdir(self.data_dir) if f.endswith('.jsonl')]
            jsonl_files.sort()
        except Exception as e:
            raise FileNotFoundError(f"æ— æ³•è®¿é—®æ•°æ®ç›®å½• {self.data_dir}: {e}")
        
        for filename in jsonl_files:
            # æå–æ—¥æœŸ
            date_str = filename.replace('.jsonl', '')
            try:
                file_date = datetime.strptime(date_str, '%Y-%m-%d')
            except ValueError:
                continue
                
            # æ£€æŸ¥æ—¥æœŸèŒƒå›´
            if start_date:
                start_dt = datetime.strptime(start_date, '%Y-%m-%d')
                if file_date < start_dt:
                    continue
                    
            if end_date:
                end_dt = datetime.strptime(end_date, '%Y-%m-%d')
                if file_date > end_dt:
                    continue
            
            # åŠ è½½æ–‡ä»¶
            filepath = os.path.join(self.data_dir, filename)
            try:
                file_papers = []
                with open(filepath, 'r', encoding='utf-8') as f:
                    for line in f:
                        if line.strip():
                            paper = json.loads(line.strip())
                            paper['date'] = date_str
                            
                            # ç±»åˆ«ç­›é€‰
                            if categories:
                                paper_categories = paper.get('categories', [])
                                # æ£€æŸ¥è®ºæ–‡æ˜¯å¦å±äºæŒ‡å®šç±»åˆ«
                                if not any(cat in paper_categories for cat in categories):
                                    continue
                            
                            file_papers.append(paper)
                
                papers.extend(file_papers)
                logger.info(f"å·²åŠ è½½æ–‡ä»¶: {filename} ({len(file_papers)} ç¯‡è®ºæ–‡)")
            except Exception as e:
                logger.error(f"åŠ è½½æ–‡ä»¶ {filename} å¤±è´¥: {e}")
                
        # ç»Ÿè®¡ç±»åˆ«ä¿¡æ¯
        if categories:
            logger.info(f"æŒ‰ç±»åˆ«ç­›é€‰ååŠ è½½äº† {len(papers)} ç¯‡è®ºæ–‡ (ç­›é€‰ç±»åˆ«: {categories})")
        else:
            logger.info(f"æ€»å…±åŠ è½½äº† {len(papers)} ç¯‡è®ºæ–‡")
        
        return papers
    
    def get_available_categories(self, start_date: str = None, end_date: str = None) -> Dict[str, int]:
        """
        è·å–æ•°æ®ä¸­æ‰€æœ‰å¯ç”¨çš„arXivç±»åˆ«åŠå…¶è®ºæ–‡æ•°é‡
        
        Args:
            start_date: å¼€å§‹æ—¥æœŸ (YYYY-MM-DD)
            end_date: ç»“æŸæ—¥æœŸ (YYYY-MM-DD)
            
        Returns:
            ç±»åˆ«å­—å…¸ {ç±»åˆ«: è®ºæ–‡æ•°é‡}
        """
        papers = self.load_jsonl_files(start_date, end_date, categories=None)
        
        category_counts = defaultdict(int)
        for paper in papers:
            categories = paper.get('categories', [])
            for category in categories:
                category_counts[category] += 1
        
        # æŒ‰è®ºæ–‡æ•°é‡é™åºæ’åº
        sorted_categories = dict(sorted(category_counts.items(), key=lambda x: x[1], reverse=True))
        
        logger.info(f"æ‰¾åˆ° {len(sorted_categories)} ä¸ªä¸åŒçš„arXivç±»åˆ«")
        return sorted_categories
    
    def filter_papers_by_category(self, papers: List[Dict], categories: List[str]) -> List[Dict]:
        """
        æŒ‰ç±»åˆ«ç­›é€‰è®ºæ–‡
        
        Args:
            papers: è®ºæ–‡åˆ—è¡¨
            categories: è¦ç­›é€‰çš„ç±»åˆ«åˆ—è¡¨
            
        Returns:
            ç­›é€‰åçš„è®ºæ–‡åˆ—è¡¨
        """
        if not categories:
            return papers
        
        filtered_papers = []
        for paper in papers:
            paper_categories = paper.get('categories', [])
            if any(cat in paper_categories for cat in categories):
                filtered_papers.append(paper)
        
        logger.info(f"æŒ‰ç±»åˆ«ç­›é€‰: {len(papers)} -> {len(filtered_papers)} ç¯‡è®ºæ–‡")
        return filtered_papers
    
    def analyze_paper_with_gpt4o(self, abstract: str) -> Dict:
        """
        ä½¿ç”¨OpenAI APIå¤§æ¨¡å‹åˆ†æå•ç¯‡è®ºæ–‡æ‘˜è¦ - ä¼˜åŒ–ç‰ˆæœ¬
        
        Args:
            abstract: è®ºæ–‡æ‘˜è¦
            
        Returns:
            åˆ†æç»“æœå­—å…¸
        """
        if not abstract or len(abstract.strip()) < 50:
            return self._get_default_analysis()
            
        try:
            prompt = self.analysis_prompt.format(abstract=abstract[:2000])  # é™åˆ¶æ‘˜è¦é•¿åº¦ä»¥å‡å°‘å¤„ç†æ—¶é—´
            
            # ä½¿ç”¨OpenAIå®¢æˆ·ç«¯è°ƒç”¨API
            completion = self.client.chat.completions.create(
                model=self.model_name,
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä¸€åä¸“ä¸šçš„å­¦æœ¯åˆ†æä¸“å®¶ï¼Œè¯·ä¸¥æ ¼æŒ‰ç…§JSONæ ¼å¼è¿”å›åˆ†æç»“æœã€‚ä½ çš„å›å¤å¿…é¡»æ˜¯ä¸€ä¸ªæœ‰æ•ˆçš„JSONå¯¹è±¡ï¼Œä¸è¦åŒ…å«ä»»ä½•å…¶ä»–æ–‡æœ¬æˆ–è§£é‡Šã€‚"},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.1,  # é™ä½éšæœºæ€§ä»¥è·å¾—æ›´ä¸€è‡´çš„ç»“æœ
                max_tokens=800    # å‡å°‘tokenæ•°ä»¥åŠ å¿«å“åº”
            )
            
            result_data = completion.model_dump()
            
            # æ£€æŸ¥å“åº”æ ¼å¼
            if 'choices' not in result_data or not result_data['choices']:
                logger.error("APIå“åº”æ ¼å¼å¼‚å¸¸")
                return self._get_default_analysis()
                
            result_text = result_data['choices'][0]['message']['content'].strip()
            
            # å°è¯•è§£æJSON - æ”¹è¿›ç‰ˆæœ¬
            try:
                # æå–JSONéƒ¨åˆ†ï¼ˆå»é™¤å¯èƒ½çš„markdownæ ‡è®°ï¼‰
                if '```json' in result_text:
                    json_part = result_text.split('```json')[1].split('```')[0]
                elif '```' in result_text:
                    json_part = result_text.split('```')[1]
                elif '{' in result_text and '}' in result_text:
                    # æå–ç¬¬ä¸€ä¸ªå®Œæ•´çš„JSONå¯¹è±¡
                    start_idx = result_text.find('{')
                    end_idx = result_text.rfind('}') + 1
                    json_part = result_text[start_idx:end_idx]
                else:
                    json_part = result_text
                    
                result = json.loads(json_part.strip())
                
                # éªŒè¯å’Œæ¸…ç†å¢å¼ºç‰ˆåˆ†æç»“æœ
                cleaned_result = {}
                
                # ç›´æ¥ä½¿ç”¨ç®€åŒ–çš„å­—æ®µ
                cleaned_result['problems'] = result.get('problems', [])[:5]
                cleaned_result['methods'] = result.get('methods', [])[:5]
                cleaned_result['domains'] = result.get('domains', [])[:3]
                cleaned_result['keywords'] = result.get('keywords', [])[:10]
                cleaned_result['innovation'] = result.get('innovation', '')
                
                # éªŒè¯åˆ†æ•°
                score = result.get('score', 0)
                if isinstance(score, (int, float)) and 0 <= score <= 5:
                    cleaned_result['score'] = float(score)
                else:
                    cleaned_result['score'] = 0.0
                
                return cleaned_result
                
            except (json.JSONDecodeError, KeyError) as e:
                logger.warning(f"JSONè§£æå¤±è´¥ï¼Œå°è¯•ç®€åŒ–è§£æ: {e}")
                # å°è¯•ç®€åŒ–çš„æ–‡æœ¬è§£æ
                return self._parse_text_fallback(result_text)
                
        except Exception as e:
            logger.error(f"OpenAI APIåˆ†æå¤±è´¥: {e}")
            return self._get_default_analysis()
    
    def _parse_text_fallback(self, text: str) -> Dict:
        """
        æ–‡æœ¬è§£æå¤‡ç”¨æ–¹æ¡ˆ
        
        Args:
            text: APIè¿”å›çš„æ–‡æœ¬
            
        Returns:
            è§£æç»“æœ
        """
        result = self._get_default_analysis()
        
        try:
            # å°è¯•ä»æ–‡æœ¬ä¸­æå–ä¿¡æ¯
            text_lower = text.lower()
            
            # æå–å…³é”®è¯ï¼ˆç®€å•çš„å¯å‘å¼æ–¹æ³•ï¼‰
            keywords = []
            if 'å…³é”®è¯' in text or 'keyword' in text_lower:
                # æŸ¥æ‰¾å¯èƒ½çš„å…³é”®è¯åˆ—è¡¨
                lines = text.split('\n')
                for line in lines:
                    if 'å…³é”®è¯' in line or 'keyword' in line.lower():
                        # æå–å†’å·åçš„å†…å®¹
                        if ':' in line:
                            keyword_text = line.split(':', 1)[1].strip()
                            keywords = [k.strip() for k in keyword_text.split(',') if k.strip()][:5]
                        break
            
            if keywords:
                result['keywords'] = keywords
            
            # å°è¯•æå–åˆ†æ•°
            import re
            score_pattern = r'[åˆ†è¯„]\s*[ï¼š:]\s*([0-9.]+)'
            score_match = re.search(score_pattern, text)
            if score_match:
                try:
                    score = float(score_match.group(1))
                    if 0 <= score <= 5:
                        result['score'] = score
                except ValueError:
                    pass
                    
        except Exception as e:
            logger.debug(f"æ–‡æœ¬è§£æå¤‡ç”¨æ–¹æ¡ˆå¤±è´¥: {e}")
        
        return result
    
    def _get_default_analysis(self) -> Dict:
        """è¿”å›é»˜è®¤åˆ†æç»“æœ"""
        return {
            'problems': [],
            'methods': [],
            'domains': [],
            'score': 0.0,
            'keywords': [],
            'innovation': ''
        }
    
    def batch_analyze_papers(self, papers: List[Dict], max_workers: int = 10, 
                           batch_size: int = 50) -> List[Dict]:
        """
        æ‰¹é‡åˆ†æè®ºæ–‡ - ä¼˜åŒ–ç‰ˆæœ¬
        
        Args:
            papers: è®ºæ–‡åˆ—è¡¨
            max_workers: æœ€å¤§å¹¶å‘æ•° (æé«˜åˆ°10)
            batch_size: æ‰¹æ¬¡å¤§å° (å‡å°‘åˆ°50ä»¥æé«˜å¹¶å‘æ•ˆç‡)
            
        Returns:
            åˆ†æç»“æœåˆ—è¡¨
        """
        results = []
        
        # ä½¿ç”¨ç±»çš„ç¼“å­˜
        cache = self.analysis_cache.copy()
        
        # é¢„å¤„ç†ï¼šåˆ†ç¦»éœ€è¦åˆ†æçš„è®ºæ–‡å’Œå·²ç¼“å­˜çš„è®ºæ–‡
        papers_to_analyze = []
        cached_results = []
        
        for paper in papers:
            paper_id = paper.get('id', '')
            abstract = paper.get('summary', '').strip()
            
            if not abstract:
                # æ·»åŠ é»˜è®¤ç»“æœç»™æ²¡æœ‰æ‘˜è¦çš„è®ºæ–‡
                result = paper.copy()
                result.update(self._get_default_analysis())
                cached_results.append(result)
                continue
                
            # æ£€æŸ¥ç¼“å­˜
            if paper_id in cache:
                result = cache[paper_id].copy()
                result.update(paper)
                cached_results.append(result)
            else:
                papers_to_analyze.append(paper)
        
        total_papers = len(papers)
        cached_count = len(cached_results)
        analyze_count = len(papers_to_analyze)
        
        logger.info(f"æ€»è®ºæ–‡æ•°: {total_papers}, ç¼“å­˜å‘½ä¸­: {cached_count}, éœ€è¦åˆ†æ: {analyze_count}")
        
        # å¦‚æœæ‰€æœ‰è®ºæ–‡éƒ½å·²ç¼“å­˜
        if not papers_to_analyze:
            logger.info("æ‰€æœ‰è®ºæ–‡éƒ½å·²ç¼“å­˜ï¼Œç›´æ¥è¿”å›ç»“æœ")
            return cached_results
        
        # å¤šçº¿ç¨‹æ‰¹é‡åˆ†æ
        analyzed_results = []
        processed = 0
        
        # ä½¿ç”¨æ›´å¤§çš„çº¿ç¨‹æ± è¿›è¡Œå¹¶å‘å¤„ç†
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # æ‰¹æ¬¡æäº¤ä»»åŠ¡
            batch_futures = []
            
            for i in range(0, len(papers_to_analyze), batch_size):
                batch_papers = papers_to_analyze[i:i + batch_size]
                
                # ä¸ºæ¯ä¸ªæ‰¹æ¬¡ä¸­çš„æ¯ç¯‡è®ºæ–‡æäº¤å•ç‹¬çš„ä»»åŠ¡
                for paper in batch_papers:
                    abstract = paper.get('summary', '').strip()
                    future = executor.submit(self._analyze_single_paper_with_retry, paper, abstract)
                    batch_futures.append(future)
            
            # æ”¶é›†æ‰€æœ‰ç»“æœ
            for future in as_completed(batch_futures):
                try:
                    result = future.result()
                    if result:
                        analyzed_results.append(result)
                        
                        # æ›´æ–°ç¼“å­˜
                        paper_id = result.get('id', '')
                        if paper_id:
                            analysis_data = {
                                'problems': result.get('problems', []),
                                'methods': result.get('methods', []),
                                'domains': result.get('domains', []),
                                'score': result.get('score', 0.0),
                                'keywords': result.get('keywords', [])
                            }
                            cache[paper_id] = analysis_data
                        
                        processed += 1
                        if processed % 5 == 0:  # æ›´é¢‘ç¹çš„è¿›åº¦æŠ¥å‘Š
                            progress = (cached_count + processed) / total_papers * 100
                            logger.info(f"å·²å¤„ç† {cached_count + processed}/{total_papers} ç¯‡è®ºæ–‡ ({progress:.1f}%)")
                            
                except Exception as exc:
                    logger.error(f"æ‰¹æ¬¡å¤„ç†å¼‚å¸¸: {exc}")
                    processed += 1
        
        # åˆå¹¶æ‰€æœ‰ç»“æœ
        all_results = cached_results + analyzed_results
        
        # æ›´æ–°ç±»çš„ç¼“å­˜å¹¶ä¿å­˜
        try:
            self.analysis_cache.update(cache)
            self._save_cache()
            logger.info(f"ç¼“å­˜å·²æ›´æ–°ï¼ŒåŒ…å« {len(self.analysis_cache)} ä¸ªåˆ†æç»“æœ")
        except Exception as e:
            logger.error(f"ä¿å­˜ç¼“å­˜å¤±è´¥: {e}")
        
        logger.info(f"æ‰¹é‡åˆ†æå®Œæˆï¼Œå…±å¤„ç† {len(all_results)} ç¯‡è®ºæ–‡")
        return all_results
    
    def _analyze_single_paper_with_retry(self, paper: Dict, abstract: str, max_retries: int = 3) -> Dict:
        """
        åˆ†æå•ç¯‡è®ºæ–‡ï¼Œå¸¦é‡è¯•æœºåˆ¶
        
        Args:
            paper: è®ºæ–‡æ•°æ®
            abstract: è®ºæ–‡æ‘˜è¦
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
            
        Returns:
            åˆ†æç»“æœ
        """
        for attempt in range(max_retries):
            try:
                analysis = self.analyze_paper_with_gpt4o(abstract)
                
                # åˆå¹¶ç»“æœ
                result = paper.copy()
                result.update(analysis)
                return result
                
            except Exception as e:
                if attempt < max_retries - 1:
                    wait_time = (attempt + 1) * 0.5  # é€’å¢ç­‰å¾…æ—¶é—´
                    logger.warning(f"è®ºæ–‡ {paper.get('id', 'unknown')} åˆ†æå¤±è´¥ (å°è¯• {attempt + 1}/{max_retries}): {e}, {wait_time}ç§’åé‡è¯•")
                    time.sleep(wait_time)
                else:
                    logger.error(f"è®ºæ–‡ {paper.get('id', 'unknown')} åˆ†ææœ€ç»ˆå¤±è´¥: {e}")
                    # è¿”å›é»˜è®¤ç»“æœ
                    result = paper.copy()
                    result.update(self._get_default_analysis())
                    return result
        
        # å¦‚æœæ‰€æœ‰é‡è¯•éƒ½å¤±è´¥ï¼Œè¿”å›é»˜è®¤ç»“æœ
        result = paper.copy()
        result.update(self._get_default_analysis())
        return result
    
    def calculate_trends(self, analyzed_papers: List[Dict]) -> Dict:
        """
        è®¡ç®—è¶‹åŠ¿ç»Ÿè®¡
        
        Args:
            analyzed_papers: åˆ†æåçš„è®ºæ–‡åˆ—è¡¨
            
        Returns:
            è¶‹åŠ¿ç»Ÿè®¡ç»“æœ
        """
        trends = {
            'temporal_trends': defaultdict(lambda: defaultdict(list)),
            'category_trends': defaultdict(lambda: defaultdict(list)),
            'keyword_trends': defaultdict(lambda: defaultdict(int)),
            'method_trends': defaultdict(lambda: defaultdict(int)),
            'problem_trends': defaultdict(lambda: defaultdict(int)),
            'innovation_trends': defaultdict(list),
            'category_distribution': defaultdict(int),
            'daily_stats': defaultdict(lambda: {
                'total_papers': 0,
                'avg_innovation_score': 0.0,
                'top_keywords': [],
                'top_methods': [],
                'top_problems': []
            })
        }
        
        # æŒ‰æ—¥æœŸå’Œç±»åˆ«ç»Ÿè®¡
        for paper in analyzed_papers:
            date = paper.get('date', '')
            categories = paper.get('categories', [])
            keywords = paper.get('keywords', [])
            methods = paper.get('methods', [])
            problems = paper.get('problems', [])
            score = paper.get('score', 0.0)
            
            # å¤„ç†ç±»åˆ«
            main_category = categories[0] if categories else 'unknown'
            category_name = self.category_mapping.get(main_category, main_category)
            
            # æ—¶é—´è¶‹åŠ¿
            trends['temporal_trends'][date][category_name].append(paper)
            trends['innovation_trends'][date].append(score)
            
            # ç±»åˆ«è¶‹åŠ¿
            trends['category_trends'][category_name][date].append(paper)
            trends['category_distribution'][category_name] += 1
            
            # å…³é”®è¯è¶‹åŠ¿
            for keyword in keywords:
                if keyword:
                    trends['keyword_trends'][date][keyword] += 1
            
            # æ–¹æ³•è¶‹åŠ¿
            for method in methods:
                if method:
                    trends['method_trends'][date][method] += 1
            
            # é—®é¢˜è¶‹åŠ¿
            for problem in problems:
                if problem:
                    trends['problem_trends'][date][problem] += 1
            
            # æ¯æ—¥ç»Ÿè®¡
            daily_stat = trends['daily_stats'][date]
            daily_stat['total_papers'] += 1
            
            # æ›´æ–°å…³é”®è¯ã€æ–¹æ³•ã€é—®é¢˜ç»Ÿè®¡
            for keyword in keywords:
                if keyword:
                    daily_stat.setdefault('keywords', Counter())[keyword] += 1
            for method in methods:
                if method:
                    daily_stat.setdefault('methods', Counter())[method] += 1
            for problem in problems:
                if problem:
                    daily_stat.setdefault('problems', Counter())[problem] += 1
        
        # è®¡ç®—å¹³å‡åˆ›æ–°åˆ†æ•°å’ŒTopé¡¹ç›®
        for date, stats in trends['daily_stats'].items():
            scores = trends['innovation_trends'][date]
            if scores:
                stats['avg_innovation_score'] = np.mean(scores)
            
            # Topå…³é”®è¯ã€æ–¹æ³•ã€é—®é¢˜
            if 'keywords' in stats:
                stats['top_keywords'] = stats['keywords'].most_common(10)
            if 'methods' in stats:
                stats['top_methods'] = stats['methods'].most_common(10)
            if 'problems' in stats:
                stats['top_problems'] = stats['problems'].most_common(10)
        
        return dict(trends)
    
    def analyze_by_category(self, analyzed_papers: List[Dict], target_categories: List[str] = None) -> Dict:
        """
        æŒ‰ç±»åˆ«è¿›è¡Œæ·±åº¦åˆ†æ
        
        Args:
            analyzed_papers: åˆ†æåçš„è®ºæ–‡åˆ—è¡¨
            target_categories: ç›®æ ‡ç±»åˆ«åˆ—è¡¨ï¼ŒNoneè¡¨ç¤ºåˆ†ææ‰€æœ‰ç±»åˆ«
            
        Returns:
            æŒ‰ç±»åˆ«çš„åˆ†æç»“æœ
        """
        category_analysis = defaultdict(lambda: {
            'total_papers': 0,
            'avg_innovation_score': 0.0,
            'temporal_trend': defaultdict(int),
            'top_keywords': Counter(),
            'top_methods': Counter(),
            'top_problems': Counter(),
            'innovation_distribution': [],
            'daily_avg_scores': defaultdict(list),
            'paper_details': []
        })
        
        # ç­›é€‰è®ºæ–‡
        if target_categories:
            filtered_papers = self.filter_papers_by_category(analyzed_papers, target_categories)
        else:
            filtered_papers = analyzed_papers
        
        # æŒ‰ç±»åˆ«åˆ†ç»„åˆ†æ
        for paper in filtered_papers:
            categories = paper.get('categories', [])
            
            for category in categories:
                if target_categories and category not in target_categories:
                    continue
                    
                category_name = self.category_mapping.get(category, category)
                analysis = category_analysis[category_name]
                
                # åŸºç¡€ç»Ÿè®¡
                analysis['total_papers'] += 1
                
                # æ—¶é—´è¶‹åŠ¿
                date = paper.get('date', '')
                analysis['temporal_trend'][date] += 1
                
                # å…³é”®è¯ã€æ–¹æ³•ã€é—®é¢˜ç»Ÿè®¡
                for keyword in paper.get('keywords', []):
                    if keyword:
                        analysis['top_keywords'][keyword] += 1
                
                for method in paper.get('methods', []):
                    if method:
                        analysis['top_methods'][method] += 1
                
                for problem in paper.get('problems', []):
                    if problem:
                        analysis['top_problems'][problem] += 1
                
                # åˆ›æ–°åˆ†æ•°
                score = paper.get('score', 0.0)
                analysis['innovation_distribution'].append(score)
                analysis['daily_avg_scores'][date].append(score)
                
                # è®ºæ–‡è¯¦æƒ…
                analysis['paper_details'].append({
                    'id': paper.get('id', ''),
                    'title': paper.get('title', ''),
                    'date': date,
                    'score': score,
                    'categories': categories,
                    'keywords': paper.get('keywords', []),
                    'methods': paper.get('methods', []),
                    'problems': paper.get('problems', [])
                })
        
        # è®¡ç®—ç»Ÿè®¡æ•°æ®
        for category_name, analysis in category_analysis.items():
            if analysis['innovation_distribution']:
                analysis['avg_innovation_score'] = np.mean(analysis['innovation_distribution'])
                analysis['std_innovation_score'] = np.std(analysis['innovation_distribution'])
                analysis['min_innovation_score'] = min(analysis['innovation_distribution'])
                analysis['max_innovation_score'] = max(analysis['innovation_distribution'])
            
            # è®¡ç®—æ¯æ—¥å¹³å‡åˆ†æ•°
            daily_scores = {}
            for date, scores in analysis['daily_avg_scores'].items():
                if scores:
                    daily_scores[date] = np.mean(scores)
            analysis['daily_avg_scores'] = daily_scores
            
            # è½¬æ¢Counterä¸ºæ™®é€šå­—å…¸ï¼Œä¾¿äºJSONåºåˆ—åŒ–
            analysis['top_keywords'] = dict(analysis['top_keywords'].most_common(20))
            analysis['top_methods'] = dict(analysis['top_methods'].most_common(15))
            analysis['top_problems'] = dict(analysis['top_problems'].most_common(15))
            analysis['temporal_trend'] = dict(analysis['temporal_trend'])
        
        return dict(category_analysis)
    
    def compare_categories(self, analyzed_papers: List[Dict], categories: List[str]) -> Dict:
        """
        æ¯”è¾ƒä¸åŒç±»åˆ«ä¹‹é—´çš„å·®å¼‚
        
        Args:
            analyzed_papers: åˆ†æåçš„è®ºæ–‡åˆ—è¡¨
            categories: è¦æ¯”è¾ƒçš„ç±»åˆ«åˆ—è¡¨
            
        Returns:
            ç±»åˆ«æ¯”è¾ƒç»“æœ
        """
        comparison = {
            'categories': categories,
            'comparison_metrics': {},
            'category_details': {}
        }
        
        category_analysis = self.analyze_by_category(analyzed_papers, categories)
        
        # æ¯”è¾ƒæŒ‡æ ‡
        metrics = ['total_papers', 'avg_innovation_score', 'std_innovation_score']
        
        for metric in metrics:
            comparison['comparison_metrics'][metric] = {}
            for category in categories:
                category_name = self.category_mapping.get(category, category)
                if category_name in category_analysis:
                    value = category_analysis[category_name].get(metric, 0)
                    comparison['comparison_metrics'][metric][category_name] = value
        
        # è¯¦ç»†æ•°æ®
        comparison['category_details'] = category_analysis
        
        # è®¡ç®—ç›¸å…³æ€§å’Œå·®å¼‚
        if len(categories) >= 2:
            scores_by_category = {}
            for category in categories:
                category_name = self.category_mapping.get(category, category)
                if category_name in category_analysis:
                    scores_by_category[category_name] = category_analysis[category_name]['innovation_distribution']
            
            # ç»Ÿè®¡æ˜¾è‘—æ€§æµ‹è¯•å¯ä»¥åœ¨è¿™é‡Œæ·»åŠ 
            comparison['statistical_summary'] = {
                'total_categories_compared': len(scores_by_category),
                'categories_with_data': list(scores_by_category.keys())
            }
        
        return comparison
    
    def save_analysis_results(self, analyzed_papers: List[Dict], 
                            trends: Dict, output_dir: str = "analysis_results"):
        """
        ä¿å­˜åˆ†æç»“æœ
        
        Args:
            analyzed_papers: åˆ†æåçš„è®ºæ–‡åˆ—è¡¨
            trends: è¶‹åŠ¿ç»Ÿè®¡ç»“æœ
            output_dir: è¾“å‡ºç›®å½•
        """
        os.makedirs(output_dir, exist_ok=True)
        
        # ä¿å­˜åˆ†æåçš„è®ºæ–‡æ•°æ®
        papers_file = os.path.join(output_dir, 'analyzed_papers.json')
        with open(papers_file, 'w', encoding='utf-8') as f:
            json.dump(analyzed_papers, f, ensure_ascii=False, indent=2)
        
        # ä¿å­˜è¶‹åŠ¿æ•°æ®
        trends_file = os.path.join(output_dir, 'trends_analysis.json')
        
        # å°†defaultdictè½¬æ¢ä¸ºæ™®é€šdictä»¥ä¾¿JSONåºåˆ—åŒ–
        def convert_defaultdict(obj):
            if isinstance(obj, defaultdict):
                return dict(obj)
            elif isinstance(obj, dict):
                return {k: convert_defaultdict(v) for k, v in obj.items()}
            elif isinstance(obj, list):
                return [convert_defaultdict(item) for item in obj]
            elif isinstance(obj, Counter):
                return dict(obj)
            else:
                return obj
        
        trends_serializable = convert_defaultdict(trends)
        
        with open(trends_file, 'w', encoding='utf-8') as f:
            json.dump(trends_serializable, f, ensure_ascii=False, indent=2)
        
        # åˆ›å»ºCSVç»Ÿè®¡æŠ¥å‘Š
        self._create_csv_reports(analyzed_papers, trends, output_dir)
        
        logger.info(f"åˆ†æç»“æœå·²ä¿å­˜åˆ° {output_dir} ç›®å½•")
    
    def _create_csv_reports(self, analyzed_papers: List[Dict], 
                          trends: Dict, output_dir: str):
        """åˆ›å»ºCSVæ ¼å¼çš„ç»Ÿè®¡æŠ¥å‘Š"""
        
        # 1. æ¯æ—¥è¶‹åŠ¿æŠ¥å‘Š
        daily_data = []
        for date, stats in trends['daily_stats'].items():
            daily_data.append({
                'date': date,
                'total_papers': stats['total_papers'],
                'avg_innovation_score': stats['avg_innovation_score'],
                'top_keywords': ', '.join([k for k, _ in stats.get('top_keywords', [])[:5]]),
                'top_methods': ', '.join([k for k, _ in stats.get('top_methods', [])[:5]]),
                'top_problems': ', '.join([k for k, _ in stats.get('top_problems', [])[:5]])
            })
        
        df_daily = pd.DataFrame(daily_data)
        df_daily.to_csv(os.path.join(output_dir, 'daily_trends.csv'), 
                       index=False, encoding='utf-8')
        
        # 2. ç±»åˆ«åˆ†å¸ƒæŠ¥å‘Š
        category_data = []
        for category, count in trends['category_distribution'].items():
            category_data.append({
                'category': category,
                'paper_count': count,
                'percentage': count / len(analyzed_papers) * 100
            })
        
        df_category = pd.DataFrame(category_data)
        df_category = df_category.sort_values('paper_count', ascending=False)
        df_category.to_csv(os.path.join(output_dir, 'category_distribution.csv'), 
                          index=False, encoding='utf-8')
        
        # 3. å…³é”®è¯è¶‹åŠ¿æŠ¥å‘Š
        keyword_trends_data = []
        for date, keywords in trends['keyword_trends'].items():
            for keyword, count in keywords.items():
                keyword_trends_data.append({
                    'date': date,
                    'keyword': keyword,
                    'count': count
                })
        
        df_keywords = pd.DataFrame(keyword_trends_data)
        df_keywords.to_csv(os.path.join(output_dir, 'keyword_trends.csv'), 
                          index=False, encoding='utf-8')
        
        logger.info("CSVæŠ¥å‘Šå·²ç”Ÿæˆ")

    def run_full_analysis(self, start_date: str = None, end_date: str = None,
                         max_workers: int = 15, categories: List[str] = None,
                         include_category_analysis: bool = True) -> Tuple[List[Dict], Dict]:
        """
        è¿è¡Œå®Œæ•´çš„åˆ†ææµç¨‹
        
        Args:
            start_date: å¼€å§‹æ—¥æœŸ (YYYY-MM-DD)
            end_date: ç»“æŸæ—¥æœŸ (YYYY-MM-DD)
            max_workers: æœ€å¤§å¹¶å‘æ•°
            categories: è¦åˆ†æçš„arXivç±»åˆ«åˆ—è¡¨
            include_category_analysis: æ˜¯å¦åŒ…å«è¯¦ç»†çš„ç±»åˆ«åˆ†æ
            
        Returns:
            (åˆ†æåçš„è®ºæ–‡åˆ—è¡¨, è¶‹åŠ¿ç»Ÿè®¡ç»“æœ)
        """
        logger.info("å¼€å§‹è®ºæ–‡è¶‹åŠ¿åˆ†æ...")
        
        # 1. åŠ è½½æ•°æ®
        logger.info("æ­£åœ¨åŠ è½½è®ºæ–‡æ•°æ®...")
        papers = self.load_jsonl_files(start_date, end_date, categories)
        
        if not papers:
            logger.warning("æ²¡æœ‰æ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„è®ºæ–‡æ•°æ®")
            return [], {}
        
        # 2. æ‰¹é‡åˆ†æ
        logger.info("å¼€å§‹ä½¿ç”¨OpenAI/gpt-4oå¤§æ¨¡å‹åˆ†æè®ºæ–‡...")
        analyzed_papers = self.batch_analyze_papers(papers, max_workers)
        
        # 3. è®¡ç®—åŸºç¡€è¶‹åŠ¿
        logger.info("æ­£åœ¨è®¡ç®—è¶‹åŠ¿ç»Ÿè®¡...")
        trends = self.calculate_trends(analyzed_papers)
        
        # 4. æ·»åŠ ç±»åˆ«åˆ†æï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if include_category_analysis:
            logger.info("è¿›è¡Œè¯¦ç»†ç±»åˆ«åˆ†æ...")
            trends['category_analysis'] = self.analyze_by_category(analyzed_papers, categories)
            
            # å¦‚æœæŒ‡å®šäº†å¤šä¸ªç±»åˆ«ï¼Œè¿›è¡Œæ¯”è¾ƒåˆ†æ
            if categories and len(categories) > 1:
                logger.info("è¿›è¡Œç±»åˆ«æ¯”è¾ƒåˆ†æ...")
                trends['category_comparison'] = self.compare_categories(analyzed_papers, categories)
        
        # 5. ä¿å­˜ç»“æœ
        logger.info("æ­£åœ¨ä¿å­˜åˆ†æç»“æœ...")
        self.save_analysis_results(analyzed_papers, trends)
        
        logger.info("OpenAI/gpt-4oè®ºæ–‡è¶‹åŠ¿åˆ†æå®Œæˆï¼")
        return analyzed_papers, trends
    
    def run_category_focused_analysis(self, categories: List[str], start_date: str = None, 
                                     end_date: str = None, max_workers: int = 15) -> Dict:
        """
        è¿è¡Œä¸“æ³¨äºç‰¹å®šç±»åˆ«çš„åˆ†æ
        
        Args:
            categories: è¦åˆ†æçš„arXivç±»åˆ«åˆ—è¡¨
            start_date: å¼€å§‹æ—¥æœŸ (YYYY-MM-DD)
            end_date: ç»“æŸæ—¥æœŸ (YYYY-MM-DD)
            max_workers: æœ€å¤§å¹¶å‘æ•°
            
        Returns:
            ç±»åˆ«åˆ†æç»“æœ
        """
        logger.info(f"å¼€å§‹é’ˆå¯¹ç±»åˆ« {categories} çš„ä¸“é¡¹åˆ†æ...")
        
        # è·å–å¯ç”¨ç±»åˆ«ä¿¡æ¯
        available_categories = self.get_available_categories(start_date, end_date)
        
        # éªŒè¯è¯·æ±‚çš„ç±»åˆ«æ˜¯å¦å­˜åœ¨
        valid_categories = [cat for cat in categories if cat in available_categories]
        if not valid_categories:
            logger.warning(f"æŒ‡å®šçš„ç±»åˆ« {categories} åœ¨æ•°æ®ä¸­æœªæ‰¾åˆ°")
            return {
                'error': 'No valid categories found',
                'available_categories': available_categories
            }
        
        # è¿è¡Œå®Œæ•´åˆ†æ
        analyzed_papers, trends = self.run_full_analysis(
            start_date=start_date,
            end_date=end_date,
            max_workers=max_workers,
            categories=valid_categories,
            include_category_analysis=True
        )
        
        # è¿”å›ç±»åˆ«ä¸“é¡¹ç»“æœ
        return {
            'analyzed_papers': analyzed_papers,
            'basic_trends': trends,
            'category_analysis': trends.get('category_analysis', {}),
            'category_comparison': trends.get('category_comparison', {}),
            'available_categories': available_categories,
            'analyzed_categories': valid_categories
        }

    def summarize_trends_with_gpt4o(self, trends: Dict, categories: List[str] = None, start_date: str = None, end_date: str = None) -> str:
        """
        ä½¿ç”¨GPT-4oå¯¹ç»Ÿè®¡ç»“æœè¿›è¡Œæ™ºèƒ½æ€»ç»“
        Args:
            trends: ç»Ÿè®¡ç»“æœå­—å…¸
            categories: ç”¨æˆ·é€‰æ‹©çš„ç±»åˆ«
            start_date: èµ·å§‹æ—¥æœŸ
            end_date: ç»“æŸæ—¥æœŸ
        Returns:
            æ™ºèƒ½æ€»ç»“æ–‡æœ¬
        """
        # æ„å»ºç»Ÿè®¡æ‘˜è¦
        total_papers = sum(trends.get('category_distribution', {}).values())
        avg_score = 0.0
        if 'innovation_trends' in trends and trends['innovation_trends']:
            all_scores = []
            for v in trends['innovation_trends'].values():
                all_scores.extend(v)
            if all_scores:
                avg_score = float(np.mean(all_scores))
        top_keywords = []
        if 'keyword_trends' in trends and trends['keyword_trends']:
            keyword_counter = Counter()
            for day in trends['keyword_trends']:
                keyword_counter.update(trends['keyword_trends'][day])
            top_keywords = [k for k, v in keyword_counter.most_common(10)]
        # æ„å»ºä¼˜åŒ–çš„prompt
        date_range = f"{start_date or 'æœ€è¿‘'} åˆ° {end_date or 'å½“å‰'}" if start_date or end_date else "å½“å‰æ—¶é—´æ®µ"
        category_info = f"ï¼Œèšç„¦ç±»åˆ«ï¼š{', '.join(categories)}" if categories else ""
        
        prompt = f"""
ä½ æ˜¯ä¸€åé¡¶çº§çš„å­¦æœ¯ç ”ç©¶è¶‹åŠ¿åˆ†æä¸“å®¶ï¼Œè¯·åŸºäºä»¥ä¸‹ç»Ÿè®¡æ•°æ®å¯¹ {date_range}{category_info} çš„å­¦æœ¯è®ºæ–‡è¿›è¡Œæ·±åº¦è¶‹åŠ¿åˆ†æã€‚

**ç»Ÿè®¡æ•°æ®æ‘˜è¦**ï¼š
ğŸ“Š è®ºæ–‡æ€»æ•°ï¼š{total_papers}
ğŸ† å¹³å‡åˆ›æ–°åˆ†æ•°ï¼š{avg_score:.2f}/5.0
ğŸ”¥ çƒ­é—¨å…³é”®è¯ï¼š{', '.join(top_keywords[:10]) if top_keywords else 'æš‚æ— æ•°æ®'}
ğŸ“ˆ ç±»åˆ«åˆ†å¸ƒï¼š{dict(list(trends.get('category_distribution', {}).items())[:8])}

**åˆ†æè¦æ±‚**ï¼š
è¯·ä»ä»¥ä¸‹ç»´åº¦è¿›è¡Œä¸“ä¸šåˆ†æï¼Œè¾“å‡ºç»“æ„åŒ–çš„ä¸­æ–‡æŠ¥å‘Šï¼š

1. **æ€»ä½“è¶‹åŠ¿æ¦‚è§ˆ**
   - ç ”ç©¶æ´»è·ƒåº¦è¯„ä¼°
   - åˆ›æ–°æ°´å¹³æ€»ä½“ç‰¹å¾
   - è·¨é¢†åŸŸèåˆæƒ…å†µ

2. **æŠ€æœ¯çƒ­ç‚¹è¯†åˆ«**
   - æ ¸å¿ƒæŠ€æœ¯å…³é”®è¯åˆ†æ
   - æ–°å…´æŠ€æœ¯æ–¹å‘è¯†åˆ«
   - æŠ€æœ¯æˆç†Ÿåº¦è¯„ä¼°

3. **ç ”ç©¶ç„¦ç‚¹åˆ†æ**
   - ä¸»è¦ç ”ç©¶é—®é¢˜å’ŒæŒ‘æˆ˜
   - è§£å†³æ–¹æ¡ˆçš„æŠ€æœ¯è·¯å¾„
   - åº”ç”¨åœºæ™¯çš„æ‹“å±•æƒ…å†µ

4. **åˆ›æ–°äº®ç‚¹æ€»ç»“**
   - é«˜åˆ†è®ºæ–‡çš„å…±åŒç‰¹å¾
   - çªç ´æ€§æŠ€æœ¯æˆ–æ–¹æ³•
   - å…·æœ‰å½±å“åŠ›çš„ç ”ç©¶æ–¹å‘

5. **æœªæ¥å‘å±•é¢„æµ‹**
   - åŸºäºå½“å‰è¶‹åŠ¿çš„å‘å±•é¢„æµ‹
   - æ½œåœ¨çš„ç ”ç©¶æœºä¼š
   - æŠ€æœ¯å‘å±•çš„å¯èƒ½æ–¹å‘

**è¾“å‡ºæ ¼å¼**ï¼š
è¯·ç”¨ä¸“ä¸šä¸”æ˜“æ‡‚çš„ä¸­æ–‡æ’°å†™åˆ†ææŠ¥å‘Šï¼Œæ¯ä¸ªç»´åº¦ç”¨æ¸…æ™°çš„æ®µè½ç»„ç»‡ï¼Œé€‚å½“ä½¿ç”¨è¡¨æƒ…ç¬¦å·å¢å¼ºå¯è¯»æ€§ã€‚
æŠ¥å‘Šåº”å½“å®¢è§‚ã€å‡†ç¡®ï¼ŒåŸºäºæ•°æ®å¾—å‡ºç»“è®ºï¼Œé¿å…è¿‡åº¦æ¨æµ‹ã€‚
"""
        
        # å°è¯•ä¸åŒçš„æ¨¡å‹åç§°ï¼Œä»¥é€‚é…ä¸åŒçš„APIç«¯ç‚¹
        models_to_try = [
            self.model_name,  # å½“å‰é…ç½®çš„æ¨¡å‹
            "gpt-4o",         # æ ‡å‡†OpenAIæ¨¡å‹å
            "gpt-4",          # å¤‡ç”¨æ¨¡å‹
            "gpt-3.5-turbo"   # æœ€åå¤‡ç”¨
        ]
        
        for model in models_to_try:
            try:
                logger.info(f"å°è¯•ä½¿ç”¨æ¨¡å‹: {model}")
                # è°ƒç”¨å¤§æ¨¡å‹
                response = self.client.chat.completions.create(
                    model=model,
                    messages=[
                        {"role": "system", "content": "ä½ æ˜¯ä¸€åèµ„æ·±çš„å­¦æœ¯ç ”ç©¶è¶‹åŠ¿åˆ†æä¸“å®¶ï¼Œå…·æœ‰æ·±åšçš„äººå·¥æ™ºèƒ½ã€è®¡ç®—æœºç§‘å­¦å’Œè·¨å­¦ç§‘ç ”ç©¶èƒŒæ™¯ã€‚ä½ æ“…é•¿ä»å¤§é‡ç»Ÿè®¡æ•°æ®ä¸­æç‚¼æœ‰ä»·å€¼çš„è¶‹åŠ¿æ´å¯Ÿï¼Œèƒ½å¤Ÿè¯†åˆ«æŠ€æœ¯å‘å±•çš„è§„å¾‹å’Œæœªæ¥æ–¹å‘ï¼Œå¹¶ç”¨æ¸…æ™°ä¸“ä¸šçš„è¯­è¨€è¡¨è¾¾å¤æ‚çš„å­¦æœ¯è§‚ç‚¹ã€‚"},
                        {"role": "user", "content": prompt}
                    ],
                    max_tokens=800,
                    temperature=0.7
                )
                logger.info(f"æˆåŠŸä½¿ç”¨æ¨¡å‹ {model} ç”Ÿæˆæ€»ç»“")
                return response.choices[0].message.content.strip()
                
            except Exception as e:
                logger.warning(f"ä½¿ç”¨æ¨¡å‹ {model} å¤±è´¥: {e}")
                if model == models_to_try[-1]:  # æœ€åä¸€ä¸ªæ¨¡å‹ä¹Ÿå¤±è´¥äº†
                    error_msg = f"""
æ™ºèƒ½æ€»ç»“ç”Ÿæˆå¤±è´¥ã€‚å¯èƒ½çš„åŸå› ï¼š
1. APIé…ç½®é—®é¢˜ - è¯·æ£€æŸ¥ .env æ–‡ä»¶ä¸­çš„é…ç½®
2. æ¨¡å‹ä¸å¯ç”¨ - å½“å‰APIç«¯ç‚¹å¯èƒ½ä¸æ”¯æŒæŒ‡å®šçš„æ¨¡å‹
3. APIé¢åº¦ä¸è¶³ - è¯·æ£€æŸ¥APIè´¦æˆ·ä½™é¢

å½“å‰é…ç½®ï¼š
- APIåŸºç¡€URL: {self.base_url}
- å°è¯•çš„æ¨¡å‹: {models_to_try}

å»ºè®®è§£å†³æ–¹æ¡ˆï¼š
1. ç¡®è®¤ .env æ–‡ä»¶åŒ…å«æ­£ç¡®çš„ OPENAI_API_KEY å’Œ OPENAI_BASE_URL
2. å¦‚æœä½¿ç”¨ç¬¬ä¸‰æ–¹APIï¼Œè¯·ç¡®è®¤æ”¯æŒçš„æ¨¡å‹åç§°
3. å¯ä»¥å°è¯•ç›´æ¥ä½¿ç”¨ 'gpt-3.5-turbo' æˆ– 'gpt-4' ç­‰æ ‡å‡†æ¨¡å‹å

æœ€åä¸€ä¸ªé”™è¯¯: {e}
"""
                    raise Exception(error_msg)
                continue
        
        # ç†è®ºä¸Šä¸ä¼šåˆ°è¾¾è¿™é‡Œï¼Œä½†ä¸ºäº†å®‰å…¨èµ·è§
        return "æŠ±æ­‰ï¼Œæ™ºèƒ½æ€»ç»“ç”Ÿæˆå¤±è´¥ï¼Œè¯·æ£€æŸ¥APIé…ç½®ã€‚"






def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='OpenAI/gpt-4oè®ºæ–‡è¶‹åŠ¿åˆ†æ - æ”¯æŒç±»åˆ«ç­›é€‰')
    parser.add_argument('--start-date', help='å¼€å§‹æ—¥æœŸ (YYYY-MM-DD)')
    parser.add_argument('--end-date', help='ç»“æŸæ—¥æœŸ (YYYY-MM-DD)')
    parser.add_argument('--api-key', help='OpenAI API Key')
    parser.add_argument('--data-dir', default='data', help='æ•°æ®ç›®å½•')
    parser.add_argument('--max-workers', type=int, default=15, help='æœ€å¤§å¹¶å‘æ•°')
    parser.add_argument('--categories', nargs='+', help='è¦åˆ†æçš„arXivç±»åˆ« (å¦‚: cs.CV cs.AI cs.LG)')
    parser.add_argument('--list-categories', action='store_true', help='åˆ—å‡ºæ‰€æœ‰å¯ç”¨ç±»åˆ«')
    parser.add_argument('--category-only', action='store_true', help='åªè¿›è¡Œç±»åˆ«ä¸“é¡¹åˆ†æ')
    parser.add_argument('--summarize-trends', action='store_true', help='ç”¨å¤§æ¨¡å‹å¯¹ç»Ÿè®¡ç»“æœè¿›è¡Œæ™ºèƒ½æ€»ç»“')
    
    args = parser.parse_args()
    
    try:
        # åˆ›å»ºåˆ†æå™¨
        analyzer = GPT4oTrendAnalyzer(
            api_key=args.api_key,
            data_dir=args.data_dir
        )
        
               
        # å¦‚æœåªæ˜¯åˆ—å‡ºç±»åˆ«
        if args.list_categories:
            print("ğŸ“Š è·å–å¯ç”¨ç±»åˆ«...")
            categories = analyzer.get_available_categories(args.start_date, args.end_date)
            print(f"\nğŸ“ˆ æ‰¾åˆ° {len(categories)} ä¸ªç±»åˆ«:\n")
            
            for i, (category, count) in enumerate(categories.items(), 1):
                category_name = analyzer.category_mapping.get(category, category)
                print(f"{i:3d}. {category:12s} ({category_name:20s}): {count:4d} ç¯‡")
            return
        
        # ç±»åˆ«ä¸“é¡¹åˆ†æ
        if args.category_only and args.categories:
            print(f"ğŸ¯ è¿›è¡Œç±»åˆ«ä¸“é¡¹åˆ†æ: {args.categories}")
            result = analyzer.run_category_focused_analysis(
                categories=args.categories,
                start_date=args.start_date,
                end_date=args.end_date,
                max_workers=args.max_workers
            )
            
            if 'error' in result:
                print(f"âŒ {result['error']}")
                print("å¯ç”¨ç±»åˆ«:", list(result['available_categories'].keys())[:10])
                return
            
            print(f"\nâœ… ç±»åˆ«åˆ†æå®Œæˆï¼")
            print(f"åˆ†æè®ºæ–‡: {len(result['analyzed_papers'])} ç¯‡")
            print(f"åˆ†æç±»åˆ«: {result['analyzed_categories']}")
            
            # æ˜¾ç¤ºå„ç±»åˆ«è¯¦æƒ…
            for category_name, analysis in result['category_analysis'].items():
                print(f"\nğŸ“Š {category_name}:")
                print(f"  è®ºæ–‡æ•°é‡: {analysis['total_papers']}")
                print(f"  å¹³å‡åˆ›æ–°åˆ†æ•°: {analysis['avg_innovation_score']:.2f}")
                print(f"  çƒ­é—¨å…³é”®è¯: {list(analysis['top_keywords'].keys())[:5]}")
                print(f"  ä¸»è¦æ–¹æ³•: {list(analysis['top_methods'].keys())[:3]}")
        
        else:
            # å¸¸è§„åˆ†æï¼ˆå¯é€‰æ‹©æ€§ç­›é€‰ç±»åˆ«ï¼‰
            print("ğŸš€ å¼€å§‹åˆ†æ...")
            if args.categories:
                print(f"ç­›é€‰ç±»åˆ«: {args.categories}")
            
            analyzed_papers, trends = analyzer.run_full_analysis(
                start_date=args.start_date,
                end_date=args.end_date,
                max_workers=args.max_workers,
                categories=args.categories
            )
            
            print(f"\nâœ… åˆ†æå®Œæˆï¼")
            print(f"å¤„ç†è®ºæ–‡: {len(analyzed_papers)} ç¯‡")
            print(f"æ¶‰åŠç±»åˆ«: {len(trends['category_distribution'])} ä¸ª")
            
            # æ˜¾ç¤ºç±»åˆ«åˆ†å¸ƒ
            print(f"\nğŸ“ˆ ç±»åˆ«åˆ†å¸ƒ (å‰10):")
            category_dist = trends['category_distribution']
            for i, (category, count) in enumerate(list(category_dist.items())[:10], 1):
                percentage = count / len(analyzed_papers) * 100
                print(f"{i:2d}. {category:20s}: {count:4d} ç¯‡ ({percentage:5.1f}%)")
            
            # å¦‚æœæœ‰ç±»åˆ«åˆ†æç»“æœ
            if 'category_analysis' in trends:
                print(f"\nğŸ”¬ è¯¦ç»†ç±»åˆ«åˆ†æå·²å®Œæˆ")
                
            if 'category_comparison' in trends:
                print(f"ğŸ“Š ç±»åˆ«æ¯”è¾ƒåˆ†æå·²å®Œæˆ")
            
            print(f"\nğŸ’¾ è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ° analysis_results ç›®å½•")
            
            if args.summarize_trends:
                print("\nğŸ¤– æ­£åœ¨ç”¨å¤§æ¨¡å‹æ™ºèƒ½æ€»ç»“è¶‹åŠ¿...")
                summary = analyzer.summarize_trends_with_gpt4o(
                    trends,
                    categories=args.categories,
                    start_date=args.start_date,
                    end_date=args.end_date
                )
                print("\nğŸ“‹ æ™ºèƒ½æ€»ç»“ï¼š\n" + summary)
        
    except Exception as e:
        logger.error(f"åˆ†æè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()