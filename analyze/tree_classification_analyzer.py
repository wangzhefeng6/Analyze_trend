#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è®ºæ–‡æ ‘å½¢åˆ†ç±»åˆ†æç³»ç»Ÿ
åŸºäºä¸¤æ­¥éª¤çš„ç»†ç²’åº¦åˆ†ç±»åˆ†æï¼š
1. ç¬¬ä¸€æ­¥ï¼šä½¿ç”¨å¤§æ¨¡å‹å¯¹è®ºæ–‡è¿›è¡Œç»†åˆ†åˆ†ç±»ï¼Œä¿å­˜åˆ†ç±»è·¯å¾„
2. ç¬¬äºŒæ­¥ï¼šæ ¹æ®åˆ†ç±»è·¯å¾„èšåˆåˆ°å¶å­èŠ‚ç‚¹ï¼Œåˆ†æç»Ÿè®¡æ¯ä¸ªå¶å­èŠ‚ç‚¹
"""

import json
import os
import sys
import logging
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from collections import defaultdict, Counter
from typing import Dict, List, Any, Tuple, Set, Optional
from openai import OpenAI
import time
from concurrent.futures import ThreadPoolExecutor, as_completed
import pickle
from pathlib import Path
from dotenv import load_dotenv
from dataclasses import dataclass, asdict
from threading import Lock
import re

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('tree_classification.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# å¯¼å…¥åŠ¨æ€è¯­ä¹‰åˆå¹¶å™¨
try:
    from dynamic_semantic_merger import DynamicSemanticMerger
except ImportError:
    DynamicSemanticMerger = None
    print("âš ï¸  è­¦å‘Š: æ— æ³•å¯¼å…¥åŠ¨æ€è¯­ä¹‰åˆå¹¶å™¨ï¼Œå°†ä½¿ç”¨åŸæœ‰çš„é™æ€åˆå¹¶æ–¹æ³•")

@dataclass
class ClassificationPath:
    """åˆ†ç±»è·¯å¾„æ•°æ®ç»“æ„"""
    root: str                    # æ ¹èŠ‚ç‚¹ (å¦‚: "è®¡ç®—æœºç§‘å­¦")
    level1: str                  # ä¸€çº§åˆ†ç±» (å¦‚: "è®¡ç®—æœºè§†è§‰") 
    level2: Optional[str]        # äºŒçº§åˆ†ç±» (å¦‚: "å›¾åƒå¤„ç†")
    level3: Optional[str]        # ä¸‰çº§åˆ†ç±» (å¦‚: "å›¾åƒè¶…åˆ†è¾¨ç‡")
    depth: int                   # åˆ†ç±»æ·±åº¦ (2-4)
    confidence: float            # åˆ†ç±»ç½®ä¿¡åº¦
    reasoning: str = ""          # åˆ†ç±»ç†ç”±
    
    def to_path_string(self) -> str:
        """è½¬æ¢ä¸ºè·¯å¾„å­—ç¬¦ä¸²"""
        path_parts = [self.root, self.level1]
        if self.level2:
            path_parts.append(self.level2)
        if self.level3:
            path_parts.append(self.level3)
        return " â†’ ".join(path_parts)
    
    def get_path_list(self) -> List[str]:
        """è·å–åˆ†ç±»è·¯å¾„åˆ—è¡¨"""
        path_parts = [self.root, self.level1]
        if self.level2:
            path_parts.append(self.level2)
        if self.level3:
            path_parts.append(self.level3)
        return path_parts
    
    def get_leaf_node(self) -> str:
        """è·å–å¶å­èŠ‚ç‚¹åç§°"""
        if self.level3:
            return self.level3
        elif self.level2:
            return self.level2
        else:
            return self.level1
    
    def get_parent_node(self) -> str:
        """è·å–å€’æ•°ç¬¬äºŒå±‚èŠ‚ç‚¹åç§°ï¼ˆç”¨äºç»Ÿè®¡èšåˆï¼‰"""
        if self.depth == 2:
            return self.root  # 2å±‚æ—¶ï¼Œç»Ÿè®¡èšåˆåˆ°root
        elif self.depth == 3:
            return self.level1  # 3å±‚æ—¶ï¼Œç»Ÿè®¡èšåˆåˆ°level1
        elif self.depth == 4:
            return self.level2  # 4å±‚æ—¶ï¼Œç»Ÿè®¡èšåˆåˆ°level2
        return self.root  # é»˜è®¤è¿”å›æ ¹èŠ‚ç‚¹

class TreeNode:
    """æ ‘èŠ‚ç‚¹ç±»"""
    def __init__(self, name: str, level: int):
        self.name = name
        self.level = level
        self.children: Dict[str, 'TreeNode'] = {}
        self.papers: List[Dict] = []  # å­˜å‚¨å±äºè¯¥èŠ‚ç‚¹çš„è®ºæ–‡
        self.statistics: Dict = {}    # å­˜å‚¨ç»Ÿè®¡ä¿¡æ¯
        
    def add_child(self, child_name: str) -> 'TreeNode':
        """æ·»åŠ å­èŠ‚ç‚¹"""
        if child_name not in self.children:
            self.children[child_name] = TreeNode(child_name, self.level + 1)
        return self.children[child_name]
    
    def add_paper(self, paper: Dict):
        """å‘èŠ‚ç‚¹æ·»åŠ è®ºæ–‡"""
        self.papers.append(paper)
    
    def is_leaf(self) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºå¶å­èŠ‚ç‚¹"""
        return len(self.children) == 0
    
    def get_leaf_nodes(self) -> List['TreeNode']:
        """è·å–æ‰€æœ‰å¶å­èŠ‚ç‚¹"""
        if self.is_leaf():
            return [self]
        
        leaves = []
        for child in self.children.values():
            leaves.extend(child.get_leaf_nodes())
        return leaves

class TreeClassificationAnalyzer:
    """æ ‘å½¢åˆ†ç±»åˆ†æå™¨"""
    
    def __init__(self, api_key=None, data_dir="../data", ignore_cache=False):
        """åˆå§‹åŒ–åˆ†æå™¨"""
        # åˆå§‹åŒ–logger
        self.logger = logging.getLogger(__name__)
        
        # åŠ è½½ç¯å¢ƒå˜é‡
        env_file = '.env'
        if os.path.exists(env_file):
            load_dotenv(env_file)
            logger.info(f"å·²åŠ è½½ç¯å¢ƒå˜é‡æ–‡ä»¶: {env_file}")
        
        # APIé…ç½®
        self.api_key = api_key or os.getenv('OPENAI_API_KEY')
        if not self.api_key:
            raise ValueError("æœªæ‰¾åˆ°APIå¯†é’¥ï¼Œè¯·è®¾ç½®OPENAI_API_KEYç¯å¢ƒå˜é‡")
        
        # OpenAIå®¢æˆ·ç«¯é…ç½®
        self.base_url = os.getenv('OPENAI_BASE_URL', 'https://api.dou.chat/v1')
        self.model_name = "openai/gpt-4.1"
        
        # åˆå§‹åŒ–OpenAIå®¢æˆ·ç«¯
        self.client = OpenAI(
            api_key=self.api_key,
            base_url=self.base_url
        )
        
        logger.info(f"ä½¿ç”¨APIåŸºç¡€URL: {self.base_url}")
        logger.info(f"ä½¿ç”¨æ¨¡å‹: {self.model_name}")
        
        # ç¼“å­˜æ–‡ä»¶è·¯å¾„
        self.classification_cache_file = Path("cache/classification_cache.pkl")
        self.analysis_cache_file = Path("cache/leaf_analysis_cache.pkl")
        self.classification_cache_file.parent.mkdir(exist_ok=True)
        
        # æ•°æ®ç›®å½• - æ™ºèƒ½æ£€æµ‹
        self.data_dir = self._resolve_data_dir(data_dir)
        
        # åŠ è½½ç¼“å­˜ï¼ˆå¦‚æœä¸å¿½ç•¥çš„è¯ï¼‰
        if ignore_cache:
            logger.info("âš ï¸ å¿½ç•¥ç¼“å­˜ï¼Œå°†é‡æ–°åˆ†ææ‰€æœ‰è®ºæ–‡")
            self.classification_cache = {}
            self.analysis_cache = {}
        else:
            self.classification_cache = self._load_cache(self.classification_cache_file)
            self.analysis_cache = self._load_cache(self.analysis_cache_file)
        
        # åˆ†ç±»æ ‘
        self.classification_tree = TreeNode("æ ¹èŠ‚ç‚¹", 0)
        
        logger.info("âœ… æ ‘å½¢åˆ†ç±»åˆ†æå™¨åˆå§‹åŒ–æˆåŠŸ")
        
        # ç¬¬ä¸€æ­¥ï¼šè®ºæ–‡åˆ†ç±»æç¤ºè¯
        self.classification_prompt = """
ä½ æ˜¯ä¸€åä¸“ä¸šçš„å­¦æœ¯åˆ†ç±»ä¸“å®¶ï¼Œç²¾é€šè®¡ç®—æœºç§‘å­¦ã€äººå·¥æ™ºèƒ½ç­‰æŠ€æœ¯é¢†åŸŸçš„ç»†åˆ†åˆ†ç±»ã€‚

è¯·å¯¹ä»¥ä¸‹è®ºæ–‡è¿›è¡Œ**çµæ´»å±‚æ¬¡åˆ†ç±»**ï¼Œæ ¹æ®è®ºæ–‡å†…å®¹çš„å…·ä½“ç¨‹åº¦é€‰æ‹©åˆé€‚çš„åˆ†ç±»æ·±åº¦ï¼ˆ2-4å±‚ï¼‰ï¼š

**è®ºæ–‡æ ‡é¢˜**: {title}
**è®ºæ–‡æ‘˜è¦**: {abstract}
**arXivç±»åˆ«**: {categories}

**åˆ†ç±»å±‚æ¬¡è¯´æ˜**ï¼š
- **2å±‚åˆ†ç±»**: é€‚ç”¨äºå®½æ³›çš„è·¨é¢†åŸŸç ”ç©¶ (å¦‚: "äººå·¥æ™ºèƒ½" â†’ "æœºå™¨å­¦ä¹ ç†è®º")
- **3å±‚åˆ†ç±»**: é€‚ç”¨äºå¸¸è§„ä¸“ä¸šç ”ç©¶ (å¦‚: "è®¡ç®—æœºç§‘å­¦" â†’ "è®¡ç®—æœºè§†è§‰" â†’ "å›¾åƒå¤„ç†")  
- **4å±‚åˆ†ç±»**: é€‚ç”¨äºé«˜åº¦ä¸“ä¸šåŒ–ç ”ç©¶ (å¦‚: "è®¡ç®—æœºç§‘å­¦" â†’ "è®¡ç®—æœºè§†è§‰" â†’ "å›¾åƒå¤„ç†" â†’ "å›¾åƒè¶…åˆ†è¾¨ç‡")

**åˆ†ç±»åŸåˆ™**ï¼š
1. æ ¹æ®è®ºæ–‡çš„ä¸“ä¸šåŒ–ç¨‹åº¦é€‰æ‹©åˆ†ç±»æ·±åº¦
2. ç¡®ä¿åŒç±»è®ºæ–‡èƒ½å¤Ÿèšé›†åœ¨åŒä¸€ä¸ªå€’æ•°ç¬¬äºŒå±‚èŠ‚ç‚¹ä¸‹
3. é¿å…è¿‡åº¦ç»†åˆ†å¯¼è‡´èŠ‚ç‚¹è¿‡äºç¨€ç–
4. ä¿æŒåˆ†ç±»çš„é€»è¾‘å±‚æ¬¡æ€§å’Œè¯­ä¹‰ä¸€è‡´æ€§

**è¾“å‡ºæ ¼å¼**ï¼š
ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¿”å›ï¼Œæœªä½¿ç”¨çš„å±‚çº§è®¾ä¸ºnullï¼š

{{
  "root": "æ ¹èŠ‚ç‚¹åç§°",
  "level1": "ä¸€çº§åˆ†ç±»åç§°", 
  "level2": "äºŒçº§åˆ†ç±»åç§°æˆ–null",
  "level3": "ä¸‰çº§åˆ†ç±»åç§°æˆ–null",
  "depth": åˆ†ç±»æ·±åº¦æ•°å­—(2-4),
  "confidence": 0.95,
  "reasoning": "é€‰æ‹©è¯¥åˆ†ç±»æ·±åº¦å’Œè·¯å¾„çš„ç†ç”±"
}}

**åˆ†ç±»ç¤ºä¾‹**ï¼š
- 2å±‚: {{"root": "æ•°å­¦", "level1": "æ¦‚ç‡è®ºä¸ç»Ÿè®¡", "level2": null, "level3": null, "depth": 2}}
- 3å±‚: {{"root": "è®¡ç®—æœºç§‘å­¦", "level1": "è®¡ç®—æœºè§†è§‰", "level2": "å›¾åƒå¤„ç†", "level3": null, "depth": 3}}
- 4å±‚: {{"root": "è®¡ç®—æœºç§‘å­¦", "level1": "è®¡ç®—æœºè§†è§‰", "level2": "å›¾åƒå¤„ç†", "level3": "å›¾åƒè¶…åˆ†è¾¨ç‡", "depth": 4}}
"""
        
        # ç¬¬äºŒæ­¥ï¼šå¶å­èŠ‚ç‚¹ç»Ÿè®¡åˆ†ææç¤ºè¯
        self.leaf_analysis_prompt = """
ä½ æ˜¯ä¸€åä¸“ä¸šçš„å­¦æœ¯ç»Ÿè®¡åˆ†æä¸“å®¶ï¼Œè¯·å¯¹ä»¥ä¸‹å¶å­èŠ‚ç‚¹çš„è®ºæ–‡é›†åˆè¿›è¡Œæ·±åº¦ç»Ÿè®¡åˆ†æã€‚

**å¶å­èŠ‚ç‚¹**: {leaf_name}
**åˆ†ç±»è·¯å¾„**: {classification_path}
**è®ºæ–‡æ•°é‡**: {paper_count}

**è®ºæ–‡æ‘˜è¦é›†åˆ**:
{abstracts}

**åˆ†æè¦æ±‚**ï¼š
è¯·å¯¹è¯¥å¶å­èŠ‚ç‚¹çš„è®ºæ–‡è¿›è¡Œä»¥ä¸‹ä¸‰ä¸ªç»´åº¦çš„ç»Ÿè®¡åˆ†æï¼Œæ¯ä¸ªç»´åº¦æå–TOP10ï¼š

1. **å…³é”®è¯åˆ†æ** (keywords): æå–æœ€é¢‘ç¹å‡ºç°çš„æŠ€æœ¯å…³é”®è¯
2. **æŠ€æœ¯æ–¹æ³•åˆ†æ** (methods): æ€»ç»“ä¸»è¦ä½¿ç”¨çš„æŠ€æœ¯æ–¹æ³•å’Œç®—æ³•
3. **ç ”ç©¶é—®é¢˜åˆ†æ** (problems): è¯†åˆ«ä¸»è¦è§£å†³çš„ç ”ç©¶é—®é¢˜å’ŒæŒ‘æˆ˜

**è¾“å‡ºæ ¼å¼**ï¼š
ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¿”å›ï¼š

{{
  "keywords": [
    {{"term": "å…³é”®è¯1", "frequency": 15, "description": "ç®€çŸ­æè¿°"}},
    {{"term": "å…³é”®è¯2", "frequency": 12, "description": "ç®€çŸ­æè¿°"}},
    ...
  ],
  "methods": [
    {{"method": "æŠ€æœ¯æ–¹æ³•1", "frequency": 8, "description": "æ–¹æ³•æè¿°"}},
    {{"method": "æŠ€æœ¯æ–¹æ³•2", "frequency": 6, "description": "æ–¹æ³•æè¿°"}},
    ...
  ],
  "problems": [
    {{"problem": "ç ”ç©¶é—®é¢˜1", "frequency": 10, "description": "é—®é¢˜æè¿°"}},
    {{"problem": "ç ”ç©¶é—®é¢˜2", "frequency": 7, "description": "é—®é¢˜æè¿°"}},
    ...
  ],
  "summary": "è¯¥å¶å­èŠ‚ç‚¹çš„æ•´ä½“ç ”ç©¶è¶‹åŠ¿å’Œç‰¹ç‚¹æ€»ç»“"
}}

**æ³¨æ„äº‹é¡¹**ï¼š
- ç¡®ä¿ç»Ÿè®¡ç»“æœåŸºäºå®é™…è®ºæ–‡å†…å®¹
- ä¼˜å…ˆæå–å…·æœ‰ä»£è¡¨æ€§çš„ä¸“ä¸šæœ¯è¯­
- é¢‘æ¬¡ç»Ÿè®¡åº”è¯¥å‡†ç¡®åæ˜ åœ¨è®ºæ–‡é›†åˆä¸­çš„å‡ºç°é¢‘ç‡
- æ¯ä¸ªç»´åº¦æœ€å¤šè¿”å›10ä¸ªæ¡ç›®ï¼ŒæŒ‰é¢‘æ¬¡é™åºæ’åˆ—
"""

    def _resolve_data_dir(self, data_dir: str) -> str:
        """æ™ºèƒ½è§£ææ•°æ®ç›®å½•è·¯å¾„"""
        # å°è¯•å¤šä¸ªå¯èƒ½çš„è·¯å¾„
        possible_paths = [
            data_dir,                    # ç”¨æˆ·æŒ‡å®šçš„è·¯å¾„
            "../data",                   # é¡¹ç›®æ ¹ç›®å½•ä¸‹çš„data
            "../../data",               # å¦‚æœåœ¨å­ç›®å½•ä¸­
            "./data",                    # å½“å‰ç›®å½•çš„data
            os.path.join(os.getcwd(), "data"),  # å½“å‰å·¥ä½œç›®å½•çš„data
        ]
        
        for path in possible_paths:
            abs_path = Path(path).resolve()
            if abs_path.exists() and abs_path.is_dir():
                logger.info(f"æ‰¾åˆ°æ•°æ®ç›®å½•: {abs_path}")
                return str(abs_path)
        
        # å¦‚æœéƒ½æ²¡æ‰¾åˆ°ï¼Œè¿”å›åŸå§‹è·¯å¾„ï¼ˆä¼šåœ¨åç»­å¤„ç†ä¸­æŠ¥é”™ï¼‰
        logger.warning(f"æœªæ‰¾åˆ°æ•°æ®ç›®å½•ï¼Œå°†ä½¿ç”¨æŒ‡å®šè·¯å¾„: {data_dir}")
        return data_dir
    
    def _load_cache(self, cache_file: Path) -> Dict:
        """åŠ è½½ç¼“å­˜"""
        try:
            if cache_file.exists():
                with open(cache_file, 'rb') as f:
                    cache = pickle.load(f)
                logger.info(f"å·²åŠ è½½ç¼“å­˜æ–‡ä»¶: {cache_file}")
                return cache
        except Exception as e:
            logger.warning(f"åŠ è½½ç¼“å­˜å¤±è´¥: {e}")
        return {}
    
    def _save_cache(self, cache: Dict, cache_file: Path):
        """ä¿å­˜ç¼“å­˜"""
        try:
            with open(cache_file, 'wb') as f:
                pickle.dump(cache, f)
            logger.debug(f"ç¼“å­˜å·²ä¿å­˜: {cache_file}")
        except Exception as e:
            logger.warning(f"ä¿å­˜ç¼“å­˜å¤±è´¥: {e}")
    
    def _convert_old_cache_format(self, cached_result: Dict) -> Dict:
        """
        è½¬æ¢æ—§æ ¼å¼ç¼“å­˜ä¸ºæ–°æ ¼å¼
        æ—§æ ¼å¼: {root, level1, level2, leaf, confidence}
        æ–°æ ¼å¼: {root, level1, level2, level3, depth, confidence, reasoning}
        """
        new_format = {
            'root': cached_result.get('root', 'æœªåˆ†ç±»'),
            'level1': cached_result.get('level1', 'æœªçŸ¥é¢†åŸŸ'),
            'level2': cached_result.get('level2'),
            'level3': cached_result.get('leaf'),  # å°†æ—§çš„leafè½¬ä¸ºlevel3
            'depth': 4 if cached_result.get('leaf') else 3,  # æ¨æµ‹æ·±åº¦
            'confidence': cached_result.get('confidence', 0.5),
            'reasoning': 'ä»æ—§æ ¼å¼ç¼“å­˜è½¬æ¢'
        }
        
        # å¤„ç†Noneå€¼å’Œç‰¹æ®Šæ ‡è®°
        if new_format['level2'] in ['æœªçŸ¥å­é¢†åŸŸ', None]:
            new_format['level2'] = None
        if new_format['level3'] in ['æœªçŸ¥å…·ä½“é¢†åŸŸ', None]:
            new_format['level3'] = None
            
        # é‡æ–°è®¡ç®—æ·±åº¦
        depth = 2
        if new_format['level2']:
            depth = 3
        if new_format['level3']:
            depth = 4
        new_format['depth'] = depth
        
        logger.debug(f"è½¬æ¢æ—§æ ¼å¼ç¼“å­˜: {cached_result} -> {new_format}")
        return new_format
    
    def load_papers_by_date(self, start_date: str = None, end_date: str = None) -> List[Dict]:
        """
        æ ¹æ®æŒ‡å®šæ—¥æœŸåŠ è½½è®ºæ–‡æ•°æ®
        
        Args:
            start_date: å¼€å§‹æ—¥æœŸ (YYYY-MM-DD)
            end_date: ç»“æŸæ—¥æœŸ (YYYY-MM-DD)
            
        Returns:
            è®ºæ–‡æ•°æ®åˆ—è¡¨
        """
        papers = []
        
        # è·å–æ•°æ®ç›®å½•ä¸­çš„æ‰€æœ‰jsonlæ–‡ä»¶
        data_path = Path(self.data_dir)
        if not data_path.exists():
            logger.error(f"æ•°æ®ç›®å½•ä¸å­˜åœ¨: {data_path.absolute()}")
            logger.error(f"è¯·ç¡®ä¿æ•°æ®ç›®å½•å­˜åœ¨ï¼Œæˆ–ä½¿ç”¨ --data-dir å‚æ•°æŒ‡å®šæ­£ç¡®çš„è·¯å¾„")
            return papers
        
        # æ”¶é›†æ—¥æœŸèŒƒå›´å†…çš„æ–‡ä»¶
        target_files = []
        for jsonl_file in data_path.glob("*.jsonl"):
            file_date = jsonl_file.stem  # è·å–ä¸å¸¦æ‰©å±•åçš„æ–‡ä»¶å
            
            # æ£€æŸ¥æ—¥æœŸæ ¼å¼å’ŒèŒƒå›´
            try:
                file_datetime = datetime.strptime(file_date, "%Y-%m-%d")
                
                # æ£€æŸ¥æ—¥æœŸèŒƒå›´
                in_range = True
                if start_date:
                    start_datetime = datetime.strptime(start_date, "%Y-%m-%d")
                    in_range = in_range and file_datetime >= start_datetime
                
                if end_date:
                    end_datetime = datetime.strptime(end_date, "%Y-%m-%d")
                    in_range = in_range and file_datetime <= end_datetime
                
                if in_range:
                    target_files.append(jsonl_file)
                    
            except ValueError:
                # è·³è¿‡æ—¥æœŸæ ¼å¼ä¸æ­£ç¡®çš„æ–‡ä»¶
                continue
        
        # åŠ è½½æ–‡ä»¶å†…å®¹
        for jsonl_file in sorted(target_files):
            try:
                with open(jsonl_file, 'r', encoding='utf-8') as f:
                    for line in f:
                        if line.strip():
                            paper = json.loads(line.strip())
                            papers.append(paper)
            except Exception as e:
                logger.warning(f"åŠ è½½æ–‡ä»¶å¤±è´¥ {jsonl_file}: {e}")
        
        date_range_str = f"{start_date or 'æœ€æ—©'} åˆ° {end_date or 'æœ€æ–°'}"
        logger.info(f"åŠ è½½è®ºæ–‡æ•°æ®: {len(papers)} ç¯‡ (æ—¶é—´èŒƒå›´: {date_range_str})")
        
        return papers
    
    def is_ai_related_paper(self, title, abstract):
        """
        å¢å¼ºçš„AIç›¸å…³æ€§åˆ¤æ–­ï¼Œæ›´ä¸¥æ ¼çš„ç­›é€‰
        """
        # ç»„åˆæ ‡é¢˜å’Œæ‘˜è¦è¿›è¡Œåˆ†æ
        text = f"{title} {abstract}".lower()
        
        # AIæ ¸å¿ƒå…³é”®è¯ - å¤§å¹…æ‰©å±•å’Œæ›´ç²¾å‡†
        ai_core_keywords = {
            # æœºå™¨å­¦ä¹ ä¸æ·±åº¦å­¦ä¹ æ ¸å¿ƒ
            'machine learning', 'deep learning', 'neural network', 'neural networks', 
            'artificial intelligence', 'ai', 'deep neural network', 'convolutional neural network',
            'cnn', 'rnn', 'lstm', 'gru', 'transformer', 'attention mechanism',
            'reinforcement learning', 'supervised learning', 'unsupervised learning',
            'semi-supervised learning', 'self-supervised learning', 'contrastive learning',
            'few-shot learning', 'zero-shot learning', 'meta-learning', 'transfer learning',
            'federated learning', 'continual learning', 'lifelong learning',
            
            # å¤§è¯­è¨€æ¨¡å‹ä¸NLP
            'large language model', 'language model', 'llm', 'gpt', 'bert', 'roberta',
            'natural language processing', 'nlp', 'text generation', 'language generation',
            'text-to-text', 'seq2seq', 'encoder-decoder', 'pre-trained model',
            'fine-tuning', 'prompt engineering', 'prompt learning', 'in-context learning',
            'chain-of-thought', 'reasoning', 'question answering', 'dialogue system',
            'conversation', 'chatbot', 'retrieval-augmented generation', 'rag',
            'hallucination', 'text classification', 'sentiment analysis', 'named entity recognition',
            'information extraction', 'machine translation', 'summarization',
            
            # è®¡ç®—æœºè§†è§‰æ ¸å¿ƒ
            'computer vision', 'image processing', 'object detection', 'image classification',
            'semantic segmentation', 'instance segmentation', 'object recognition',
            'face recognition', 'facial recognition', 'pose estimation', 'action recognition',
            'video analysis', 'video understanding', 'image generation', 'text-to-image',
            'diffusion model', 'generative adversarial network', 'gan', 'variational autoencoder',
            'vae', '3d reconstruction', 'depth estimation', 'optical flow',
            'visual tracking', 'multi-object tracking', 'visual question answering',
            'image captioning', 'visual grounding', 'object localization',
            
            # å¤šæ¨¡æ€ä¸è§†è§‰-è¯­è¨€
            'multimodal', 'multi-modal', 'vision-language', 'vision language model',
            'clip', 'visual-linguistic', 'cross-modal', 'image-text', 'video-text',
            'audio-visual', 'speech recognition', 'automatic speech recognition',
            
            # AIåº”ç”¨ä¸æ™ºèƒ½ç³»ç»Ÿ
            'autonomous driving', 'self-driving', 'robotics', 'intelligent agent',
            'recommender system', 'recommendation system', 'knowledge graph',
            'graph neural network', 'gnn', 'embedding', 'representation learning',
            'generative model', 'discriminative model', 'adversarial learning',
            'domain adaptation', 'data augmentation', 'active learning',
            
            # AIå®‰å…¨ä¸å¯è§£é‡Šæ€§
            'adversarial attack', 'adversarial example', 'robustness', 'explainable ai',
            'interpretable ai', 'fairness', 'bias detection', 'model explanation',
            'attribution', 'saliency', 'grad-cam', 'attention visualization'
        }
        
        # ç›´æ¥æ’é™¤çš„éAIé¢†åŸŸå…³é”®è¯
        excluded_keywords = {
            # çº¯å›¾å½¢å­¦æ¸²æŸ“
            'skeletal animation', 'vertex shader', 'fragment shader', 'ray tracing',
            'rasterization', 'procedural generation', 'terrain generation', 'mesh generation',
            'texture mapping', 'normal mapping', 'lighting model', 'brdf', 'specular highlights',
            'real-time rendering', 'gpu rendering', 'opengl', 'directx', 'vulkan',
            
            # çº¯ç½‘ç»œé€šä¿¡
            'wireless communication', 'channel estimation', 'signal processing',
            'antenna design', 'mimo', 'ofdm', 'network protocol', 'routing protocol',
            'network topology', 'bandwidth allocation', 'terahertz communication',
            'channel modeling', 'path loss', '5g', '6g network', 'cellular network',
            
            # è½¯ä»¶å·¥ç¨‹
            'uml modeling', 'software architecture', 'design pattern', 'code generation',
            'software testing', 'version control', 'agile development', 'scrum',
            
            # æ•°å­—äººæ–‡
            'digital humanities', 'textual scholarship', 'literary analysis',
            'historical analysis', 'cultural studies', 'philology',
            
            # çº¯æ•°å­¦/ç‰©ç†
            'quantum mechanics', 'quantum computing', 'quantum algorithm',
            'mathematical modeling', 'numerical simulation', 'finite element',
            'fluid dynamics', 'thermodynamics', 'electromagnetics',
            
            # ç”Ÿç‰©åŒ»å­¦ï¼ˆéAIè¾…åŠ©ï¼‰
            'protein folding', 'dna sequencing', 'molecular biology',
            'biochemistry', 'pharmacology', 'clinical trial',
            
            # å…¶ä»–å·¥ç¨‹é¢†åŸŸ
            'mechanical engineering', 'civil engineering', 'electrical circuits',
            'power systems', 'control systems', 'embedded systems'
        }
        
        # è®¡ç®—AIå…³é”®è¯åŒ¹é…æ•°
        ai_score = 0
        for keyword in ai_core_keywords:
            if keyword in text:
                ai_score += 1
        
        # æ£€æŸ¥æ’é™¤å…³é”®è¯
        excluded_score = 0
        for keyword in excluded_keywords:
            if keyword in text:
                excluded_score += 1
        
        # æ›´ä¸¥æ ¼çš„åˆ¤æ–­é€»è¾‘
        # å¿…é¡»æœ‰è‡³å°‘3ä¸ªAIå…³é”®è¯åŒ¹é…ï¼Œä¸”æ’é™¤å…³é”®è¯ä¸è¶…è¿‡1ä¸ª
        is_ai_related = (ai_score >= 3) and (excluded_score <= 1)
        
        # å¦‚æœAIåˆ†æ•°å¾ˆé«˜(>=5)ï¼Œå³ä½¿æœ‰å°‘é‡æ’é™¤è¯ä¹Ÿæ¥å—
        if ai_score >= 5 and excluded_score <= 2:
            is_ai_related = True
            
        # ç‰¹æ®Šæƒ…å†µï¼šå¦‚æœæ˜ç¡®åŒ…å«AIåº”ç”¨ä½†ç”¨äº†ä¼ ç»ŸæŠ€æœ¯è¯æ±‡
        ai_applications = ['medical imaging', 'medical image', 'autonomous vehicle', 
                          'intelligent system', 'smart system', 'ai-assisted',
                          'ai-based', 'machine learning-based', 'deep learning-based']
        
        for app in ai_applications:
            if app in text and ai_score >= 2:
                is_ai_related = True
                break
        
        self.logger.debug(f"AIç­›é€‰ - æ ‡é¢˜: {title[:50]}...")
        self.logger.debug(f"AIåˆ†æ•°: {ai_score}, æ’é™¤åˆ†æ•°: {excluded_score}, ç»“æœ: {is_ai_related}")
        
        return is_ai_related

    def classify_paper(self, paper: Dict) -> ClassificationPath:
        """
        ç¬¬ä¸€æ­¥ï¼šå¯¹å•ç¯‡è®ºæ–‡è¿›è¡Œç»†åˆ†åˆ†ç±»
        
        Args:
            paper: è®ºæ–‡æ•°æ®
            
        Returns:
            åˆ†ç±»è·¯å¾„ï¼Œå¦‚æœä¸æ˜¯AIç›¸å…³è®ºæ–‡åˆ™è¿”å›None
        """
        # é¦–å…ˆæ£€æŸ¥æ˜¯å¦ä¸ºAIç›¸å…³è®ºæ–‡
        if not self.is_ai_related_paper(paper.get('title', ''), paper.get('summary', '')):
            logger.info(f"è·³è¿‡éAIç›¸å…³è®ºæ–‡: {paper.get('title', '')[:50]}...")
            return None
            
        paper_id = paper.get('id', '')
        
        # æ£€æŸ¥ç¼“å­˜
        if paper_id in self.classification_cache:
            cached_result = self.classification_cache[paper_id]
            
            # å¤„ç†æ—§æ ¼å¼ç¼“å­˜å…¼å®¹æ€§
            if 'leaf' in cached_result:
                # å°†æ—§æ ¼å¼è½¬æ¢ä¸ºæ–°æ ¼å¼
                cached_result = self._convert_old_cache_format(cached_result)
            
            return ClassificationPath(**cached_result)
        
        # å‡†å¤‡è®ºæ–‡ä¿¡æ¯
        title = paper.get('title', '').strip()
        abstract = paper.get('summary', '').strip()
        categories = paper.get('categories', [])
        
        if not abstract:
            # è¿”å›é»˜è®¤åˆ†ç±»
            default_path = ClassificationPath(
                root="æœªåˆ†ç±»",
                level1="æœªçŸ¥é¢†åŸŸ", 
                level2=None,
                level3=None,
                depth=2,
                confidence=0.0
            )
            return default_path
        
        try:
            # è°ƒç”¨å¤§æ¨¡å‹è¿›è¡Œåˆ†ç±»
            prompt = self.classification_prompt.format(
                title=title[:200],  # é™åˆ¶æ ‡é¢˜é•¿åº¦
                abstract=abstract[:1500],  # é™åˆ¶æ‘˜è¦é•¿åº¦
                categories=', '.join(categories[:5])  # é™åˆ¶ç±»åˆ«æ•°é‡
            )
            
            response = self.client.chat.completions.create(
                model=self.model_name,
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä¸€åä¸“ä¸šçš„å­¦æœ¯åˆ†ç±»ä¸“å®¶ï¼Œè¯·ä¸¥æ ¼æŒ‰ç…§JSONæ ¼å¼è¿”å›åˆ†ç±»ç»“æœã€‚"},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.1,
                max_tokens=500
            )
            
            result_text = response.choices[0].message.content.strip()
            
            # è§£æJSONç»“æœ
            try:
                # æå–JSONéƒ¨åˆ†
                if '```json' in result_text:
                    json_part = result_text.split('```json')[1].split('```')[0]
                elif '```' in result_text:
                    json_part = result_text.split('```')[1]
                else:
                    json_part = result_text
                
                result = json.loads(json_part.strip())
                
                # åˆ›å»ºåˆ†ç±»è·¯å¾„
                classification_path = ClassificationPath(
                    root=result.get('root', 'æœªåˆ†ç±»'),
                    level1=result.get('level1', 'æœªçŸ¥é¢†åŸŸ'),
                    level2=result.get('level2'), 
                    level3=result.get('level3'),
                    depth=int(result.get('depth', 2)),
                    confidence=float(result.get('confidence', 0.5)),
                    reasoning=result.get('reasoning', '')
                )
                
                # ä¿å­˜åˆ°ç¼“å­˜
                self.classification_cache[paper_id] = {
                    'root': classification_path.root,
                    'level1': classification_path.level1,
                    'level2': classification_path.level2,
                    'level3': classification_path.level3,
                    'depth': classification_path.depth,
                    'confidence': classification_path.confidence,
                    'reasoning': classification_path.reasoning
                }
                
                return classification_path
                
            except (json.JSONDecodeError, KeyError) as e:
                logger.warning(f"åˆ†ç±»ç»“æœè§£æå¤±è´¥: {e}")
                raise
                
        except Exception as e:
            logger.error(f"è®ºæ–‡åˆ†ç±»å¤±è´¥ {paper_id}: {e}")
            # è¿”å›é»˜è®¤åˆ†ç±»
            return ClassificationPath(
                root="åˆ†ç±»å¤±è´¥",
                level1="APIé”™è¯¯",
                level2=None,
                level3=None,
                depth=2,
                confidence=0.0
            )
    
    def batch_classify_papers(self, papers: List[Dict], max_workers: int = 8) -> List[Tuple[Dict, ClassificationPath]]:
        """
        æ‰¹é‡åˆ†ç±»è®ºæ–‡
        
        Args:
            papers: è®ºæ–‡åˆ—è¡¨
            max_workers: æœ€å¤§å¹¶å‘æ•°
            
        Returns:
            (è®ºæ–‡, åˆ†ç±»è·¯å¾„) å…ƒç»„åˆ—è¡¨
        """
        results = []
        
        # åˆ†ç¦»éœ€è¦åˆ†ç±»çš„è®ºæ–‡å’Œå·²ç¼“å­˜çš„è®ºæ–‡
        papers_to_classify = []
        cached_results = []
        
        for paper in papers:
            paper_id = paper.get('id', '')
            
            if paper_id in self.classification_cache:
                cached_result = self.classification_cache[paper_id]
                
                # å¤„ç†æ—§æ ¼å¼ç¼“å­˜å…¼å®¹æ€§
                if 'leaf' in cached_result:
                    cached_result = self._convert_old_cache_format(cached_result)
                
                classification_path = ClassificationPath(**cached_result)
                cached_results.append((paper, classification_path))
            else:
                papers_to_classify.append(paper)
        
        logger.info(f"æ€»è®ºæ–‡æ•°: {len(papers)}, ç¼“å­˜å‘½ä¸­: {len(cached_results)}, éœ€è¦åˆ†ç±»: {len(papers_to_classify)}")
        
        # å¤„ç†éœ€è¦åˆ†ç±»çš„è®ºæ–‡
        if papers_to_classify:
            with ThreadPoolExecutor(max_workers=max_workers) as executor:
                # æäº¤åˆ†ç±»ä»»åŠ¡
                future_to_paper = {
                    executor.submit(self.classify_paper, paper): paper
                    for paper in papers_to_classify
                }
                
                # æ”¶é›†ç»“æœ
                for future in as_completed(future_to_paper):
                    paper = future_to_paper[future]
                    try:
                        classification_path = future.result()
                        # å¦‚æœè¿”å›Noneï¼Œè¯´æ˜ä¸æ˜¯AIç›¸å…³è®ºæ–‡ï¼Œè·³è¿‡
                        if classification_path is not None:
                            results.append((paper, classification_path))
                        
                        # å®šæœŸä¿å­˜ç¼“å­˜
                        if len(results) % 50 == 0:
                            self._save_cache(self.classification_cache, self.classification_cache_file)
                            logger.info(f"å·²å®Œæˆåˆ†ç±»: {len(results) + len(cached_results)}/{len(papers)} (è·³è¿‡éAIè®ºæ–‡)")
                            
                    except Exception as e:
                        logger.error(f"è®ºæ–‡åˆ†ç±»å¤±è´¥: {e}")
                        # æ·»åŠ é»˜è®¤åˆ†ç±»
                        default_path = ClassificationPath(
                            root="åˆ†ç±»å¤±è´¥",
                            level1="å¤„ç†é”™è¯¯",
                            level2=None,
                            level3=None,
                            depth=2,
                            confidence=0.0
                        )
                        results.append((paper, default_path))
        
        # åˆå¹¶ç»“æœ
        all_results = cached_results + results
        
        # ä¿å­˜ç¼“å­˜
        self._save_cache(self.classification_cache, self.classification_cache_file)
        
        # ç»Ÿè®¡ç­›é€‰æ•ˆæœ
        total_papers = len(papers)
        ai_papers = len(all_results)
        filtered_out = total_papers - ai_papers
        filter_rate = (filtered_out / total_papers * 100) if total_papers > 0 else 0
        
        logger.info(f"è®ºæ–‡åˆ†ç±»å®Œæˆ: {ai_papers} ç¯‡AIç›¸å…³è®ºæ–‡ (æ€»è®¡ {total_papers} ç¯‡ï¼Œç­›é€‰æ‰ {filtered_out} ç¯‡éAIè®ºæ–‡ï¼Œç­›é€‰ç‡ {filter_rate:.1f}%)")
        return all_results
    
    def build_classification_tree(self, classified_papers: List[Tuple[Dict, ClassificationPath]]) -> TreeNode:
        """
        æ„å»ºåˆ†ç±»æ ‘å¹¶å°†è®ºæ–‡åˆ†é…åˆ°å¶å­èŠ‚ç‚¹
        
        Args:
            classified_papers: å·²åˆ†ç±»çš„è®ºæ–‡åˆ—è¡¨
            
        Returns:
            åˆ†ç±»æ ‘æ ¹èŠ‚ç‚¹
        """
        # é‡æ–°åˆå§‹åŒ–åˆ†ç±»æ ‘
        self.classification_tree = TreeNode("æ ¹èŠ‚ç‚¹", 0)
        
        # æ„å»ºæ ‘ç»“æ„å¹¶åˆ†é…è®ºæ–‡
        for paper, classification_path in classified_papers:
            # æ·»åŠ åˆ†ç±»è·¯å¾„åˆ°è®ºæ–‡æ•°æ®ä¸­
            paper_with_path = paper.copy()
            paper_with_path['classification_path'] = classification_path.to_path_string()
            paper_with_path['classification_confidence'] = classification_path.confidence
            paper_with_path['classification_depth'] = classification_path.depth
            
            # æ ¹æ®åˆ†ç±»æ·±åº¦æ„å»ºè·¯å¾„
            current_node = self.classification_tree.add_child(classification_path.root)
            current_node = current_node.add_child(classification_path.level1)
            
            if classification_path.level2:
                current_node = current_node.add_child(classification_path.level2)
            
            if classification_path.level3:
                current_node = current_node.add_child(classification_path.level3)
            
            # å°†è®ºæ–‡æ·»åŠ åˆ°å¶å­èŠ‚ç‚¹
            current_node.add_paper(paper_with_path)
        
        logger.info("åˆ†ç±»æ ‘æ„å»ºå®Œæˆ")
        return self.classification_tree
    
    def analyze_leaf_node(self, leaf_node: TreeNode) -> Dict:
        """
        ç¬¬äºŒæ­¥ï¼šåˆ†æå•ä¸ªå¶å­èŠ‚ç‚¹çš„è®ºæ–‡é›†åˆ
        
        Args:
            leaf_node: å¶å­èŠ‚ç‚¹
            
        Returns:
            åˆ†æç»“æœ
        """
        leaf_name = leaf_node.name
        papers = leaf_node.papers
        
        if not papers:
            return {
                'keywords': [],
                'methods': [],
                'problems': [],
                'summary': 'è¯¥å¶å­èŠ‚ç‚¹æ²¡æœ‰è®ºæ–‡æ•°æ®'
            }
        
        # æ£€æŸ¥ç¼“å­˜
        cache_key = f"{leaf_name}_{len(papers)}"
        if cache_key in self.analysis_cache:
            return self.analysis_cache[cache_key]
        
        # å‡†å¤‡è®ºæ–‡æ‘˜è¦
        abstracts = []
        for i, paper in enumerate(papers[:20]):  # é™åˆ¶æœ€å¤š20ç¯‡è®ºæ–‡ä»¥æ§åˆ¶prompté•¿åº¦
            abstract = paper.get('summary', '').strip()
            if abstract:
                abstracts.append(f"{i+1}. {abstract[:300]}")  # æ¯ç¯‡æ‘˜è¦é™åˆ¶300å­—ç¬¦
        
        if not abstracts:
            return {
                'keywords': [],
                'methods': [],
                'problems': [],
                'summary': 'è¯¥å¶å­èŠ‚ç‚¹çš„è®ºæ–‡ç¼ºå°‘æ‘˜è¦ä¿¡æ¯'
            }
        
        # æ„å»ºåˆ†ç±»è·¯å¾„ä¿¡æ¯
        classification_path = "æœªçŸ¥"
        if papers:
            first_paper = papers[0]
            classification_path = first_paper.get('classification_path', 'æœªçŸ¥')
        
        try:
            # è°ƒç”¨å¤§æ¨¡å‹è¿›è¡Œåˆ†æ
            prompt = self.leaf_analysis_prompt.format(
                leaf_name=leaf_name,
                classification_path=classification_path,
                paper_count=len(papers),
                abstracts='\n'.join(abstracts)
            )
            
            response = self.client.chat.completions.create(
                model=self.model_name,
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä¸€åä¸“ä¸šçš„å­¦æœ¯ç»Ÿè®¡åˆ†æä¸“å®¶ï¼Œè¯·ä¸¥æ ¼æŒ‰ç…§JSONæ ¼å¼è¿”å›åˆ†æç»“æœã€‚"},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.1,
                max_tokens=1200
            )
            
            result_text = response.choices[0].message.content.strip()
            
            # è§£æJSONç»“æœ
            try:
                # æå–JSONéƒ¨åˆ†
                if '```json' in result_text:
                    json_part = result_text.split('```json')[1].split('```')[0]
                elif '```' in result_text:
                    json_part = result_text.split('```')[1]
                else:
                    json_part = result_text
                
                result = json.loads(json_part.strip())
                
                # ç¡®ä¿æ¯ä¸ªåˆ—è¡¨æœ€å¤š10ä¸ªå…ƒç´ 
                result['keywords'] = result.get('keywords', [])[:10]
                result['methods'] = result.get('methods', [])[:10]
                result['problems'] = result.get('problems', [])[:10]
                result['summary'] = result.get('summary', '')
                
                # ä¿å­˜åˆ°ç¼“å­˜
                self.analysis_cache[cache_key] = result
                
                return result
                
            except (json.JSONDecodeError, KeyError) as e:
                logger.warning(f"å¶å­èŠ‚ç‚¹åˆ†æç»“æœè§£æå¤±è´¥: {e}")
                raise
                
        except Exception as e:
            logger.error(f"å¶å­èŠ‚ç‚¹åˆ†æå¤±è´¥ {leaf_name}: {e}")
            return {
                'keywords': [],
                'methods': [],
                'problems': [],
                'summary': f'åˆ†æå¤±è´¥: {str(e)}'
            }
    
    def batch_analyze_leaf_nodes(self, max_workers: int = 6) -> Dict:
        """
        æ‰¹é‡åˆ†ææ‰€æœ‰å¶å­èŠ‚ç‚¹
        
        Args:
            max_workers: æœ€å¤§å¹¶å‘æ•°
            
        Returns:
            æ‰€æœ‰å¶å­èŠ‚ç‚¹çš„åˆ†æç»“æœ
        """
        # è·å–æ‰€æœ‰å¶å­èŠ‚ç‚¹
        leaf_nodes = self.classification_tree.get_leaf_nodes()
        
        # è¿‡æ»¤å‡ºæœ‰è®ºæ–‡çš„å¶å­èŠ‚ç‚¹
        valid_leaf_nodes = [node for node in leaf_nodes if node.papers]
        
        logger.info(f"å¼€å§‹åˆ†æå¶å­èŠ‚ç‚¹: {len(valid_leaf_nodes)} ä¸ªæœ‰æ•ˆèŠ‚ç‚¹")
        
        analysis_results = {}
        
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # æäº¤åˆ†æä»»åŠ¡
            future_to_node = {
                executor.submit(self.analyze_leaf_node, node): node
                for node in valid_leaf_nodes
            }
            
            # æ”¶é›†ç»“æœ
            completed = 0
            for future in as_completed(future_to_node):
                node = future_to_node[future]
                try:
                    analysis_result = future.result()
                    analysis_results[node.name] = {
                        'node_info': {
                            'name': node.name,
                            'paper_count': len(node.papers),
                            'classification_path': node.papers[0].get('classification_path', 'æœªçŸ¥') if node.papers else 'æœªçŸ¥'
                        },
                        'analysis': analysis_result
                    }
                    
                    completed += 1
                    if completed % 10 == 0 or completed == len(valid_leaf_nodes):
                        logger.info(f"å¶å­èŠ‚ç‚¹åˆ†æè¿›åº¦: {completed}/{len(valid_leaf_nodes)}")
                        # å®šæœŸä¿å­˜ç¼“å­˜
                        self._save_cache(self.analysis_cache, self.analysis_cache_file)
                        
                except Exception as e:
                    logger.error(f"å¶å­èŠ‚ç‚¹åˆ†æå¤±è´¥ {node.name}: {e}")
        
        # ä¿å­˜ç¼“å­˜
        self._save_cache(self.analysis_cache, self.analysis_cache_file)
        
        logger.info(f"å¶å­èŠ‚ç‚¹åˆ†æå®Œæˆ: {len(analysis_results)} ä¸ªèŠ‚ç‚¹")
        return analysis_results
    
    def semantic_merge_similar_nodes(self, parent_aggregation: Dict) -> Dict:
        """
        åŸºäºè¯­ä¹‰ç›¸ä¼¼æ€§åˆå¹¶ç›¸è¿‘çš„çˆ¶èŠ‚ç‚¹
        """
        # å°è¯•ä½¿ç”¨åŠ¨æ€è¯­ä¹‰åˆå¹¶å™¨
        if DynamicSemanticMerger is not None:
            try:
                print("ğŸš€ ä½¿ç”¨åŠ¨æ€è¯­ä¹‰åˆå¹¶å™¨è¿›è¡Œæ™ºèƒ½åˆå¹¶...")
                merger = DynamicSemanticMerger(similarity_threshold=0.85)
                return merger.merge_similar_nodes(parent_aggregation)
            except Exception as e:
                print(f"âš ï¸  åŠ¨æ€åˆå¹¶å¤±è´¥ï¼Œå›é€€åˆ°é™æ€åˆå¹¶: {e}")
        
        # å›é€€åˆ°åŸæœ‰çš„é™æ€åˆå¹¶æ–¹æ³•
        print("ğŸ“š ä½¿ç”¨é™æ€è¯­ä¹‰è¯å…¸è¿›è¡Œåˆå¹¶...")
        
        # æ‰©å±•çš„é™æ€è¯­ä¹‰ç›¸ä¼¼çš„èŠ‚ç‚¹ç»„ï¼ˆåŒ…å«æ›´å¤šNLPæœ¯è¯­ï¼‰
        semantic_groups = {
            # NLPç›¸å…³æœ¯è¯­ - å¤§å¹…æ‰©å±•
            "å¤§è¯­è¨€æ¨¡å‹": [
                "large language models", "large language model", "llm", "llms",
                "å¤§è¯­è¨€æ¨¡å‹", "å¤§å‹è¯­è¨€æ¨¡å‹", "è¯­è¨€å¤§æ¨¡å‹", "è¯­è¨€æ¨¡å‹"
            ],
            "æ–‡æœ¬ç”Ÿæˆ": [
                "æ–‡æœ¬ç”Ÿæˆ", "è‡ªåŠ¨æ–‡æœ¬ç†è§£ä¸ç”Ÿæˆ", "æ–‡æœ¬ç”Ÿæˆä¸ç†è§£", "æ–‡æœ¬ç”Ÿæˆæ£€æµ‹",
                "text generation", "natural language generation", "nlg"
            ],
            "å¯¹è¯ç³»ç»Ÿ": [
                "å¯¹è¯ç³»ç»Ÿ", "å¯¹è¯ç³»ç»Ÿä¸å¯¹è¯å»ºæ¨¡", "å¯¹è¯ç³»ç»Ÿä¸äº¤äº’å¼å­¦ä¹ ",
                "dialogue systems", "dialog systems", "conversational ai"
            ],
            "è‡ªç„¶è¯­è¨€å¤„ç†": [
                "è‡ªç„¶è¯­è¨€å¤„ç†", "natural language processing", "nlp", "computational linguistics"
            ],
            "æ–‡æœ¬åˆ†ç±»": [
                "æ–‡æœ¬åˆ†ç±»", "text classification", "document classification"
            ],
            "æœºå™¨ç¿»è¯‘": [
                "æœºå™¨ç¿»è¯‘", "machine translation", "neural machine translation"
            ],
            "é—®ç­”ç³»ç»Ÿ": [
                "é—®ç­”ç³»ç»Ÿ", "question answering", "qa", "æ™ºèƒ½é—®ç­”"
            ],
            "ä¿¡æ¯æŠ½å–": [
                "ä¿¡æ¯æŠ½å–", "information extraction", "å‘½åå®ä½“è¯†åˆ«", "named entity recognition"
            ],
            "æƒ…æ„Ÿåˆ†æ": [
                "æƒ…æ„Ÿåˆ†æ", "sentiment analysis", "emotion analysis"
            ],
            "å¤šæ¨¡æ€å­¦ä¹ ": [
                "å¤šæ¨¡æ€å­¦ä¹ ", "å¤šæ¨¡æ€æ¨ç†ä¸ç†è§£", "è§†è§‰é—®ç­”ä¸å¤šæ¨¡æ€ç†è§£",
                "multimodal learning", "vision-language"
            ],
            
            # CVç›¸å…³æœ¯è¯­ï¼ˆä¿æŒåŸæœ‰ï¼‰
            "ä¸‰ç»´è§†è§‰ä¸é‡å»º": [
                "ä¸‰ç»´è§†è§‰ä¸é‡å»º", "ä¸‰ç»´è§†è§‰ä¸åœºæ™¯é‡å»º", "ä¸‰ç»´è§†è§‰ä¸å‡ ä½•å»ºæ¨¡", 
                "ä¸‰ç»´é‡å»ºä¸è¿åŠ¨æ•æ‰", "ä¸‰ç»´é‡å»ºä¸åœºæ™¯ç†è§£", "ä¸‰ç»´è§†è§‰ä¸åœºæ™¯ç†è§£",
                "ä¸‰ç»´é‡å»ºä¸ç”Ÿæˆ", "ä¸‰ç»´é‡å»ºä¸å»ºæ¨¡", "ä¸‰ç»´é‡å»ºä¸æµ‹é‡", 
                "ä¸‰ç»´è§†è§‰ä¸å§¿æ€ä¼°è®¡", "ä¸‰ç»´è§†è§‰ä¸å›¾å½¢å­¦", "ä¸‰ç»´è§†è§‰ä¸ç”Ÿæˆ",
                "ä¸‰ç»´è§†è§‰ä¸ç‚¹äº‘å¤„ç†", "ä¸‰ç»´è§†è§‰ä¸ç‚¹äº‘åˆ†æ", "ç«‹ä½“è§†è§‰ä¸ä¸‰ç»´é‡å»º",
                "ä¸‰ç»´é‡å»º", "ä¸‰ç»´é‡å»ºä¸è§†å›¾åˆæˆ"
            ],
            "åŒ»å­¦å›¾åƒåˆ†æ": [
                "åŒ»å­¦å›¾åƒåˆ†æ", "åŒ»å­¦å½±åƒåˆ†æ", "åŒ»å­¦å›¾åƒå¤„ç†"
            ],
            "æ·±åº¦å­¦ä¹ ": [
                "æ·±åº¦å­¦ä¹ ", "æ·±åº¦å­¦ä¹ å¯è§£é‡Šæ€§"
            ],
            "ç›®æ ‡æ£€æµ‹": [
                "ç›®æ ‡æ£€æµ‹", "ç›®æ ‡æ£€æµ‹ä¸è¯†åˆ«", "ç›®æ ‡æ£€æµ‹ä¸å®šä½", "ç›®æ ‡æ£€æµ‹ä¸è·Ÿè¸ª", "ç›®æ ‡æ£€æµ‹ä¸è¡Œä¸ºåˆ†æ"
            ],
            "å›¾åƒç”Ÿæˆ": [
                "å›¾åƒç”Ÿæˆ", "å›¾åƒç”Ÿæˆä¸ç¼–è¾‘", "å›¾åƒç”Ÿæˆä¸ç†è§£", "å›¾åƒç”Ÿæˆä¸åˆ†å‰²"
            ],
            "è§†è§‰ä¸è¯­è¨€": [
                "è§†è§‰ä¸è¯­è¨€æ¨ç†", "è§†è§‰ä¸è¯­è¨€èåˆ", "è§†è§‰-è¯­è¨€æ¨¡å‹", "å›¾åƒæè¿°ä¸è§†è§‰è¯­è¨€æ¨¡å‹"
            ],
            "æœºå™¨äººæ„ŸçŸ¥": [
                "æœºå™¨äººæ„ŸçŸ¥ä¸åœºæ™¯ç†è§£", "æœºå™¨äººæ„ŸçŸ¥ä¸è®¤çŸ¥", "æœºå™¨äººæ„ŸçŸ¥ä¸è§†è§‰", "æœºå™¨äººæ„ŸçŸ¥ä¸æ§åˆ¶"
            ],
            "è§†é¢‘å¤„ç†": [
                "è§†é¢‘å¤„ç†", "è§†é¢‘ç†è§£", "è§†é¢‘åˆ†æä¸ç†è§£", "è§†é¢‘ç†è§£ä¸ç”Ÿæˆ"
            ]
        }
        
        merged_aggregation = {}
        used_nodes = set()
        
        # å¤„ç†è¯­ä¹‰åˆ†ç»„
        for target_name, similar_nodes in semantic_groups.items():
            found_nodes = []
            total_papers = 0
            total_leaf_count = 0
            all_leaf_nodes = []
            all_keywords = []
            all_methods = []
            all_problems = []
            
            # æŸ¥æ‰¾å½“å‰èšåˆç»“æœä¸­çš„ç›¸ä¼¼èŠ‚ç‚¹
            for node_key in parent_aggregation.keys():
                node_display_name = parent_aggregation[node_key]['node_info'].get('display_name', node_key)
                if node_display_name in similar_nodes:
                    found_nodes.append(node_key)
                    node_data = parent_aggregation[node_key]
                    total_papers += node_data['node_info']['total_papers']
                    total_leaf_count += node_data['node_info']['leaf_count']
                    all_leaf_nodes.extend(node_data['node_info']['leaf_nodes'])
                    all_keywords.extend(node_data['aggregated_analysis']['keywords'])
                    all_methods.extend(node_data['aggregated_analysis']['methods'])
                    all_problems.extend(node_data['aggregated_analysis']['problems'])
                    used_nodes.add(node_key)
            
            # å¦‚æœæ‰¾åˆ°å¤šä¸ªç›¸ä¼¼èŠ‚ç‚¹ï¼Œåˆ™åˆå¹¶
            if len(found_nodes) > 1:
                print(f"ğŸ”— åˆå¹¶è¯­ä¹‰ç›¸ä¼¼èŠ‚ç‚¹: {[parent_aggregation[node]['node_info'].get('display_name', node) for node in found_nodes]} -> {target_name}")
                
                # é‡æ–°èšåˆç»Ÿè®¡
                def merge_items(items_list, item_type='keywords'):
                    term_counts = defaultdict(int)
                    term_descriptions = {}
                    
                    for item in items_list:
                        if isinstance(item, dict):
                            if item_type == 'keywords':
                                term = item.get('term', '')
                            elif item_type == 'methods':
                                term = item.get('method', '')
                            elif item_type == 'problems':
                                term = item.get('problem', '')
                            else:
                                term = item.get('term', item.get('method', item.get('problem', '')))
                            
                            frequency = item.get('frequency', 1)
                            description = item.get('description', '')
                            
                            if term:
                                term_counts[term] += frequency
                                if description and term not in term_descriptions:
                                    term_descriptions[term] = description
                    
                    # æ’åºå¹¶å–TOP10
                    sorted_terms = sorted(term_counts.items(), key=lambda x: x[1], reverse=True)[:10]
                    
                    result = []
                    for term, count in sorted_terms:
                        if item_type == 'keywords':
                            result.append({'term': term, 'frequency': count, 'description': term_descriptions.get(term, '')})
                        elif item_type == 'methods':
                            result.append({'method': term, 'frequency': count, 'description': term_descriptions.get(term, '')})
                        elif item_type == 'problems':
                            result.append({'problem': term, 'frequency': count, 'description': term_descriptions.get(term, '')})
                    
                    return result
                
                merged_keywords = merge_items(all_keywords, 'keywords')
                merged_methods = merge_items(all_methods, 'methods')
                merged_problems = merge_items(all_problems, 'problems')
                
                # ç”Ÿæˆæ±‡æ€»æè¿°
                summary = f"è¯¥é¢†åŸŸåŒ…å«{total_leaf_count}ä¸ªå­é¢†åŸŸï¼Œå…±{total_papers}ç¯‡è®ºæ–‡ã€‚"
                if merged_keywords:
                    top_keywords = [item['term'] for item in merged_keywords[:3]]
                    summary += f" ä¸»è¦å…³é”®è¯ï¼š{', '.join(top_keywords)}ã€‚"
                
                merged_aggregation[target_name] = {
                    'node_info': {
                        'name': target_name,
                        'display_name': target_name,
                        'total_papers': total_papers,
                        'leaf_count': total_leaf_count,
                        'leaf_nodes': list(set(all_leaf_nodes)),  # å»é‡
                        'merged_from': [parent_aggregation[node]['node_info'].get('display_name', node) for node in found_nodes]
                    },
                    'aggregated_analysis': {
                        'keywords': merged_keywords,
                        'methods': merged_methods,
                        'problems': merged_problems,
                        'summary': summary
                    }
                }
            elif len(found_nodes) == 1:
                # å•ä¸ªèŠ‚ç‚¹ï¼Œç›´æ¥ä½¿ç”¨ç›®æ ‡åç§°
                node_key = found_nodes[0]
                node_data = parent_aggregation[node_key]
                merged_aggregation[target_name] = {
                    'node_info': {
                        'name': target_name,
                        'display_name': target_name,
                        'total_papers': node_data['node_info']['total_papers'],
                        'leaf_count': node_data['node_info']['leaf_count'],
                        'leaf_nodes': node_data['node_info']['leaf_nodes']
                    },
                    'aggregated_analysis': node_data['aggregated_analysis']
                }
        
        # æ·»åŠ æœªè¢«åˆå¹¶çš„èŠ‚ç‚¹
        for node_key, node_data in parent_aggregation.items():
            if node_key not in used_nodes:
                display_name = node_data['node_info'].get('display_name', node_key)
                merged_aggregation[display_name] = {
                    'node_info': {
                        'name': display_name,
                        'display_name': display_name,
                        'total_papers': node_data['node_info']['total_papers'],
                        'leaf_count': node_data['node_info']['leaf_count'],
                        'leaf_nodes': node_data['node_info']['leaf_nodes']
                    },
                    'aggregated_analysis': node_data['aggregated_analysis']
                }
        
        return merged_aggregation

    def aggregate_parent_node_analysis(self, leaf_analysis: Dict) -> Dict:
        """
        å°†å¶å­èŠ‚ç‚¹åˆ†æç»“æœèšåˆåˆ°å€’æ•°ç¬¬äºŒå±‚èŠ‚ç‚¹
        
        Args:
            leaf_analysis: å¶å­èŠ‚ç‚¹åˆ†æç»“æœ
            
        Returns:
            çˆ¶èŠ‚ç‚¹èšåˆåˆ†æç»“æœ
        """
        parent_aggregation = {}
        
        # æŒ‰ç…§åˆ†ç±»è·¯å¾„èšåˆè®ºæ–‡
        classification_groups = defaultdict(list)
        
        for leaf_name, leaf_result in leaf_analysis.items():
            node_info = leaf_result['node_info']
            classification_path = node_info.get('classification_path', '')
            
            # è§£æåˆ†ç±»è·¯å¾„æ‰¾åˆ°å€’æ•°ç¬¬äºŒå±‚èŠ‚ç‚¹
            path_parts = classification_path.split(' â†’ ')
            
            if len(path_parts) >= 2:
                if len(path_parts) == 2:
                    # 2å±‚ï¼šèšåˆåˆ°root
                    parent_node = path_parts[0]
                elif len(path_parts) == 3:
                    # 3å±‚ï¼šèšåˆåˆ°level1  
                    parent_node = f"{path_parts[0]} â†’ {path_parts[1]}"
                elif len(path_parts) >= 4:
                    # 4å±‚æˆ–æ›´å¤šï¼šèšåˆåˆ°å€’æ•°ç¬¬äºŒå±‚
                    parent_node = " â†’ ".join(path_parts[:-1])
                else:
                    parent_node = "æœªçŸ¥çˆ¶èŠ‚ç‚¹"
                
                classification_groups[parent_node].append((leaf_name, leaf_result))
        
        # å¯¹æ¯ä¸ªçˆ¶èŠ‚ç‚¹è¿›è¡Œèšåˆåˆ†æ
        for parent_node, leaf_results in classification_groups.items():
            # ç»Ÿè®¡ä¿¡æ¯
            total_papers = sum(result[1]['node_info']['paper_count'] for result in leaf_results)
            leaf_count = len(leaf_results)
            
            # èšåˆå…³é”®è¯
            all_keywords = []
            all_methods = []
            all_problems = []
            
            for leaf_name, leaf_result in leaf_results:
                analysis = leaf_result['analysis']
                all_keywords.extend(analysis.get('keywords', []))
                all_methods.extend(analysis.get('methods', []))
                all_problems.extend(analysis.get('problems', []))
            
            # ç»Ÿè®¡é¢‘æ¬¡å¹¶å–TOP10
            def aggregate_items(items_list, item_type='term'):
                term_counts = defaultdict(int)
                term_descriptions = {}
                
                for item in items_list:
                    if isinstance(item, dict):
                        # æ ¹æ®item_typeæå–å¯¹åº”çš„å­—æ®µ
                        if item_type == 'keywords':
                            term = item.get('term', '')
                        elif item_type == 'methods':
                            term = item.get('method', '')
                        elif item_type == 'problems':
                            term = item.get('problem', '')
                        else:
                            term = item.get('term', item.get('method', item.get('problem', '')))
                        
                        frequency = item.get('frequency', 1)
                        description = item.get('description', '')
                        
                        if term:  # åªå¤„ç†éç©ºterm
                            term_counts[term] += frequency
                            if description and term not in term_descriptions:
                                term_descriptions[term] = description
                
                # æ’åºå¹¶å–TOP10
                sorted_terms = sorted(term_counts.items(), key=lambda x: x[1], reverse=True)[:10]
                
                # æ ¹æ®item_typeè¿”å›æ­£ç¡®çš„æ ¼å¼
                result = []
                for term, count in sorted_terms:
                    if item_type == 'keywords':
                        result.append({
                            'term': term,
                            'frequency': count,
                            'description': term_descriptions.get(term, '')
                        })
                    elif item_type == 'methods':
                        result.append({
                            'method': term,
                            'frequency': count,
                            'description': term_descriptions.get(term, '')
                        })
                    elif item_type == 'problems':
                        result.append({
                            'problem': term,
                            'frequency': count,
                            'description': term_descriptions.get(term, '')
                        })
                    else:
                        result.append({
                            'term': term,
                            'frequency': count,
                            'description': term_descriptions.get(term, '')
                        })
                
                return result
            
            aggregated_keywords = aggregate_items(all_keywords, 'keywords')
            aggregated_methods = aggregate_items(all_methods, 'methods')  
            aggregated_problems = aggregate_items(all_problems, 'problems')
            
            # ç”Ÿæˆæ±‡æ€»æè¿°
            summary = f"è¯¥é¢†åŸŸåŒ…å«{leaf_count}ä¸ªå­é¢†åŸŸï¼Œå…±{total_papers}ç¯‡è®ºæ–‡ã€‚"
            if aggregated_keywords:
                top_keywords = [item['term'] for item in aggregated_keywords[:3]]
                summary += f" ä¸»è¦å…³é”®è¯ï¼š{', '.join(top_keywords)}ã€‚"
            
            # æå–èŠ‚ç‚¹çš„ç®€æ´åç§°ï¼ˆæœ€åä¸€ä¸ªéƒ¨åˆ†ï¼‰
            display_name = parent_node.split(' â†’ ')[-1]
            
            parent_aggregation[parent_node] = {
                'node_info': {
                    'name': parent_node,  # å®Œæ•´è·¯å¾„ï¼Œç”¨äºå”¯ä¸€æ ‡è¯†
                    'display_name': display_name,  # ç®€æ´åç§°ï¼Œç”¨äºæ˜¾ç¤º
                    'total_papers': total_papers,
                    'leaf_count': leaf_count,
                    'leaf_nodes': [result[0] for result in leaf_results]
                },
                'aggregated_analysis': {
                    'keywords': aggregated_keywords,
                    'methods': aggregated_methods,
                    'problems': aggregated_problems,
                    'summary': summary
                }
            }
        
        logger.info(f"çˆ¶èŠ‚ç‚¹èšåˆåˆ†æå®Œæˆ: {len(parent_aggregation)} ä¸ªçˆ¶èŠ‚ç‚¹")
        
        # ç¬¬äºŒæ­¥ï¼šåŸºäºè¯­ä¹‰ç›¸ä¼¼æ€§åˆå¹¶ç›¸è¿‘èŠ‚ç‚¹
        logger.info("ğŸ”„ å¼€å§‹è¯­ä¹‰ç›¸ä¼¼æ€§åˆå¹¶...")
        merged_aggregation = self.semantic_merge_similar_nodes(parent_aggregation)
        logger.info(f"âœ… åˆå¹¶å®Œæˆ: {len(parent_aggregation)} -> {len(merged_aggregation)} ä¸ªçˆ¶èŠ‚ç‚¹")
        
        return merged_aggregation
    
    def run_tree_analysis(self, start_date: str = None, end_date: str = None, max_workers: int = 6) -> Dict:
        """
        è¿è¡Œå®Œæ•´çš„æ ‘å½¢åˆ†ææµç¨‹
        
        Args:
            start_date: å¼€å§‹æ—¥æœŸ (YYYY-MM-DD)
            end_date: ç»“æŸæ—¥æœŸ (YYYY-MM-DD)
            max_workers: æœ€å¤§å¹¶å‘æ•°
            
        Returns:
            å®Œæ•´çš„åˆ†æç»“æœ
        """
        logger.info("ğŸŒ³ å¼€å§‹æ ‘å½¢åˆ†ç±»åˆ†æ...")
        
        # 1. åŠ è½½è®ºæ–‡æ•°æ®
        logger.info("ğŸ“š ç¬¬ä¸€æ­¥ï¼šåŠ è½½è®ºæ–‡æ•°æ®...")
        papers = self.load_papers_by_date(start_date, end_date)
        
        if not papers:
            logger.warning("æ²¡æœ‰æ‰¾åˆ°è®ºæ–‡æ•°æ®")
            return {'error': 'æ²¡æœ‰æ‰¾åˆ°è®ºæ–‡æ•°æ®'}
        
        # 2. ç¬¬ä¸€æ­¥ï¼šè®ºæ–‡åˆ†ç±»
        logger.info("ğŸ·ï¸ ç¬¬äºŒæ­¥ï¼šè®ºæ–‡ç»†åˆ†åˆ†ç±»...")
        classified_papers = self.batch_classify_papers(papers, max_workers)
        
        # 3. æ„å»ºåˆ†ç±»æ ‘
        logger.info("ğŸŒ² ç¬¬ä¸‰æ­¥ï¼šæ„å»ºåˆ†ç±»æ ‘...")
        tree_root = self.build_classification_tree(classified_papers)
        
        # 4. ç¬¬äºŒæ­¥ï¼šå¶å­èŠ‚ç‚¹åˆ†æ
        logger.info("ğŸ“Š ç¬¬å››æ­¥ï¼šå¶å­èŠ‚ç‚¹ç»Ÿè®¡åˆ†æ...")
        leaf_analysis_results = self.batch_analyze_leaf_nodes(max_workers)
        
        # 5. çˆ¶èŠ‚ç‚¹èšåˆåˆ†æï¼ˆæ ¸å¿ƒæ”¹è¿›ï¼‰
        logger.info("ğŸ”„ ç¬¬äº”æ­¥ï¼šçˆ¶èŠ‚ç‚¹èšåˆåˆ†æ...")
        parent_aggregation_results = self.aggregate_parent_node_analysis(leaf_analysis_results)
        
        # 6. ç”Ÿæˆæ€»ä½“ç»Ÿè®¡
        logger.info("ğŸ“ˆ ç¬¬å…­æ­¥ï¼šç”Ÿæˆæ€»ä½“ç»Ÿè®¡...")
        tree_statistics = self.generate_tree_statistics(tree_root, leaf_analysis_results)
        
        # 7. ä¿å­˜ç»“æœ
        results = {
            'date_range': {
                'start_date': start_date,
                'end_date': end_date
            },
            'total_papers': len(papers),
            'classification_tree_stats': tree_statistics,
            'leaf_analysis_results': leaf_analysis_results,
            'parent_aggregation_results': parent_aggregation_results,  # æ–°å¢çˆ¶èŠ‚ç‚¹èšåˆç»“æœ
            'analysis_timestamp': datetime.now().isoformat()
        }
        
        # ä¿å­˜ç»“æœåˆ°æ–‡ä»¶
        self.save_analysis_results(results)
        
        logger.info("âœ… æ ‘å½¢åˆ†ç±»åˆ†æå®Œæˆï¼")
        return results
    
    def generate_tree_statistics(self, tree_root: TreeNode, leaf_analysis: Dict) -> Dict:
        """ç”Ÿæˆåˆ†ç±»æ ‘çš„ç»Ÿè®¡ä¿¡æ¯"""
        stats = {
            'total_leaf_nodes': 0,
            'total_papers_classified': 0,
            'classification_distribution': {},
            'top_leaf_nodes_by_papers': [],
            'classification_confidence_stats': {
                'mean': 0.0,
                'std': 0.0,
                'distribution': {}
            }
        }
        
        # æ”¶é›†æ‰€æœ‰å¶å­èŠ‚ç‚¹
        leaf_nodes = tree_root.get_leaf_nodes()
        valid_leaf_nodes = [node for node in leaf_nodes if node.papers]
        
        stats['total_leaf_nodes'] = len(valid_leaf_nodes)
        
        # ç»Ÿè®¡å„çº§åˆ†ç±»çš„åˆ†å¸ƒ
        classification_counts = defaultdict(lambda: defaultdict(lambda: defaultdict(lambda: defaultdict(int))))
        confidence_scores = []
        leaf_paper_counts = []
        
        for node in valid_leaf_nodes:
            paper_count = len(node.papers)
            stats['total_papers_classified'] += paper_count
            leaf_paper_counts.append((node.name, paper_count))
            
            # åˆ†ææ¯ç¯‡è®ºæ–‡çš„åˆ†ç±»è·¯å¾„
            for paper in node.papers:
                classification_path = paper.get('classification_path', '')
                confidence = paper.get('classification_confidence', 0.0)
                confidence_scores.append(confidence)
                
                # è§£æåˆ†ç±»è·¯å¾„
                if ' â†’ ' in classification_path:
                    path_parts = classification_path.split(' â†’ ')
                    if len(path_parts) >= 4:
                        root, level1, level2, leaf = path_parts[:4]
                        classification_counts[root][level1][level2][leaf] += 1
        
        # è®¡ç®—ç½®ä¿¡åº¦ç»Ÿè®¡
        if confidence_scores:
            stats['classification_confidence_stats']['mean'] = float(np.mean(confidence_scores))
            stats['classification_confidence_stats']['std'] = float(np.std(confidence_scores))
            
            # ç½®ä¿¡åº¦åˆ†å¸ƒ
            confidence_ranges = [(0.0, 0.2), (0.2, 0.4), (0.4, 0.6), (0.6, 0.8), (0.8, 1.0)]
            for low, high in confidence_ranges:
                range_name = f"{low}-{high}"
                count = sum(1 for score in confidence_scores if low <= score < high)
                stats['classification_confidence_stats']['distribution'][range_name] = count
        
        # è½¬æ¢åˆ†ç±»åˆ†å¸ƒä¸ºæ™®é€šå­—å…¸
        stats['classification_distribution'] = self._convert_nested_defaultdict(classification_counts)
        
        # Topå¶å­èŠ‚ç‚¹æ’åº
        stats['top_leaf_nodes_by_papers'] = sorted(leaf_paper_counts, key=lambda x: x[1], reverse=True)[:20]
        
        return stats
    
    def _convert_nested_defaultdict(self, d):
        """é€’å½’è½¬æ¢åµŒå¥—çš„defaultdictä¸ºæ™®é€šdict"""
        if isinstance(d, defaultdict):
            d = dict(d)
        
        for key, value in d.items():
            if isinstance(value, defaultdict):
                d[key] = self._convert_nested_defaultdict(value)
        
        return d
    
    def save_analysis_results(self, results: Dict, output_dir: str = "tree_analysis_results"):
        """ä¿å­˜åˆ†æç»“æœ"""
        # åˆ›å»ºè¾“å‡ºç›®å½•
        output_path = Path(output_dir)
        output_path.mkdir(exist_ok=True)
        
        # ä¿å­˜å®Œæ•´ç»“æœ
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # å®Œæ•´ç»“æœæ–‡ä»¶
        full_results_file = output_path / f"tree_analysis_{timestamp}.json"
        with open(full_results_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        # å¶å­èŠ‚ç‚¹åˆ†ææ‘˜è¦
        summary_file = output_path / f"leaf_analysis_summary_{timestamp}.json"
        leaf_summary = {}
        for leaf_name, leaf_data in results.get('leaf_analysis_results', {}).items():
            leaf_summary[leaf_name] = {
                'paper_count': leaf_data['node_info']['paper_count'],
                'classification_path': leaf_data['node_info']['classification_path'],
                'top_keywords': leaf_data['analysis']['keywords'][:5],
                'top_methods': leaf_data['analysis']['methods'][:5],
                'top_problems': leaf_data['analysis']['problems'][:5],
                'summary': leaf_data['analysis']['summary'][:200] + '...' if len(leaf_data['analysis']['summary']) > 200 else leaf_data['analysis']['summary']
            }
        
        with open(summary_file, 'w', encoding='utf-8') as f:
            json.dump(leaf_summary, f, ensure_ascii=False, indent=2)
        
        # çˆ¶èŠ‚ç‚¹èšåˆç»“æœæ‘˜è¦
        parent_summary_file = output_path / f"parent_aggregation_summary_{timestamp}.json"
        parent_summary = {}
        for parent_name, parent_data in results.get('parent_aggregation_results', {}).items():
            display_name = parent_data['node_info'].get('display_name', parent_name)
            parent_summary[display_name] = {
                'full_path': parent_name,
                'total_papers': parent_data['node_info']['total_papers'],
                'leaf_count': parent_data['node_info']['leaf_count'],
                'leaf_nodes': parent_data['node_info']['leaf_nodes'],
                'top_keywords': parent_data['aggregated_analysis']['keywords'][:10],
                'top_methods': parent_data['aggregated_analysis']['methods'][:10],
                'top_problems': parent_data['aggregated_analysis']['problems'][:10],
                'summary': parent_data['aggregated_analysis']['summary']
            }
        
        with open(parent_summary_file, 'w', encoding='utf-8') as f:
            json.dump(parent_summary, f, ensure_ascii=False, indent=2)
        
        # CSVæ ¼å¼çš„å¶å­èŠ‚ç‚¹ç»Ÿè®¡
        csv_file = output_path / f"leaf_nodes_stats_{timestamp}.csv"
        csv_data = []
        for leaf_name, leaf_data in results.get('leaf_analysis_results', {}).items():
            csv_data.append({
                'leaf_node': leaf_name,
                'classification_path': leaf_data['node_info']['classification_path'],
                'paper_count': leaf_data['node_info']['paper_count'],
                'top_keywords': '; '.join([item['term'] for item in leaf_data['analysis']['keywords'][:5]]),
                'top_methods': '; '.join([item['method'] for item in leaf_data['analysis']['methods'][:5]]),
                'top_problems': '; '.join([item['problem'] for item in leaf_data['analysis']['problems'][:5]])
            })
        
        if csv_data:
            df = pd.DataFrame(csv_data)
            df.to_csv(csv_file, index=False, encoding='utf-8-sig')
        
        # CSVæ ¼å¼çš„çˆ¶èŠ‚ç‚¹èšåˆç»Ÿè®¡
        parent_csv_file = output_path / f"parent_aggregation_stats_{timestamp}.csv"
        parent_csv_data = []
        for parent_name, parent_data in results.get('parent_aggregation_results', {}).items():
            display_name = parent_data['node_info'].get('display_name', parent_name)
            parent_csv_data.append({
                'parent_node': display_name,
                'full_path': parent_name,
                'total_papers': parent_data['node_info']['total_papers'],
                'leaf_count': parent_data['node_info']['leaf_count'],
                'top_keywords': '; '.join([item.get('term', '') for item in parent_data['aggregated_analysis']['keywords'][:5]]),
                'top_methods': '; '.join([item.get('method', '') for item in parent_data['aggregated_analysis']['methods'][:5]]),
                'top_problems': '; '.join([item.get('problem', '') for item in parent_data['aggregated_analysis']['problems'][:5]]),
                'summary': parent_data['aggregated_analysis']['summary']
            })
        
        if parent_csv_data:
            parent_df = pd.DataFrame(parent_csv_data)
            parent_df.to_csv(parent_csv_file, index=False, encoding='utf-8-sig')
        
        logger.info(f"åˆ†æç»“æœå·²ä¿å­˜åˆ°: {output_path}")
        logger.info(f"  å®Œæ•´ç»“æœ: {full_results_file}")
        logger.info(f"  å¶å­èŠ‚ç‚¹æ‘˜è¦: {summary_file}")
        logger.info(f"  çˆ¶èŠ‚ç‚¹èšåˆæ‘˜è¦: {parent_summary_file}")
        logger.info(f"  å¶å­èŠ‚ç‚¹CSV: {csv_file}")
        logger.info(f"  çˆ¶èŠ‚ç‚¹èšåˆCSV: {parent_csv_file}")

def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='è®ºæ–‡æ ‘å½¢åˆ†ç±»åˆ†æç³»ç»Ÿ')
    parser.add_argument('--start-date', help='å¼€å§‹æ—¥æœŸ (YYYY-MM-DD)')
    parser.add_argument('--end-date', help='ç»“æŸæ—¥æœŸ (YYYY-MM-DD)')
    parser.add_argument('--api-key', help='OpenAI API Key')
    parser.add_argument('--data-dir', default='data', help='æ•°æ®ç›®å½•')
    parser.add_argument('--max-workers', type=int, default=6, help='æœ€å¤§å¹¶å‘æ•°')
    
    args = parser.parse_args()
    
    try:
        # åˆ›å»ºåˆ†æå™¨
        analyzer = TreeClassificationAnalyzer(
            api_key=args.api_key,
            data_dir=args.data_dir
        )
        
        # è¿è¡Œåˆ†æ
        results = analyzer.run_tree_analysis(
            start_date=args.start_date,
            end_date=args.end_date,
            max_workers=args.max_workers
        )
        
        if 'error' in results:
            print(f"âŒ åˆ†æå¤±è´¥: {results['error']}")
            return
        
        # æ˜¾ç¤ºç»“æœæ‘˜è¦
        print("\nğŸ‰ åˆ†æå®Œæˆï¼")
        print(f"ğŸ“Š æ€»è®ºæ–‡æ•°: {results['total_papers']}")
        print(f"ğŸŒ² å¶å­èŠ‚ç‚¹æ•°: {results['classification_tree_stats']['total_leaf_nodes']}")
        print(f"ğŸ“š å·²åˆ†ç±»è®ºæ–‡: {results['classification_tree_stats']['total_papers_classified']}")
        print(f"ğŸ”„ çˆ¶èŠ‚ç‚¹èšåˆ: {len(results['parent_aggregation_results'])} ä¸ªçˆ¶èŠ‚ç‚¹")
        
        # æ˜¾ç¤ºTopçˆ¶èŠ‚ç‚¹èšåˆï¼ˆæ ¸å¿ƒæ”¹è¿›æ•ˆæœï¼‰
        print("\nğŸ“ˆ è®ºæ–‡æ•°é‡æœ€å¤šçš„çˆ¶èŠ‚ç‚¹èšåˆ:")
        parent_nodes = [(name, data['node_info']['total_papers']) 
                       for name, data in results['parent_aggregation_results'].items()]
        parent_nodes.sort(key=lambda x: x[1], reverse=True)
        
        for i, (node_name, paper_count) in enumerate(parent_nodes[:10], 1):
            parent_data = results['parent_aggregation_results'][node_name]
            leaf_count = parent_data['node_info']['leaf_count']
            display_name = parent_data['node_info'].get('display_name', node_name)
            print(f"  {i:2d}. {display_name}: {paper_count} ç¯‡è®ºæ–‡ ({leaf_count} ä¸ªå­é¢†åŸŸ)")
        
        # æ˜¾ç¤ºTopå¶å­èŠ‚ç‚¹ï¼ˆå¯¹æ¯”ï¼‰
        print("\nğŸ“Š è®ºæ–‡æ•°é‡æœ€å¤šçš„å¶å­èŠ‚ç‚¹ï¼ˆå¯¹æ¯”ï¼‰:")
        top_nodes = results['classification_tree_stats']['top_leaf_nodes_by_papers'][:5]
        for i, (node_name, paper_count) in enumerate(top_nodes, 1):
            print(f"  {i:2d}. {node_name}: {paper_count} ç¯‡")
        
        print(f"\nğŸ’¾ è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ° tree_analysis_results/ ç›®å½•")
        print("ğŸ“‹ æ ¸å¿ƒæ”¹è¿›ï¼šç°åœ¨ç»Ÿè®¡èšåˆåˆ°å€’æ•°ç¬¬äºŒå±‚èŠ‚ç‚¹ï¼Œæœ‰æ•ˆè§£å†³äº†å¶å­èŠ‚ç‚¹è¿‡å¤šçš„é—®é¢˜ï¼")
        
    except Exception as e:
        logger.error(f"ç¨‹åºæ‰§è¡Œå¤±è´¥: {e}")
        print(f"âŒ ç¨‹åºæ‰§è¡Œå¤±è´¥: {e}")

if __name__ == "__main__":
    main()